{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saumya Kothari - Introduction to Neural Networks & Deep Learning Project [Part 4].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhEAXCwK1dW"
      },
      "source": [
        "# **Saumya Kothari - Introduction to Neural Networks & Deep Learning Project [Part 4]**\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "## **Part 4 [solved in the other ipynb notebook]**\n",
        "\n",
        "#### DOMAIN: \n",
        "##### Autonomous Vehicles\n",
        "\n",
        "#### BUSINESS CONTEXT: \n",
        "- A Recognising multi-digit numbers in photographs captured at street level is an important component of modern-day map making. A classic example of a corpus of such street-level photographs is Googleâ€™s Street View imagery composed of hundreds of millions of geo-located 360-degree panoramic images.\n",
        "- The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. More broadly, recognising numbers in photographs is a problem of interest to the optical character recognition community.\n",
        "- While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts, colours, styles, orientations, and character arrangements.\n",
        "- The recognition problem is further complicated by environmental factors such as lighting, shadows, specularity, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs. In this project, we will use the dataset with images centred around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more complex than MNIST because of the distractors.\n",
        "\n",
        "#### DATA DESCRIPTION: \n",
        "The SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with the minimal requirement on data formatting but comes from a significantly harder,\n",
        "unsolved, real-world problem (recognising digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. Where the labels for each of this image are the prominent number in that image i.e. 2,6,7 and 4 respectively.\n",
        "The dataset has been provided in the form of h5py files. You can read about this file format here: http://docs.h5py.org/en/stable/high/dataset.html\n",
        "\n",
        "Acknowledgement: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading\n",
        "Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and\n",
        "Unsupervised Feature Learning 2011. PDF\n",
        "http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary\n",
        "\n",
        "#### PROJECT OBJECTIVE: \n",
        "We will build a digit classifier on the SVHN (Street View Housing Number) dataset.\n",
        "Steps and tasks: [ Total Score: 30 points]\n",
        "1. Import the data.\n",
        "2. Data pre-processing and visualisation.\n",
        "3. Design, train, tune and test a neural network image classifier.\n",
        "Hint: Use best approach to refine and tune the data or the model. Be highly experimental here to get the best accuracy out of the model.\n",
        "4. Plot the training loss, validation loss vs number of epochs and training accuracy, validation accuracy vs number of\n",
        "epochs plot and write your observations on the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHG7-t6_j2QI",
        "outputId": "8c2938df-d4e8-4882-d1f2-13f1c23b9081"
      },
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eHZ-1-hj8l9"
      },
      "source": [
        "# Setting the current working directory\n",
        "import os; \n",
        "os.chdir('/content/drive/MyDrive/NeuralNetworks')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib0ySLAvkw5g"
      },
      "source": [
        "# Importing neccessary packages\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, h5py\n",
        "import matplotlib.style as style; style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "# Metrics and preprocessing\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# TF and Keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Checking if GPU is found\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "#tf.reset_default_graph()\n",
        "#tf.set_random_seed(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFphXQIxk0P5",
        "outputId": "f65e0178-e4bf-448a-cf24-27da517039af"
      },
      "source": [
        "!ls '/content/drive/MyDrive/NeuralNetworks'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Part - 4 - Autonomous_Vehicles_SVHN_single_grey1.h5'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1kUIU9EMwZc"
      },
      "source": [
        "**Load train, validatin and test datasets from h5 file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZmeGu9qlF2A",
        "outputId": "a57d9cc1-5893-4e97-ee29-e80c6d52a691"
      },
      "source": [
        "# Read the h5 file\n",
        "h5_Vehicle = h5py.File('Part - 4 - Autonomous_Vehicles_SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, validation and test sets\n",
        "X_train = h5_Vehicle['X_train'][:]\n",
        "y_train = h5_Vehicle['y_train'][:]\n",
        "X_val = h5_Vehicle['X_val'][:]\n",
        "y_val = h5_Vehicle['y_val'][:]\n",
        "X_test = h5_Vehicle['X_test'][:]\n",
        "y_test = h5_Vehicle['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "\n",
        "h5_Vehicle.close()\n",
        "\n",
        "print('Training set', X_train.shape, y_train.shape)\n",
        "print('Validation set', X_val.shape, y_val.shape)\n",
        "print('Test set', X_test.shape, y_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "print('Unique labels in y_train:', np.unique(y_train))\n",
        "print('Unique labels in y_val:', np.unique(y_val))\n",
        "print('Unique labels in y_test:', np.unique(y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (42000, 32, 32) (42000,)\n",
            "Validation set (60000, 32, 32) (60000,)\n",
            "Test set (18000, 32, 32) (18000,)\n",
            "\n",
            "\n",
            "Unique labels in y_train: [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_val: [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_test: [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk5Or8QdM61t"
      },
      "source": [
        "**Observation:**\n",
        "Length of training sets: 42k, validation sets: 60k, test sets: 18k\n",
        "\n",
        "*   Length of training sets: 42000, validation sets: 60000, test sets: 18000\n",
        "*   Size of the images: 32*32\n",
        "*   Number of class: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "_AsDTrXQox7X",
        "outputId": "1ac461d1-9705-4257-c56b-2d410fa4c82e"
      },
      "source": [
        "# Visualizing first 10 images in the dataset and their labels\n",
        "plt.figure(figsize = (15, 5))\n",
        "for i in range(10):  \n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i].reshape((32, 32)),cmap = plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "plt.subplots_adjust(wspace = -0.1, hspace = -0.1)\n",
        "plt.show()\n",
        "\n",
        "print('Label for each of the above image: %s' % (y_train[0 : 10]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "bz82BLjXpQY-",
        "outputId": "ff79c5cc-8be7-4c26-a9c4-b1941522612e"
      },
      "source": [
        "print('Checking first image and label in training set: '); print('---'*20)\n",
        "plt.imshow(X_train[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in training set: \n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "PYyOfZdNpUB9",
        "outputId": "d109d862-689e-4811-b1d9-834a78f1cc3a"
      },
      "source": [
        "print('Checking first image and label in validation set:'); print('---'*20)\n",
        "plt.imshow(X_val[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_val[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in validation set:\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "R4adt3-NKXe5",
        "outputId": "e08721cc-12aa-4d4e-9783-d8fd7c29396c"
      },
      "source": [
        "print('Checking first image and label in test set:'); print('---'*20)\n",
        "plt.imshow(X_test[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_test[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in test set:\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ytynqHN9OX"
      },
      "source": [
        "**Flatten and normalize the images for Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzsePJvFMo_j",
        "outputId": "06bbc213-e29d-4e63-aa53-c855c785f33a"
      },
      "source": [
        "# Reshaping the data to make it into two dimensions\n",
        "print('Reshaping X data: From (n, 32, 32) to (n, 1024)'); print('----'*20)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "#Changing the datatype\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('----'*20)\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('----'*20)\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('----'*20)\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: From (n, 32, 32) to (n, 1024)\n",
            "--------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "--------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "--------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buTysfVCOOZB",
        "outputId": "f66dff6c-9f83-457a-e795-4d63410fa732"
      },
      "source": [
        "#Printing SHAPE\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_val shape:', X_val.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_val shape:', y_val.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Number of images in X_train', X_train.shape[0])\n",
        "print('Number of images in X_val', X_val.shape[0])\n",
        "print('Number of images in X_test', X_test.shape[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (42000, 1024)\n",
            "X_val shape: (60000, 1024)\n",
            "X_test shape: (18000, 1024)\n",
            "\n",
            "\n",
            "y_train shape: (42000, 10, 2, 2)\n",
            "y_val shape: (60000, 10, 2, 2)\n",
            "y_test shape: (18000, 10, 2, 2)\n",
            "\n",
            "\n",
            "Number of images in X_train 42000\n",
            "Number of images in X_val 60000\n",
            "Number of images in X_test 18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgctLZmUO-OT"
      },
      "source": [
        "### **Modelling - Baby sitting the learning process**\n",
        "\n",
        "**Fully connected linear layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohZCj22pO4wF"
      },
      "source": [
        "\n",
        "class Linear():\n",
        "    def __init__(self, in_size, out_size):\n",
        "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
        "        self.b = np.zeros((1, out_size))\n",
        "        self.params = [self.W, self.b]\n",
        "        self.gradW = None\n",
        "        self.gradB = None\n",
        "        self.gradInput = None        \n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        self.output = np.dot(X, self.W) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        self.gradW = np.dot(self.X.T, nextgrad)\n",
        "        self.gradB = np.sum(nextgrad, axis=0)\n",
        "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
        "        return self.gradInput, [self.gradW, self.gradB]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZoPIv3BPPBc"
      },
      "source": [
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXZV7niHPGj9"
      },
      "source": [
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.output = np.maximum(X, 0)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        self.gradInput = nextgrad.copy()\n",
        "        self.gradInput[self.output <=0] = 0\n",
        "        return self.gradInput, []"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBNoMj8JPcww"
      },
      "source": [
        "**Softmax function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bYhns6jPRPr"
      },
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9yvghepPgbj"
      },
      "source": [
        "**Cross entropy loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f82Ng11fPYzg"
      },
      "source": [
        "class CrossEntropy:\n",
        "    def forward(self, X, y):\n",
        "        self.m = y.shape[0]\n",
        "        self.p = softmax(X)\n",
        "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
        "        loss = np.sum(cross_entropy) / self.m\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        y_idx = y.argmax()        \n",
        "        grad = softmax(X)\n",
        "        grad[range(self.m), y] -= 1\n",
        "        grad /= self.m\n",
        "        return grad"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0VmZp8kPp-D"
      },
      "source": [
        "**NN class that enables the forward prop and backward propagation of the entire network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHlX8NMPnA0"
      },
      "source": [
        "class NN():\n",
        "    def __init__(self, lossfunc = CrossEntropy(), mode = 'train'):\n",
        "        self.params = []\n",
        "        self.layers = []\n",
        "        self.loss_func = lossfunc\n",
        "        self.grads = []\n",
        "        self.mode = mode\n",
        "        \n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        self.params.append(layer.params)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "    \n",
        "    def backward(self, nextgrad):\n",
        "        self.clear_grad_param()\n",
        "        for layer in reversed(self.layers):\n",
        "            nextgrad, grad = layer.backward(nextgrad)\n",
        "            self.grads.append(grad)\n",
        "        return self.grads\n",
        "    \n",
        "    def train_step(self, X, y):\n",
        "        out = self.forward(X)\n",
        "        loss = self.loss_func.forward(out,y)\n",
        "        nextgrad = self.loss_func.backward(out,y)\n",
        "        grads = self.backward(nextgrad)\n",
        "        return loss, grads\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return np.argmax(p, axis=1)\n",
        "    \n",
        "    def predict_scores(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return p\n",
        "    \n",
        "    def clear_grad_param(self):\n",
        "        self.grads = []"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXon5plnPzIa"
      },
      "source": [
        "**Update SGD function with momentum**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7s-o_-jPygd"
      },
      "source": [
        "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
        "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
        "        for i in range(len(g)):\n",
        "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
        "            p[i] += v[i]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eFrXbAaP6VN"
      },
      "source": [
        "**Getting minibatches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmisnSczP5xp"
      },
      "source": [
        "def minibatch(X, y, minibatch_size):\n",
        "    n = X.shape[0]\n",
        "    minibatches = []\n",
        "    permutation = np.random.permutation(X.shape[0])\n",
        "    X = X[permutation]\n",
        "    y = y[permutation]\n",
        "    \n",
        "    for i in range(0, n , minibatch_size):\n",
        "        X_batch = X[i:i + minibatch_size, :]\n",
        "        y_batch = y[i:i + minibatch_size, ]\n",
        "        minibatches.append((X_batch, y_batch))\n",
        "        \n",
        "    return minibatches"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF9deApgQNxq"
      },
      "source": [
        "**The Training:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQVmadzkQCoq"
      },
      "source": [
        "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val = None, y_val = None, Lambda = 0, verb = True):\n",
        "    validationSet_loss_epochs = []\n",
        "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
        "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        loss_batch = []\n",
        "        val_loss_batch = []\n",
        "        velocity = []\n",
        "        for param_layer in net.params:\n",
        "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
        "            velocity.append(p)\n",
        "            \n",
        "        # iterate over mini batches\n",
        "        for X_mini, y_mini in minibatches:\n",
        "            loss, grads = net.train_step(X_mini, y_mini)\n",
        "            loss_batch.append(loss)\n",
        "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
        "\n",
        "        for X_mini_val, y_mini_val in minibatches_val:\n",
        "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
        "            val_loss_batch.append(val_loss)\n",
        "        \n",
        "        # accuracy of model at end of epoch after all mini batch updates\n",
        "        m_train = X_train.shape[0]\n",
        "        m_val = X_val.shape[0]\n",
        "        y_train_pred = []\n",
        "        y_val_pred = []\n",
        "        y_train1 = []\n",
        "        y_vall = []\n",
        "        for j in range(0, m_train, minibatch_size):\n",
        "            X_tr = X_train[j:j + minibatch_size, : ]\n",
        "            y_tr = y_train[j:j + minibatch_size,]\n",
        "            y_train1 = np.append(y_train1, y_tr)\n",
        "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
        "\n",
        "        for j in range(0, m_val, minibatch_size):\n",
        "            X_va = X_val[j:j + minibatch_size, : ]\n",
        "            y_va = y_val[j:j + minibatch_size,]\n",
        "            y_vall = np.append(y_vall, y_va)\n",
        "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
        "            \n",
        "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
        "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
        "        \n",
        "        ## weights\n",
        "        w = np.array(net.params[0][0])\n",
        "        \n",
        "        ## adding regularization to cost\n",
        "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
        "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
        "        \n",
        "        validationSet_loss_epochs.append(mean_val_loss)\n",
        "        if verb:\n",
        "            if i%50==0:\n",
        "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
        "    return net, val_acc"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFWx3ZlMYQZR"
      },
      "source": [
        "**Checking the accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbx9ePapYM6L"
      },
      "source": [
        "def check_accuracy(y_true, y_pred):\n",
        "    return np.mean(y_pred == y_true)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqX503cZViH"
      },
      "source": [
        "**Invoking created functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdN33AKdYXjy"
      },
      "source": [
        "# Invoking the model\n",
        "## input size\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "def train_and_test_loop(iterations, lr, Lambda, verb = True):\n",
        "    ## hyperparameters\n",
        "    iterations = iterations\n",
        "    learning_rate = lr\n",
        "    hidden_nodes1 = 10\n",
        "    output_nodes = 10\n",
        "\n",
        "    ## define neural net\n",
        "    nn = NN()\n",
        "    nn.add_layer(Linear(input_dim, hidden_nodes1))\n",
        "\n",
        "    nn, val_acc = train(nn, X_train, y_train_o, minibatch_size = 200, epoch = iterations, learning_rate = learning_rate,\\\n",
        "                      X_val = X_test, y_val = y_test_o, Lambda = Lambda, verb = verb)\n",
        "    return val_acc"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRwtTNMmZT2I",
        "outputId": "ddd9d417-8df2-4527-f1a1-cfc310d808f4"
      },
      "source": [
        "# Disable the regaularization after checking loss\n",
        "lr = 0.00001\n",
        "Lambda = 0\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.302585283088422 | Training Accuracy = 0.10192857142857142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5fHZqpTZtSc"
      },
      "source": [
        "**Increase Lambda(Regularization) and check what it does to our loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaIFCBxtZjbJ",
        "outputId": "94e161db-379c-44b0-f762-5542cacb49f6"
      },
      "source": [
        "lr = 0.00001\n",
        "Lambda = 1e4\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.3025852165243452 | Training Accuracy = 0.10192857142857142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0beyS0_ZZzsi",
        "outputId": "f024d58c-2e8f-40c0-ee0f-0659840a818c"
      },
      "source": [
        "# Overfitting to a small subset of our dataset (in this case 20 images)\n",
        "\n",
        "X_train_subset = X_train[0:20]\n",
        "y_train_subset = y_train[0:20]\n",
        "\n",
        "X_train = X_train_subset\n",
        "y_train = y_train_subset\n",
        "\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 1024), (20, 10, 2, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPBTDR-KavZF"
      },
      "source": [
        "**Making sure that we overfit very small portion of the training data**\n",
        "\n",
        "So, set a small learning rate and turn regularization off In the code below:\n",
        "\n",
        "*   Take the first 20 examples\n",
        "*   turn off regularization(reg=0.0)\n",
        "*   use simple vanilla 'sgd'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kp8MhYKaH-g",
        "outputId": "98647cb5-6dad-4fa6-e3d4-949eb2af023e"
      },
      "source": [
        "%time\n",
        "lr = 0.001\n",
        "Lambda = 0\n",
        "train_and_test_loop(5000, lr, Lambda)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
            "Wall time: 5.96 Âµs\n",
            "Epoch 0/5000: Loss = 2.302585560613877 | Training Accuracy = 0.25\n",
            "Epoch 50/5000: Loss = 2.2993515989242357 | Training Accuracy = 0.25\n",
            "Epoch 100/5000: Loss = 2.2961498719261715 | Training Accuracy = 0.25\n",
            "Epoch 150/5000: Loss = 2.2929801119822977 | Training Accuracy = 0.25\n",
            "Epoch 200/5000: Loss = 2.2898420522393947 | Training Accuracy = 0.25\n",
            "Epoch 250/5000: Loss = 2.2867354266485527 | Training Accuracy = 0.25\n",
            "Epoch 300/5000: Loss = 2.283659969985244 | Training Accuracy = 0.25\n",
            "Epoch 350/5000: Loss = 2.280615417869293 | Training Accuracy = 0.25\n",
            "Epoch 400/5000: Loss = 2.277601506784756 | Training Accuracy = 0.25\n",
            "Epoch 450/5000: Loss = 2.2746179740996704 | Training Accuracy = 0.25\n",
            "Epoch 500/5000: Loss = 2.271664558085676 | Training Accuracy = 0.25\n",
            "Epoch 550/5000: Loss = 2.2687409979374964 | Training Accuracy = 0.25\n",
            "Epoch 600/5000: Loss = 2.2658470337922565 | Training Accuracy = 0.25\n",
            "Epoch 650/5000: Loss = 2.2629824067486353 | Training Accuracy = 0.25\n",
            "Epoch 700/5000: Loss = 2.2601468588858338 | Training Accuracy = 0.25\n",
            "Epoch 750/5000: Loss = 2.257340133282354 | Training Accuracy = 0.25\n",
            "Epoch 800/5000: Loss = 2.2545619740345675 | Training Accuracy = 0.25\n",
            "Epoch 850/5000: Loss = 2.2518121262750768 | Training Accuracy = 0.25\n",
            "Epoch 900/5000: Loss = 2.249090336190844 | Training Accuracy = 0.25\n",
            "Epoch 950/5000: Loss = 2.2463963510410876 | Training Accuracy = 0.25\n",
            "Epoch 1000/5000: Loss = 2.2437299191749314 | Training Accuracy = 0.25\n",
            "Epoch 1050/5000: Loss = 2.241090790048798 | Training Accuracy = 0.25\n",
            "Epoch 1100/5000: Loss = 2.238478714243543 | Training Accuracy = 0.25\n",
            "Epoch 1150/5000: Loss = 2.2358934434813023 | Training Accuracy = 0.25\n",
            "Epoch 1200/5000: Loss = 2.233334730642075 | Training Accuracy = 0.25\n",
            "Epoch 1250/5000: Loss = 2.230802329780003 | Training Accuracy = 0.25\n",
            "Epoch 1300/5000: Loss = 2.228295996139358 | Training Accuracy = 0.25\n",
            "Epoch 1350/5000: Loss = 2.2258154861702257 | Training Accuracy = 0.25\n",
            "Epoch 1400/5000: Loss = 2.223360557543871 | Training Accuracy = 0.25\n",
            "Epoch 1450/5000: Loss = 2.2209309691677896 | Training Accuracy = 0.25\n",
            "Epoch 1500/5000: Loss = 2.2185264812004357 | Training Accuracy = 0.25\n",
            "Epoch 1550/5000: Loss = 2.2161468550656096 | Training Accuracy = 0.25\n",
            "Epoch 1600/5000: Loss = 2.21379185346652 | Training Accuracy = 0.25\n",
            "Epoch 1650/5000: Loss = 2.211461240399497 | Training Accuracy = 0.25\n",
            "Epoch 1700/5000: Loss = 2.2091547811673578 | Training Accuracy = 0.25\n",
            "Epoch 1750/5000: Loss = 2.2068722423924245 | Training Accuracy = 0.25\n",
            "Epoch 1800/5000: Loss = 2.2046133920291804 | Training Accuracy = 0.25\n",
            "Epoch 1850/5000: Loss = 2.2023779993765755 | Training Accuracy = 0.25\n",
            "Epoch 1900/5000: Loss = 2.200165835089956 | Training Accuracy = 0.25\n",
            "Epoch 1950/5000: Loss = 2.197976671192638 | Training Accuracy = 0.25\n",
            "Epoch 2000/5000: Loss = 2.195810281087105 | Training Accuracy = 0.25\n",
            "Epoch 2050/5000: Loss = 2.193666439565839 | Training Accuracy = 0.25\n",
            "Epoch 2100/5000: Loss = 2.191544922821767 | Training Accuracy = 0.25\n",
            "Epoch 2150/5000: Loss = 2.189445508458342 | Training Accuracy = 0.25\n",
            "Epoch 2200/5000: Loss = 2.187367975499241 | Training Accuracy = 0.25\n",
            "Epoch 2250/5000: Loss = 2.1853121043976813 | Training Accuracy = 0.25\n",
            "Epoch 2300/5000: Loss = 2.183277677045359 | Training Accuracy = 0.25\n",
            "Epoch 2350/5000: Loss = 2.1812644767810068 | Training Accuracy = 0.25\n",
            "Epoch 2400/5000: Loss = 2.179272288398569 | Training Accuracy = 0.25\n",
            "Epoch 2450/5000: Loss = 2.1773008981549915 | Training Accuracy = 0.25\n",
            "Epoch 2500/5000: Loss = 2.1753500937776358 | Training Accuracy = 0.25\n",
            "Epoch 2550/5000: Loss = 2.17341966447131 | Training Accuracy = 0.25\n",
            "Epoch 2600/5000: Loss = 2.17150940092491 | Training Accuracy = 0.25\n",
            "Epoch 2650/5000: Loss = 2.1696190953176986 | Training Accuracy = 0.25\n",
            "Epoch 2700/5000: Loss = 2.167748541325187 | Training Accuracy = 0.25\n",
            "Epoch 2750/5000: Loss = 2.1658975341246545 | Training Accuracy = 0.25\n",
            "Epoch 2800/5000: Loss = 2.164065870400279 | Training Accuracy = 0.25\n",
            "Epoch 2850/5000: Loss = 2.162253348347906 | Training Accuracy = 0.25\n",
            "Epoch 2900/5000: Loss = 2.160459767679435 | Training Accuracy = 0.25\n",
            "Epoch 2950/5000: Loss = 2.158684929626848 | Training Accuracy = 0.25\n",
            "Epoch 3000/5000: Loss = 2.15692863694586 | Training Accuracy = 0.25\n",
            "Epoch 3050/5000: Loss = 2.1551906939192107 | Training Accuracy = 0.25\n",
            "Epoch 3100/5000: Loss = 2.1534709063596 | Training Accuracy = 0.25\n",
            "Epoch 3150/5000: Loss = 2.1517690816122523 | Training Accuracy = 0.25\n",
            "Epoch 3200/5000: Loss = 2.150085028557144 | Training Accuracy = 0.25\n",
            "Epoch 3250/5000: Loss = 2.1484185576108623 | Training Accuracy = 0.25\n",
            "Epoch 3300/5000: Loss = 2.1467694807281283 | Training Accuracy = 0.25\n",
            "Epoch 3350/5000: Loss = 2.1451376114029728 | Training Accuracy = 0.25\n",
            "Epoch 3400/5000: Loss = 2.143522764669574 | Training Accuracy = 0.25\n",
            "Epoch 3450/5000: Loss = 2.141924757102754 | Training Accuracy = 0.25\n",
            "Epoch 3500/5000: Loss = 2.1403434068181557 | Training Accuracy = 0.25\n",
            "Epoch 3550/5000: Loss = 2.13877853347208 | Training Accuracy = 0.25\n",
            "Epoch 3600/5000: Loss = 2.1372299582610106 | Training Accuracy = 0.25\n",
            "Epoch 3650/5000: Loss = 2.1356975039208175 | Training Accuracy = 0.25\n",
            "Epoch 3700/5000: Loss = 2.134180994725646 | Training Accuracy = 0.25\n",
            "Epoch 3750/5000: Loss = 2.1326802564865055 | Training Accuracy = 0.25\n",
            "Epoch 3800/5000: Loss = 2.131195116549546 | Training Accuracy = 0.25\n",
            "Epoch 3850/5000: Loss = 2.129725403794043 | Training Accuracy = 0.25\n",
            "Epoch 3900/5000: Loss = 2.1282709486300893 | Training Accuracy = 0.25\n",
            "Epoch 3950/5000: Loss = 2.1268315829959974 | Training Accuracy = 0.25\n",
            "Epoch 4000/5000: Loss = 2.1254071403554247 | Training Accuracy = 0.25\n",
            "Epoch 4050/5000: Loss = 2.123997455694217 | Training Accuracy = 0.25\n",
            "Epoch 4100/5000: Loss = 2.122602365516987 | Training Accuracy = 0.25\n",
            "Epoch 4150/5000: Loss = 2.1212217078434166 | Training Accuracy = 0.25\n",
            "Epoch 4200/5000: Loss = 2.119855322204317 | Training Accuracy = 0.25\n",
            "Epoch 4250/5000: Loss = 2.1185030496374178 | Training Accuracy = 0.25\n",
            "Epoch 4300/5000: Loss = 2.1171647326829155 | Training Accuracy = 0.25\n",
            "Epoch 4350/5000: Loss = 2.115840215378783 | Training Accuracy = 0.25\n",
            "Epoch 4400/5000: Loss = 2.114529343255831 | Training Accuracy = 0.25\n",
            "Epoch 4450/5000: Loss = 2.1132319633325483 | Training Accuracy = 0.25\n",
            "Epoch 4500/5000: Loss = 2.1119479241097108 | Training Accuracy = 0.25\n",
            "Epoch 4550/5000: Loss = 2.1106770755647766 | Training Accuracy = 0.25\n",
            "Epoch 4600/5000: Loss = 2.109419269146059 | Training Accuracy = 0.25\n",
            "Epoch 4650/5000: Loss = 2.1081743577666985 | Training Accuracy = 0.25\n",
            "Epoch 4700/5000: Loss = 2.106942195798428 | Training Accuracy = 0.25\n",
            "Epoch 4750/5000: Loss = 2.1057226390651436 | Training Accuracy = 0.25\n",
            "Epoch 4800/5000: Loss = 2.104515544836286 | Training Accuracy = 0.25\n",
            "Epoch 4850/5000: Loss = 2.103320771820031 | Training Accuracy = 0.25\n",
            "Epoch 4900/5000: Loss = 2.1021381801563073 | Training Accuracy = 0.25\n",
            "Epoch 4950/5000: Loss = 2.1009676314096373 | Training Accuracy = 0.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10077777777777777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnfM0EN2bi1F"
      },
      "source": [
        "**Loading the original dataset again**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo4q995YacYp",
        "outputId": "cf759594-541b-4dfb-a8ca-41a9e81dd1f8"
      },
      "source": [
        "# Read the h5 file\n",
        "h5_Vehicle = h5py.File('Part - 4 - Autonomous_Vehicles_SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, validation and test sets\n",
        "X_train = h5_Vehicle['X_train'][:]\n",
        "y_train_o = h5_Vehicle['y_train'][:]\n",
        "X_val = h5_Vehicle['X_val'][:]\n",
        "y_val_o = h5_Vehicle['y_val'][:]\n",
        "X_test = h5_Vehicle['X_test'][:]\n",
        "y_test_o = h5_Vehicle['y_test'][:]\n",
        "\n",
        "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
        "y_train = to_categorical(y_train_o)\n",
        "y_val = to_categorical(y_val_o)\n",
        "y_test = to_categorical(y_test_o)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
            "--------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "--------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "--------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIQuqXBvb6tA"
      },
      "source": [
        "**Start with small regularization and find learning rate that makes the loss go down.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAOZs2xFby0b",
        "outputId": "ae4a3dbf-0008-4fb1-85f1-70c6ed3d9ec5"
      },
      "source": [
        "lr = 1e-7\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.3100252019112637 | Training Accuracy = 0.10245238095238095\n",
            "Epoch 50/500: Loss = 2.3075806310263567 | Training Accuracy = 0.10207142857142858\n",
            "Epoch 100/500: Loss = 2.306031700087846 | Training Accuracy = 0.1015\n",
            "Epoch 150/500: Loss = 2.305042411579897 | Training Accuracy = 0.10019047619047619\n",
            "Epoch 200/500: Loss = 2.3044056325756435 | Training Accuracy = 0.09988095238095238\n",
            "Epoch 250/500: Loss = 2.3039921173807456 | Training Accuracy = 0.09854761904761905\n",
            "Epoch 300/500: Loss = 2.3037205595440673 | Training Accuracy = 0.09671428571428571\n",
            "Epoch 350/500: Loss = 2.303539522404581 | Training Accuracy = 0.09511904761904762\n",
            "Epoch 400/500: Loss = 2.3034163366739833 | Training Accuracy = 0.09416666666666666\n",
            "Epoch 450/500: Loss = 2.3033301932319805 | Training Accuracy = 0.09264285714285714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08572222222222223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTOj52Lib9uM",
        "outputId": "dc6d2ad0-636b-4077-d1aa-dfb5fbd92332"
      },
      "source": [
        "# Changing learning rate to 1e-3\n",
        "\n",
        "lr = 0.001\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.3054600951507305 | Training Accuracy = 0.11395238095238096\n",
            "Epoch 50/500: Loss = 2.259522161398282 | Training Accuracy = 0.19652380952380952\n",
            "Epoch 100/500: Loss = 2.2509994247963587 | Training Accuracy = 0.20873809523809525\n",
            "Epoch 150/500: Loss = 2.2466623930996477 | Training Accuracy = 0.2149047619047619\n",
            "Epoch 200/500: Loss = 2.2437567193893795 | Training Accuracy = 0.21921428571428572\n",
            "Epoch 250/500: Loss = 2.2415719552403033 | Training Accuracy = 0.22171428571428572\n",
            "Epoch 300/500: Loss = 2.2398229369841203 | Training Accuracy = 0.2241904761904762\n",
            "Epoch 350/500: Loss = 2.2383653065434563 | Training Accuracy = 0.22564285714285715\n",
            "Epoch 400/500: Loss = 2.237115482633667 | Training Accuracy = 0.22628571428571428\n",
            "Epoch 450/500: Loss = 2.236020749372444 | Training Accuracy = 0.22785714285714287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21066666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdM-urYOcStj"
      },
      "source": [
        "**Optimizing hyperparameter and running a deeper search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlEFFLu8cKHL",
        "outputId": "2559e00f-7a57-4299-a859-8e371d9a2d5f"
      },
      "source": [
        "import math\n",
        "for i in range(1, 10):\n",
        "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
        "    Lambda = math.pow(10, np.random.uniform(-5, 2))\n",
        "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
        "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(i, 10, best_acc, lr, Lambda))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Try 1/10: Best_val_acc: 0.183, lr: 0.006194881356749269, Lambda: 3.0674849905942456e-05\n",
            "\n",
            "Try 2/10: Best_val_acc: 0.204, lr: 0.005446364406380019, Lambda: 51.87240826024816\n",
            "\n",
            "Try 3/10: Best_val_acc: 0.2088888888888889, lr: 0.0016522382707245898, Lambda: 0.1939116277632319\n",
            "\n",
            "Try 4/10: Best_val_acc: 0.19822222222222222, lr: 0.0017057627561567893, Lambda: 2.518863723902807\n",
            "\n",
            "Try 5/10: Best_val_acc: 0.19066666666666668, lr: 0.005152224038963205, Lambda: 1.2143041794020415e-05\n",
            "\n",
            "Try 6/10: Best_val_acc: 0.20827777777777778, lr: 0.00921935228718037, Lambda: 0.0004350800957907299\n",
            "\n",
            "Try 7/10: Best_val_acc: 0.18577777777777776, lr: 0.00878261288169845, Lambda: 0.0035111612834760712\n",
            "\n",
            "Try 8/10: Best_val_acc: 0.19533333333333333, lr: 0.006090543136815944, Lambda: 13.609433466281665\n",
            "\n",
            "Try 9/10: Best_val_acc: 0.18994444444444444, lr: 0.007509709326301158, Lambda: 2.3352940673188255e-05\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTRo6SxPcpj2"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "Best accuracy achieved using this method after hyperparameter optimization: ~21%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rU6AE4c-bu"
      },
      "source": [
        "## **Modelling - Neural Network**\n",
        "\n",
        "**NN model, Sigmoid activation functions, SGD optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiMbYU5vchKU",
        "outputId": "82050953-75be-412f-de22-02148edaf1ef"
      },
      "source": [
        "print('NN model with sigmoid activations'); print('----'*20)\n",
        "# Initialize the neural network classifier\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions sigmoid\n",
        "model.add(Dense(128, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model.add(Dense(64))\n",
        "# Adding activation function\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v77mS5DldSoJ",
        "outputId": "6b0e5d0b-562c-4f5b-dc41-c2813c5eb8e0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClKQXxSKdVZr",
        "outputId": "bf66aab0-0607-41f1-d8f6-0732f98336da"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 12ms/step - loss: 2.3747 - accuracy: 0.1006 - val_loss: 2.3027 - val_accuracy: 0.1032\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3030 - accuracy: 0.1005 - val_loss: 2.3028 - val_accuracy: 0.1006\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3030 - accuracy: 0.1003 - val_loss: 2.3028 - val_accuracy: 0.1019\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3029 - accuracy: 0.0995 - val_loss: 2.3026 - val_accuracy: 0.1011\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3028 - accuracy: 0.1025 - val_loss: 2.3027 - val_accuracy: 0.0993\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3028 - accuracy: 0.0998 - val_loss: 2.3025 - val_accuracy: 0.1060\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 2.3027 - accuracy: 0.1037 - val_loss: 2.3025 - val_accuracy: 0.1028\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3025 - accuracy: 0.1040 - val_loss: 2.3024 - val_accuracy: 0.1097\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3024 - accuracy: 0.1096 - val_loss: 2.3024 - val_accuracy: 0.1068\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3023 - accuracy: 0.1051 - val_loss: 2.3025 - val_accuracy: 0.1048\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3026 - accuracy: 0.1023 - val_loss: 2.3027 - val_accuracy: 0.1076\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3021 - accuracy: 0.1088 - val_loss: 2.3022 - val_accuracy: 0.1039\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3023 - accuracy: 0.1063 - val_loss: 2.3022 - val_accuracy: 0.1130\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3022 - accuracy: 0.1088 - val_loss: 2.3021 - val_accuracy: 0.1049\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3023 - accuracy: 0.1045 - val_loss: 2.3023 - val_accuracy: 0.1038\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3024 - accuracy: 0.1042 - val_loss: 2.3023 - val_accuracy: 0.1017\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3023 - accuracy: 0.1081 - val_loss: 2.3022 - val_accuracy: 0.1137\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3020 - accuracy: 0.1099 - val_loss: 2.3021 - val_accuracy: 0.1041\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3020 - accuracy: 0.1075 - val_loss: 2.3020 - val_accuracy: 0.1096\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3021 - accuracy: 0.1062 - val_loss: 2.3020 - val_accuracy: 0.1116\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3020 - accuracy: 0.1100 - val_loss: 2.3019 - val_accuracy: 0.1041\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3018 - accuracy: 0.1033 - val_loss: 2.3020 - val_accuracy: 0.1163\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3021 - accuracy: 0.1114 - val_loss: 2.3018 - val_accuracy: 0.1148\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3019 - accuracy: 0.1076 - val_loss: 2.3018 - val_accuracy: 0.1087\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3018 - accuracy: 0.1097 - val_loss: 2.3018 - val_accuracy: 0.1031\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3020 - accuracy: 0.1091 - val_loss: 2.3017 - val_accuracy: 0.1125\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3017 - accuracy: 0.1093 - val_loss: 2.3016 - val_accuracy: 0.1111\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3017 - accuracy: 0.1113 - val_loss: 2.3017 - val_accuracy: 0.1091\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3018 - accuracy: 0.1096 - val_loss: 2.3018 - val_accuracy: 0.1021\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3015 - accuracy: 0.1106 - val_loss: 2.3016 - val_accuracy: 0.1130\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3016 - accuracy: 0.1111 - val_loss: 2.3015 - val_accuracy: 0.1068\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3016 - accuracy: 0.1091 - val_loss: 2.3015 - val_accuracy: 0.1090\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3017 - accuracy: 0.1093 - val_loss: 2.3015 - val_accuracy: 0.1131\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3018 - accuracy: 0.1118 - val_loss: 2.3015 - val_accuracy: 0.1143\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3015 - accuracy: 0.1138 - val_loss: 2.3014 - val_accuracy: 0.1084\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 2.3014 - accuracy: 0.1091 - val_loss: 2.3015 - val_accuracy: 0.1060\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3015 - accuracy: 0.1098 - val_loss: 2.3013 - val_accuracy: 0.1225\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3013 - accuracy: 0.1156 - val_loss: 2.3014 - val_accuracy: 0.1104\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3015 - accuracy: 0.1113 - val_loss: 2.3012 - val_accuracy: 0.1089\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3013 - accuracy: 0.1135 - val_loss: 2.3013 - val_accuracy: 0.1163\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3011 - accuracy: 0.1129 - val_loss: 2.3011 - val_accuracy: 0.1194\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3011 - accuracy: 0.1203 - val_loss: 2.3011 - val_accuracy: 0.1261\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3013 - accuracy: 0.1177 - val_loss: 2.3012 - val_accuracy: 0.1149\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3011 - accuracy: 0.1153 - val_loss: 2.3011 - val_accuracy: 0.1160\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3011 - accuracy: 0.1142 - val_loss: 2.3010 - val_accuracy: 0.1193\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3010 - accuracy: 0.1185 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3011 - accuracy: 0.1138 - val_loss: 2.3011 - val_accuracy: 0.1118\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3010 - accuracy: 0.1175 - val_loss: 2.3009 - val_accuracy: 0.1193\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3010 - accuracy: 0.1179 - val_loss: 2.3009 - val_accuracy: 0.1196\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3009 - accuracy: 0.1198 - val_loss: 2.3008 - val_accuracy: 0.1278\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3007 - accuracy: 0.1262 - val_loss: 2.3009 - val_accuracy: 0.1211\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3010 - accuracy: 0.1159 - val_loss: 2.3008 - val_accuracy: 0.1262\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3008 - accuracy: 0.1183 - val_loss: 2.3009 - val_accuracy: 0.1168\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3009 - accuracy: 0.1208 - val_loss: 2.3009 - val_accuracy: 0.1118\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3009 - accuracy: 0.1161 - val_loss: 2.3008 - val_accuracy: 0.1151\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3009 - accuracy: 0.1154 - val_loss: 2.3008 - val_accuracy: 0.1181\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3009 - accuracy: 0.1164 - val_loss: 2.3006 - val_accuracy: 0.1358\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3007 - accuracy: 0.1232 - val_loss: 2.3007 - val_accuracy: 0.1190\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3008 - accuracy: 0.1218 - val_loss: 2.3006 - val_accuracy: 0.1115\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3007 - accuracy: 0.1152 - val_loss: 2.3005 - val_accuracy: 0.1307\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3004 - accuracy: 0.1228 - val_loss: 2.3005 - val_accuracy: 0.1320\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3005 - accuracy: 0.1265 - val_loss: 2.3006 - val_accuracy: 0.1233\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3005 - accuracy: 0.1213 - val_loss: 2.3005 - val_accuracy: 0.1299\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3004 - accuracy: 0.1262 - val_loss: 2.3006 - val_accuracy: 0.1145\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3005 - accuracy: 0.1235 - val_loss: 2.3004 - val_accuracy: 0.1319\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3005 - accuracy: 0.1218 - val_loss: 2.3004 - val_accuracy: 0.1331\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3008 - accuracy: 0.1189 - val_loss: 2.3004 - val_accuracy: 0.1199\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3005 - accuracy: 0.1236 - val_loss: 2.3003 - val_accuracy: 0.1344\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3003 - accuracy: 0.1216 - val_loss: 2.3003 - val_accuracy: 0.1412\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3004 - accuracy: 0.1317 - val_loss: 2.3002 - val_accuracy: 0.1169\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3003 - accuracy: 0.1171 - val_loss: 2.3002 - val_accuracy: 0.1291\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3004 - accuracy: 0.1251 - val_loss: 2.3001 - val_accuracy: 0.1216\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3004 - accuracy: 0.1221 - val_loss: 2.3001 - val_accuracy: 0.1256\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3003 - accuracy: 0.1267 - val_loss: 2.3001 - val_accuracy: 0.1174\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3003 - accuracy: 0.1195 - val_loss: 2.3000 - val_accuracy: 0.1288\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.3000 - accuracy: 0.1251 - val_loss: 2.3002 - val_accuracy: 0.1236\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3004 - accuracy: 0.1238 - val_loss: 2.3000 - val_accuracy: 0.1255\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3003 - accuracy: 0.1303 - val_loss: 2.2999 - val_accuracy: 0.1260\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2998 - accuracy: 0.1248 - val_loss: 2.2999 - val_accuracy: 0.1429\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3002 - accuracy: 0.1343 - val_loss: 2.2998 - val_accuracy: 0.1285\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.3001 - accuracy: 0.1280 - val_loss: 2.2998 - val_accuracy: 0.1279\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2998 - accuracy: 0.1174 - val_loss: 2.3000 - val_accuracy: 0.1258\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2999 - accuracy: 0.1301 - val_loss: 2.2998 - val_accuracy: 0.1241\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2997 - accuracy: 0.1294 - val_loss: 2.2997 - val_accuracy: 0.1276\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2998 - accuracy: 0.1278 - val_loss: 2.2997 - val_accuracy: 0.1187\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2999 - accuracy: 0.1231 - val_loss: 2.2997 - val_accuracy: 0.1394\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2999 - accuracy: 0.1305 - val_loss: 2.2998 - val_accuracy: 0.1219\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2998 - accuracy: 0.1308 - val_loss: 2.2996 - val_accuracy: 0.1315\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2993 - accuracy: 0.1363 - val_loss: 2.2996 - val_accuracy: 0.1100\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2997 - accuracy: 0.1205 - val_loss: 2.2995 - val_accuracy: 0.1472\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2996 - accuracy: 0.1379 - val_loss: 2.2996 - val_accuracy: 0.1285\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2996 - accuracy: 0.1334 - val_loss: 2.2994 - val_accuracy: 0.1283\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2996 - accuracy: 0.1360 - val_loss: 2.2994 - val_accuracy: 0.1297\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2995 - accuracy: 0.1273 - val_loss: 2.2995 - val_accuracy: 0.1405\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2995 - accuracy: 0.1301 - val_loss: 2.2994 - val_accuracy: 0.1409\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2995 - accuracy: 0.1307 - val_loss: 2.2993 - val_accuracy: 0.1452\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2996 - accuracy: 0.1296 - val_loss: 2.2993 - val_accuracy: 0.1268\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2992 - accuracy: 0.1299 - val_loss: 2.2992 - val_accuracy: 0.1282\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2991 - accuracy: 0.1319 - val_loss: 2.2993 - val_accuracy: 0.1341\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2991 - accuracy: 0.1300 - val_loss: 2.2991 - val_accuracy: 0.1480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWS4LAF0fBo9",
        "outputId": "669f7dae-aab7-4686-ee38-c576b021f61a"
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations'); print('----'*20)\n",
        "results1 = model.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.2991 - accuracy: 0.1480\n",
            "Validation accuracy: 14.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5GcygiIfVl9"
      },
      "source": [
        "**NN model, Sigmoid activation functions, SGD optimizer: Changing Learning Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWMHoxiZfO8W",
        "outputId": "1f88f71d-5288-479d-9e9f-e191b06ef68d"
      },
      "source": [
        "print('NN model with sigmoid activations - changing learning rate'); print('----'*20)\n",
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.001)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 11ms/step - loss: 2.2990 - accuracy: 0.1490 - val_loss: 2.2991 - val_accuracy: 0.1528\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1517 - val_loss: 2.2991 - val_accuracy: 0.1496\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1443 - val_loss: 2.2990 - val_accuracy: 0.1476\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1463 - val_loss: 2.2990 - val_accuracy: 0.1460\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1481 - val_loss: 2.2990 - val_accuracy: 0.1455\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1476 - val_loss: 2.2990 - val_accuracy: 0.1445\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1452 - val_loss: 2.2990 - val_accuracy: 0.1435\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1470 - val_loss: 2.2990 - val_accuracy: 0.1422\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1456 - val_loss: 2.2990 - val_accuracy: 0.1439\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1469 - val_loss: 2.2990 - val_accuracy: 0.1437\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1427 - val_loss: 2.2990 - val_accuracy: 0.1436\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1432 - val_loss: 2.2990 - val_accuracy: 0.1442\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1448 - val_loss: 2.2990 - val_accuracy: 0.1438\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1480 - val_loss: 2.2990 - val_accuracy: 0.1436\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1399 - val_loss: 2.2990 - val_accuracy: 0.1448\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1457 - val_loss: 2.2990 - val_accuracy: 0.1441\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1456 - val_loss: 2.2990 - val_accuracy: 0.1445\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1484 - val_loss: 2.2990 - val_accuracy: 0.1448\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1474 - val_loss: 2.2990 - val_accuracy: 0.1447\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1456 - val_loss: 2.2990 - val_accuracy: 0.1443\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1486 - val_loss: 2.2990 - val_accuracy: 0.1443\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1505 - val_loss: 2.2990 - val_accuracy: 0.1439\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1467 - val_loss: 2.2990 - val_accuracy: 0.1430\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1445 - val_loss: 2.2990 - val_accuracy: 0.1429\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1487 - val_loss: 2.2990 - val_accuracy: 0.1436\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1426 - val_loss: 2.2990 - val_accuracy: 0.1448\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1480 - val_loss: 2.2989 - val_accuracy: 0.1448\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1465 - val_loss: 2.2989 - val_accuracy: 0.1441\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1459 - val_loss: 2.2989 - val_accuracy: 0.1446\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1470 - val_loss: 2.2989 - val_accuracy: 0.1447\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1396 - val_loss: 2.2989 - val_accuracy: 0.1460\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2992 - accuracy: 0.1455 - val_loss: 2.2989 - val_accuracy: 0.1459\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1477 - val_loss: 2.2989 - val_accuracy: 0.1448\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1456 - val_loss: 2.2989 - val_accuracy: 0.1453\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1456 - val_loss: 2.2989 - val_accuracy: 0.1436\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1415 - val_loss: 2.2989 - val_accuracy: 0.1449\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1486 - val_loss: 2.2989 - val_accuracy: 0.1439\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1452 - val_loss: 2.2989 - val_accuracy: 0.1450\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1478 - val_loss: 2.2989 - val_accuracy: 0.1450\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1474 - val_loss: 2.2989 - val_accuracy: 0.1459\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1449 - val_loss: 2.2989 - val_accuracy: 0.1464\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1514 - val_loss: 2.2989 - val_accuracy: 0.1451\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1482 - val_loss: 2.2989 - val_accuracy: 0.1435\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1475 - val_loss: 2.2989 - val_accuracy: 0.1440\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1494 - val_loss: 2.2989 - val_accuracy: 0.1431\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1421 - val_loss: 2.2989 - val_accuracy: 0.1457\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1486 - val_loss: 2.2989 - val_accuracy: 0.1452\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1487 - val_loss: 2.2989 - val_accuracy: 0.1433\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1442 - val_loss: 2.2989 - val_accuracy: 0.1436\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2990 - accuracy: 0.1420 - val_loss: 2.2989 - val_accuracy: 0.1451\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1467 - val_loss: 2.2989 - val_accuracy: 0.1452\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2990 - accuracy: 0.1476 - val_loss: 2.2989 - val_accuracy: 0.1441\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1422 - val_loss: 2.2988 - val_accuracy: 0.1454\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1462 - val_loss: 2.2988 - val_accuracy: 0.1450\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2988 - accuracy: 0.1459 - val_loss: 2.2988 - val_accuracy: 0.1450\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1479 - val_loss: 2.2988 - val_accuracy: 0.1426\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1451 - val_loss: 2.2988 - val_accuracy: 0.1420\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1437 - val_loss: 2.2988 - val_accuracy: 0.1436\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1447 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1440 - val_loss: 2.2988 - val_accuracy: 0.1455\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1408 - val_loss: 2.2988 - val_accuracy: 0.1462\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1509 - val_loss: 2.2988 - val_accuracy: 0.1453\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1485 - val_loss: 2.2988 - val_accuracy: 0.1446\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2989 - accuracy: 0.1431 - val_loss: 2.2988 - val_accuracy: 0.1450\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1447 - val_loss: 2.2988 - val_accuracy: 0.1455\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1496 - val_loss: 2.2988 - val_accuracy: 0.1456\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1447 - val_loss: 2.2988 - val_accuracy: 0.1460\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1517 - val_loss: 2.2988 - val_accuracy: 0.1446\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1478 - val_loss: 2.2988 - val_accuracy: 0.1442\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1447 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1452 - val_loss: 2.2988 - val_accuracy: 0.1456\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 2.2985 - accuracy: 0.1476 - val_loss: 2.2988 - val_accuracy: 0.1468\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1466 - val_loss: 2.2988 - val_accuracy: 0.1470\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1475 - val_loss: 2.2988 - val_accuracy: 0.1476\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1519 - val_loss: 2.2988 - val_accuracy: 0.1455\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1427 - val_loss: 2.2987 - val_accuracy: 0.1477\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1510 - val_loss: 2.2987 - val_accuracy: 0.1460\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1443 - val_loss: 2.2987 - val_accuracy: 0.1463\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1455 - val_loss: 2.2987 - val_accuracy: 0.1449\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1459 - val_loss: 2.2987 - val_accuracy: 0.1460\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2988 - accuracy: 0.1467 - val_loss: 2.2987 - val_accuracy: 0.1465\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1483 - val_loss: 2.2987 - val_accuracy: 0.1473\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1443 - val_loss: 2.2987 - val_accuracy: 0.1482\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1490 - val_loss: 2.2987 - val_accuracy: 0.1479\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1473 - val_loss: 2.2987 - val_accuracy: 0.1473\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1485 - val_loss: 2.2987 - val_accuracy: 0.1470\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1480 - val_loss: 2.2987 - val_accuracy: 0.1466\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2983 - accuracy: 0.1495 - val_loss: 2.2987 - val_accuracy: 0.1451\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1450 - val_loss: 2.2987 - val_accuracy: 0.1456\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2985 - accuracy: 0.1489 - val_loss: 2.2987 - val_accuracy: 0.1453\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1511 - val_loss: 2.2987 - val_accuracy: 0.1441\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1454 - val_loss: 2.2987 - val_accuracy: 0.1444\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1469 - val_loss: 2.2987 - val_accuracy: 0.1454\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1462 - val_loss: 2.2987 - val_accuracy: 0.1459\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1495 - val_loss: 2.2987 - val_accuracy: 0.1458\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1439 - val_loss: 2.2987 - val_accuracy: 0.1472\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2987 - accuracy: 0.1473 - val_loss: 2.2987 - val_accuracy: 0.1466\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1477 - val_loss: 2.2987 - val_accuracy: 0.1457\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2984 - accuracy: 0.1487 - val_loss: 2.2987 - val_accuracy: 0.1460\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2986 - accuracy: 0.1502 - val_loss: 2.2987 - val_accuracy: 0.1445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNyMqalpfjzf",
        "outputId": "a3c06926-a7f6-4179-fba0-880fdd3c9811"
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
        "results1 = model.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.2987 - accuracy: 0.1445\n",
            "Validation accuracy: 14.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ElrYTDzuWIg"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   Validation score is very low, changing learning rate further reduces it.\n",
        "*   Optimizing the network in order to better learn the patterns in the dataset.\n",
        "*   Best model out of the above is the one with lower learning rate using SGD optimizer and sigmoid activations.\n",
        "\n",
        "Let's use ReLU activations and see if the score improves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33hcnoYZfxYw",
        "outputId": "f9e4fbe8-be51-420b-cbcb-0f2ba01c44c5"
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and sgd optimizers'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model2 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model2.add(Dense(128, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model2.add(Dense(64))\n",
        "# Adding activation function\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model2.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model2.add(Activation('softmax'))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 Âµs, sys: 2 Âµs, total: 4 Âµs\n",
            "Wall time: 6.91 Âµs\n",
            "NN model with relu activations and sgd optimizers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1pN4IN-uyrH",
        "outputId": "ef6c77d1-352a-4e52-cb3d-61efab0b815d"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqqxZWsJu4oE",
        "outputId": "fe3f1813-cb55-443e-d6e0-b18eaf088f3a"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 11ms/step - loss: 2.3087 - accuracy: 0.1109 - val_loss: 2.2927 - val_accuracy: 0.1367\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2904 - accuracy: 0.1413 - val_loss: 2.2816 - val_accuracy: 0.1444\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2793 - accuracy: 0.1625 - val_loss: 2.2703 - val_accuracy: 0.1745\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2673 - accuracy: 0.1877 - val_loss: 2.2549 - val_accuracy: 0.2130\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 2.2507 - accuracy: 0.2151 - val_loss: 2.2376 - val_accuracy: 0.2480\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2340 - accuracy: 0.2443 - val_loss: 2.2171 - val_accuracy: 0.2613\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.2107 - accuracy: 0.2687 - val_loss: 2.1904 - val_accuracy: 0.2929\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.1842 - accuracy: 0.2992 - val_loss: 2.1569 - val_accuracy: 0.3244\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.1492 - accuracy: 0.3238 - val_loss: 2.1177 - val_accuracy: 0.3527\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.1073 - accuracy: 0.3552 - val_loss: 2.0707 - val_accuracy: 0.3692\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.0602 - accuracy: 0.3735 - val_loss: 2.0190 - val_accuracy: 0.3600\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 2.0047 - accuracy: 0.3942 - val_loss: 1.9553 - val_accuracy: 0.4248\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.9415 - accuracy: 0.4252 - val_loss: 1.8918 - val_accuracy: 0.4531\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.8783 - accuracy: 0.4493 - val_loss: 1.8263 - val_accuracy: 0.4716\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.8106 - accuracy: 0.4732 - val_loss: 1.7648 - val_accuracy: 0.4876\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.7501 - accuracy: 0.4900 - val_loss: 1.6997 - val_accuracy: 0.5102\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.6858 - accuracy: 0.5132 - val_loss: 1.6406 - val_accuracy: 0.5429\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.6311 - accuracy: 0.5343 - val_loss: 1.5936 - val_accuracy: 0.5467\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.5787 - accuracy: 0.5478 - val_loss: 1.5486 - val_accuracy: 0.5555\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.5374 - accuracy: 0.5642 - val_loss: 1.5152 - val_accuracy: 0.5559\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 1.4915 - accuracy: 0.5814 - val_loss: 1.4586 - val_accuracy: 0.5875\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.4524 - accuracy: 0.5852 - val_loss: 1.4152 - val_accuracy: 0.6000\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.4043 - accuracy: 0.5988 - val_loss: 1.4039 - val_accuracy: 0.5937\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.3740 - accuracy: 0.6065 - val_loss: 1.3466 - val_accuracy: 0.6116\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.3432 - accuracy: 0.6157 - val_loss: 1.3244 - val_accuracy: 0.6164\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.3168 - accuracy: 0.6219 - val_loss: 1.2851 - val_accuracy: 0.6332\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2946 - accuracy: 0.6303 - val_loss: 1.2564 - val_accuracy: 0.6387\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2705 - accuracy: 0.6343 - val_loss: 1.2417 - val_accuracy: 0.6376\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2486 - accuracy: 0.6359 - val_loss: 1.2241 - val_accuracy: 0.6462\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2152 - accuracy: 0.6480 - val_loss: 1.1941 - val_accuracy: 0.6576\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2056 - accuracy: 0.6481 - val_loss: 1.1768 - val_accuracy: 0.6607\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1874 - accuracy: 0.6584 - val_loss: 1.2612 - val_accuracy: 0.6216\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1779 - accuracy: 0.6569 - val_loss: 1.1660 - val_accuracy: 0.6548\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1511 - accuracy: 0.6642 - val_loss: 1.1378 - val_accuracy: 0.6699\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1386 - accuracy: 0.6679 - val_loss: 1.1188 - val_accuracy: 0.6759\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1324 - accuracy: 0.6709 - val_loss: 1.1168 - val_accuracy: 0.6672\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1062 - accuracy: 0.6725 - val_loss: 1.1122 - val_accuracy: 0.6668\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1149 - accuracy: 0.6713 - val_loss: 1.0973 - val_accuracy: 0.6775\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0878 - accuracy: 0.6818 - val_loss: 1.0586 - val_accuracy: 0.6937\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0751 - accuracy: 0.6867 - val_loss: 1.0557 - val_accuracy: 0.6945\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0604 - accuracy: 0.6895 - val_loss: 1.0715 - val_accuracy: 0.6855\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0552 - accuracy: 0.6889 - val_loss: 1.0998 - val_accuracy: 0.6715\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0411 - accuracy: 0.6937 - val_loss: 1.0342 - val_accuracy: 0.6978\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0343 - accuracy: 0.6943 - val_loss: 1.0307 - val_accuracy: 0.6958\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0268 - accuracy: 0.6954 - val_loss: 1.0410 - val_accuracy: 0.6917\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0168 - accuracy: 0.7018 - val_loss: 1.1079 - val_accuracy: 0.6533\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0103 - accuracy: 0.6986 - val_loss: 1.0031 - val_accuracy: 0.7046\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0061 - accuracy: 0.7034 - val_loss: 1.0023 - val_accuracy: 0.7047\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9870 - accuracy: 0.7101 - val_loss: 0.9824 - val_accuracy: 0.7103\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9877 - accuracy: 0.7040 - val_loss: 0.9895 - val_accuracy: 0.7064\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9827 - accuracy: 0.7081 - val_loss: 1.0047 - val_accuracy: 0.7008\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9699 - accuracy: 0.7136 - val_loss: 0.9562 - val_accuracy: 0.7183\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9656 - accuracy: 0.7118 - val_loss: 0.9484 - val_accuracy: 0.7218\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9450 - accuracy: 0.7187 - val_loss: 0.9413 - val_accuracy: 0.7232\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9504 - accuracy: 0.7168 - val_loss: 0.9304 - val_accuracy: 0.7258\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9508 - accuracy: 0.7184 - val_loss: 0.9373 - val_accuracy: 0.7232\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9326 - accuracy: 0.7226 - val_loss: 0.9322 - val_accuracy: 0.7254\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9159 - accuracy: 0.7286 - val_loss: 0.9128 - val_accuracy: 0.7313\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9143 - accuracy: 0.7276 - val_loss: 0.9101 - val_accuracy: 0.7312\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9116 - accuracy: 0.7334 - val_loss: 0.9031 - val_accuracy: 0.7352\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9139 - accuracy: 0.7269 - val_loss: 0.9236 - val_accuracy: 0.7253\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8991 - accuracy: 0.7320 - val_loss: 0.8918 - val_accuracy: 0.7378\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8891 - accuracy: 0.7363 - val_loss: 0.8996 - val_accuracy: 0.7328\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8951 - accuracy: 0.7360 - val_loss: 0.8895 - val_accuracy: 0.7360\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8920 - accuracy: 0.7371 - val_loss: 0.9079 - val_accuracy: 0.7281\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8655 - accuracy: 0.7429 - val_loss: 0.8707 - val_accuracy: 0.7429\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8791 - accuracy: 0.7379 - val_loss: 0.8589 - val_accuracy: 0.7487\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8675 - accuracy: 0.7405 - val_loss: 0.8787 - val_accuracy: 0.7398\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8653 - accuracy: 0.7433 - val_loss: 0.8632 - val_accuracy: 0.7448\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8556 - accuracy: 0.7488 - val_loss: 0.8660 - val_accuracy: 0.7438\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8496 - accuracy: 0.7478 - val_loss: 0.8471 - val_accuracy: 0.7498\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8459 - accuracy: 0.7495 - val_loss: 0.8582 - val_accuracy: 0.7453\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8309 - accuracy: 0.7555 - val_loss: 0.8845 - val_accuracy: 0.7358\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8389 - accuracy: 0.7511 - val_loss: 0.8604 - val_accuracy: 0.7458\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8295 - accuracy: 0.7566 - val_loss: 0.8347 - val_accuracy: 0.7549\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8250 - accuracy: 0.7582 - val_loss: 0.8367 - val_accuracy: 0.7522\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8302 - accuracy: 0.7531 - val_loss: 0.8108 - val_accuracy: 0.7635\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8163 - accuracy: 0.7588 - val_loss: 0.8302 - val_accuracy: 0.7537\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8183 - accuracy: 0.7561 - val_loss: 0.8162 - val_accuracy: 0.7597\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8012 - accuracy: 0.7647 - val_loss: 0.8024 - val_accuracy: 0.7652\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8063 - accuracy: 0.7584 - val_loss: 0.8146 - val_accuracy: 0.7601\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8015 - accuracy: 0.7647 - val_loss: 0.8042 - val_accuracy: 0.7631\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7882 - accuracy: 0.7675 - val_loss: 0.8773 - val_accuracy: 0.7303\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7840 - accuracy: 0.7676 - val_loss: 0.8141 - val_accuracy: 0.7595\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7927 - accuracy: 0.7656 - val_loss: 0.7815 - val_accuracy: 0.7711\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7905 - accuracy: 0.7659 - val_loss: 0.8179 - val_accuracy: 0.7566\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7809 - accuracy: 0.7666 - val_loss: 0.7757 - val_accuracy: 0.7726\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7632 - accuracy: 0.7757 - val_loss: 0.8120 - val_accuracy: 0.7554\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7718 - accuracy: 0.7714 - val_loss: 0.7794 - val_accuracy: 0.7703\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7714 - accuracy: 0.7696 - val_loss: 0.7675 - val_accuracy: 0.7763\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7583 - accuracy: 0.7753 - val_loss: 0.7720 - val_accuracy: 0.7724\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7632 - accuracy: 0.7724 - val_loss: 0.7796 - val_accuracy: 0.7703\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7584 - accuracy: 0.7747 - val_loss: 0.7849 - val_accuracy: 0.7680\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7500 - accuracy: 0.7787 - val_loss: 0.7667 - val_accuracy: 0.7749\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7421 - accuracy: 0.7799 - val_loss: 0.7656 - val_accuracy: 0.7746\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7500 - accuracy: 0.7788 - val_loss: 0.7434 - val_accuracy: 0.7831\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7489 - accuracy: 0.7798 - val_loss: 0.7574 - val_accuracy: 0.7766\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7383 - accuracy: 0.7800 - val_loss: 0.7401 - val_accuracy: 0.7840\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7324 - accuracy: 0.7833 - val_loss: 0.7521 - val_accuracy: 0.7787\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7301 - accuracy: 0.7828 - val_loss: 0.7518 - val_accuracy: 0.7784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN1w9V3du926",
        "outputId": "1d6cc4d3-82d2-44fa-ef47-e71362b34534"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7518 - accuracy: 0.7784\n",
            "Validation accuracy: 77.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC3LqPtHv2Xk"
      },
      "source": [
        "**NN model, ReLU activations, SGD optimizer: Changing Learning Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHqUccsVvA5H",
        "outputId": "6b2b2062-7b6c-4ace-e822-ddfdaf519ee8"
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and sgd optimizers - changing learning rate'); print('--'*40)\n",
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.001)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 Âµs, sys: 1e+03 ns, total: 4 Âµs\n",
            "Wall time: 10.3 Âµs\n",
            "NN model with relu activations and sgd optimizers - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 11ms/step - loss: 0.6992 - accuracy: 0.7966 - val_loss: 0.7166 - val_accuracy: 0.7931\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6903 - accuracy: 0.8008 - val_loss: 0.7162 - val_accuracy: 0.7931\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6970 - accuracy: 0.7976 - val_loss: 0.7156 - val_accuracy: 0.7937\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6938 - accuracy: 0.8003 - val_loss: 0.7150 - val_accuracy: 0.7934\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7019 - accuracy: 0.7970 - val_loss: 0.7152 - val_accuracy: 0.7935\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6912 - accuracy: 0.8010 - val_loss: 0.7152 - val_accuracy: 0.7929\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6932 - accuracy: 0.7986 - val_loss: 0.7139 - val_accuracy: 0.7941\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6854 - accuracy: 0.8019 - val_loss: 0.7130 - val_accuracy: 0.7943\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6960 - accuracy: 0.8016 - val_loss: 0.7132 - val_accuracy: 0.7941\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.6938 - accuracy: 0.7987 - val_loss: 0.7129 - val_accuracy: 0.7939\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6924 - accuracy: 0.8010 - val_loss: 0.7132 - val_accuracy: 0.7936\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6855 - accuracy: 0.8005 - val_loss: 0.7126 - val_accuracy: 0.7938\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6920 - accuracy: 0.7966 - val_loss: 0.7115 - val_accuracy: 0.7944\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6951 - accuracy: 0.7974 - val_loss: 0.7114 - val_accuracy: 0.7942\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6910 - accuracy: 0.8005 - val_loss: 0.7118 - val_accuracy: 0.7945\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6861 - accuracy: 0.8007 - val_loss: 0.7107 - val_accuracy: 0.7942\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6894 - accuracy: 0.8006 - val_loss: 0.7102 - val_accuracy: 0.7956\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6870 - accuracy: 0.7988 - val_loss: 0.7111 - val_accuracy: 0.7941\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6934 - accuracy: 0.7994 - val_loss: 0.7092 - val_accuracy: 0.7954\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6919 - accuracy: 0.7989 - val_loss: 0.7102 - val_accuracy: 0.7952\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6811 - accuracy: 0.8000 - val_loss: 0.7089 - val_accuracy: 0.7961\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6776 - accuracy: 0.8013 - val_loss: 0.7089 - val_accuracy: 0.7959\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6799 - accuracy: 0.8017 - val_loss: 0.7095 - val_accuracy: 0.7956\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6920 - accuracy: 0.8024 - val_loss: 0.7080 - val_accuracy: 0.7958\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6856 - accuracy: 0.8002 - val_loss: 0.7077 - val_accuracy: 0.7948\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6825 - accuracy: 0.8010 - val_loss: 0.7075 - val_accuracy: 0.7965\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6843 - accuracy: 0.7998 - val_loss: 0.7064 - val_accuracy: 0.7962\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6826 - accuracy: 0.8014 - val_loss: 0.7067 - val_accuracy: 0.7957\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6791 - accuracy: 0.8010 - val_loss: 0.7071 - val_accuracy: 0.7949\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6830 - accuracy: 0.8026 - val_loss: 0.7063 - val_accuracy: 0.7962\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6741 - accuracy: 0.8039 - val_loss: 0.7058 - val_accuracy: 0.7961\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6815 - accuracy: 0.8014 - val_loss: 0.7059 - val_accuracy: 0.7952\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6802 - accuracy: 0.8026 - val_loss: 0.7059 - val_accuracy: 0.7959\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6862 - accuracy: 0.7996 - val_loss: 0.7045 - val_accuracy: 0.7964\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6781 - accuracy: 0.8017 - val_loss: 0.7042 - val_accuracy: 0.7967\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6818 - accuracy: 0.8019 - val_loss: 0.7049 - val_accuracy: 0.7970\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6807 - accuracy: 0.8019 - val_loss: 0.7033 - val_accuracy: 0.7964\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6807 - accuracy: 0.8013 - val_loss: 0.7044 - val_accuracy: 0.7959\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6822 - accuracy: 0.8018 - val_loss: 0.7029 - val_accuracy: 0.7969\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6699 - accuracy: 0.8060 - val_loss: 0.7031 - val_accuracy: 0.7970\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6787 - accuracy: 0.8020 - val_loss: 0.7026 - val_accuracy: 0.7969\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6774 - accuracy: 0.8042 - val_loss: 0.7025 - val_accuracy: 0.7974\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6817 - accuracy: 0.8019 - val_loss: 0.7018 - val_accuracy: 0.7977\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6856 - accuracy: 0.8020 - val_loss: 0.7029 - val_accuracy: 0.7970\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6830 - accuracy: 0.8020 - val_loss: 0.7015 - val_accuracy: 0.7978\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6803 - accuracy: 0.8040 - val_loss: 0.7006 - val_accuracy: 0.7978\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6744 - accuracy: 0.8040 - val_loss: 0.7012 - val_accuracy: 0.7976\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6817 - accuracy: 0.8036 - val_loss: 0.7020 - val_accuracy: 0.7968\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6820 - accuracy: 0.8012 - val_loss: 0.6999 - val_accuracy: 0.7970\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6853 - accuracy: 0.8029 - val_loss: 0.6996 - val_accuracy: 0.7978\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6859 - accuracy: 0.8034 - val_loss: 0.6992 - val_accuracy: 0.7979\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6666 - accuracy: 0.8064 - val_loss: 0.6990 - val_accuracy: 0.7981\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6781 - accuracy: 0.8034 - val_loss: 0.6991 - val_accuracy: 0.7982\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6704 - accuracy: 0.8062 - val_loss: 0.6985 - val_accuracy: 0.7985\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6785 - accuracy: 0.8042 - val_loss: 0.6996 - val_accuracy: 0.7980\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6710 - accuracy: 0.8031 - val_loss: 0.6980 - val_accuracy: 0.7983\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6780 - accuracy: 0.8003 - val_loss: 0.6980 - val_accuracy: 0.7988\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6693 - accuracy: 0.8046 - val_loss: 0.6968 - val_accuracy: 0.7982\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6789 - accuracy: 0.8025 - val_loss: 0.6968 - val_accuracy: 0.7990\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6705 - accuracy: 0.8065 - val_loss: 0.6971 - val_accuracy: 0.7985\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6815 - accuracy: 0.8022 - val_loss: 0.6964 - val_accuracy: 0.7979\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6754 - accuracy: 0.8029 - val_loss: 0.6953 - val_accuracy: 0.7995\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6743 - accuracy: 0.8036 - val_loss: 0.6957 - val_accuracy: 0.7995\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6769 - accuracy: 0.8033 - val_loss: 0.6956 - val_accuracy: 0.7993\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6774 - accuracy: 0.8036 - val_loss: 0.6952 - val_accuracy: 0.7986\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6676 - accuracy: 0.8064 - val_loss: 0.6947 - val_accuracy: 0.7986\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6625 - accuracy: 0.8086 - val_loss: 0.6953 - val_accuracy: 0.7984\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6674 - accuracy: 0.8083 - val_loss: 0.6944 - val_accuracy: 0.7992\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6700 - accuracy: 0.8060 - val_loss: 0.6938 - val_accuracy: 0.7992\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6702 - accuracy: 0.8058 - val_loss: 0.6935 - val_accuracy: 0.7998\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6601 - accuracy: 0.8080 - val_loss: 0.6943 - val_accuracy: 0.7994\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6714 - accuracy: 0.8043 - val_loss: 0.6937 - val_accuracy: 0.7998\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6638 - accuracy: 0.8087 - val_loss: 0.6941 - val_accuracy: 0.7992\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6648 - accuracy: 0.8083 - val_loss: 0.6922 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6633 - accuracy: 0.8075 - val_loss: 0.6921 - val_accuracy: 0.8003\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6692 - accuracy: 0.8048 - val_loss: 0.6911 - val_accuracy: 0.7994\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6714 - accuracy: 0.8072 - val_loss: 0.6918 - val_accuracy: 0.8007\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6698 - accuracy: 0.8070 - val_loss: 0.6922 - val_accuracy: 0.7997\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6587 - accuracy: 0.8081 - val_loss: 0.6907 - val_accuracy: 0.8002\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6552 - accuracy: 0.8091 - val_loss: 0.6908 - val_accuracy: 0.8010\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6548 - accuracy: 0.8094 - val_loss: 0.6904 - val_accuracy: 0.8008\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6711 - accuracy: 0.8029 - val_loss: 0.6914 - val_accuracy: 0.7999\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6631 - accuracy: 0.8075 - val_loss: 0.6894 - val_accuracy: 0.8013\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6595 - accuracy: 0.8087 - val_loss: 0.6898 - val_accuracy: 0.8008\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6657 - accuracy: 0.8091 - val_loss: 0.6891 - val_accuracy: 0.8012\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6686 - accuracy: 0.8049 - val_loss: 0.6891 - val_accuracy: 0.8004\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6676 - accuracy: 0.8051 - val_loss: 0.6886 - val_accuracy: 0.8004\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6653 - accuracy: 0.8073 - val_loss: 0.6878 - val_accuracy: 0.8013\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6616 - accuracy: 0.8083 - val_loss: 0.6875 - val_accuracy: 0.8013\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6579 - accuracy: 0.8084 - val_loss: 0.6882 - val_accuracy: 0.8011\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6574 - accuracy: 0.8077 - val_loss: 0.6878 - val_accuracy: 0.8018\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6613 - accuracy: 0.8055 - val_loss: 0.6879 - val_accuracy: 0.8013\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6531 - accuracy: 0.8091 - val_loss: 0.6872 - val_accuracy: 0.8020\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6685 - accuracy: 0.8052 - val_loss: 0.6863 - val_accuracy: 0.8019\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6654 - accuracy: 0.8081 - val_loss: 0.6869 - val_accuracy: 0.8011\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6591 - accuracy: 0.8084 - val_loss: 0.6861 - val_accuracy: 0.8022\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6452 - accuracy: 0.8124 - val_loss: 0.6858 - val_accuracy: 0.8021\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6592 - accuracy: 0.8081 - val_loss: 0.6856 - val_accuracy: 0.8024\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6562 - accuracy: 0.8103 - val_loss: 0.6853 - val_accuracy: 0.8019\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6615 - accuracy: 0.8098 - val_loss: 0.6889 - val_accuracy: 0.8011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMC2UWAXwHlY",
        "outputId": "789d01bf-967f-48d5-e730-7d081e8ee61d"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6889 - accuracy: 0.8011\n",
            "Validation accuracy: 80.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI8bFpNdwNPv"
      },
      "source": [
        "**NN model, ReLU activations, Adam optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mYuwO16wLz5",
        "outputId": "6e1834a5-5572-4755-be2e-e8abb18ccb99"
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.01)\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 Âµs, sys: 1e+03 ns, total: 5 Âµs\n",
            "Wall time: 7.87 Âµs\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 11ms/step - loss: 6.6549 - accuracy: 0.1595 - val_loss: 1.9363 - val_accuracy: 0.2973\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.8963 - accuracy: 0.3120 - val_loss: 1.6418 - val_accuracy: 0.4301\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.4858 - accuracy: 0.5075 - val_loss: 1.1934 - val_accuracy: 0.6210\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 1.2725 - accuracy: 0.5885 - val_loss: 1.2380 - val_accuracy: 0.6027\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.2166 - accuracy: 0.6081 - val_loss: 1.1509 - val_accuracy: 0.6299\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1465 - accuracy: 0.6381 - val_loss: 1.0718 - val_accuracy: 0.6685\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.1023 - accuracy: 0.6522 - val_loss: 1.0848 - val_accuracy: 0.6591\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 1.0603 - accuracy: 0.6716 - val_loss: 1.1060 - val_accuracy: 0.6507\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 1.0153 - accuracy: 0.6875 - val_loss: 1.0462 - val_accuracy: 0.6781\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 1.0142 - accuracy: 0.6840 - val_loss: 1.0798 - val_accuracy: 0.6605\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9965 - accuracy: 0.6907 - val_loss: 1.0275 - val_accuracy: 0.6821\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9758 - accuracy: 0.6968 - val_loss: 0.9266 - val_accuracy: 0.7181\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9388 - accuracy: 0.7110 - val_loss: 0.9802 - val_accuracy: 0.6930\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.9701 - accuracy: 0.6996 - val_loss: 1.0141 - val_accuracy: 0.6876\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9430 - accuracy: 0.7122 - val_loss: 1.0317 - val_accuracy: 0.6862\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.9426 - accuracy: 0.7088 - val_loss: 0.9097 - val_accuracy: 0.7279\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.9335 - accuracy: 0.7142 - val_loss: 0.8778 - val_accuracy: 0.7313\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8868 - accuracy: 0.7267 - val_loss: 0.9847 - val_accuracy: 0.6937\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9145 - accuracy: 0.7213 - val_loss: 0.9480 - val_accuracy: 0.7115\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.9092 - accuracy: 0.7169 - val_loss: 0.9914 - val_accuracy: 0.6923\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8933 - accuracy: 0.7216 - val_loss: 0.9467 - val_accuracy: 0.7107\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8871 - accuracy: 0.7235 - val_loss: 0.8352 - val_accuracy: 0.7476\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8836 - accuracy: 0.7253 - val_loss: 0.9748 - val_accuracy: 0.6988\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8631 - accuracy: 0.7340 - val_loss: 0.9074 - val_accuracy: 0.7213\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8486 - accuracy: 0.7400 - val_loss: 0.8554 - val_accuracy: 0.7408\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8484 - accuracy: 0.7410 - val_loss: 0.8383 - val_accuracy: 0.7501\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8694 - accuracy: 0.7338 - val_loss: 0.8757 - val_accuracy: 0.7330\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8747 - accuracy: 0.7320 - val_loss: 0.8261 - val_accuracy: 0.7498\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8508 - accuracy: 0.7377 - val_loss: 0.9092 - val_accuracy: 0.7221\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8841 - accuracy: 0.7289 - val_loss: 0.8919 - val_accuracy: 0.7278\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8507 - accuracy: 0.7399 - val_loss: 0.8896 - val_accuracy: 0.7280\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.8583 - accuracy: 0.7379 - val_loss: 0.8347 - val_accuracy: 0.7467\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8333 - accuracy: 0.7438 - val_loss: 0.8631 - val_accuracy: 0.7384\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7994 - accuracy: 0.7570 - val_loss: 0.8113 - val_accuracy: 0.7547\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8334 - accuracy: 0.7459 - val_loss: 0.8595 - val_accuracy: 0.7370\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8137 - accuracy: 0.7495 - val_loss: 0.7741 - val_accuracy: 0.7687\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8146 - accuracy: 0.7513 - val_loss: 0.8916 - val_accuracy: 0.7306\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8171 - accuracy: 0.7514 - val_loss: 0.9230 - val_accuracy: 0.7160\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8366 - accuracy: 0.7418 - val_loss: 0.8489 - val_accuracy: 0.7422\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8040 - accuracy: 0.7508 - val_loss: 0.8189 - val_accuracy: 0.7501\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8074 - accuracy: 0.7544 - val_loss: 0.8091 - val_accuracy: 0.7578\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7935 - accuracy: 0.7547 - val_loss: 0.8988 - val_accuracy: 0.7233\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8190 - accuracy: 0.7491 - val_loss: 0.8430 - val_accuracy: 0.7487\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8056 - accuracy: 0.7523 - val_loss: 0.7833 - val_accuracy: 0.7628\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8085 - accuracy: 0.7544 - val_loss: 0.8229 - val_accuracy: 0.7486\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7902 - accuracy: 0.7576 - val_loss: 0.7860 - val_accuracy: 0.7617\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7742 - accuracy: 0.7659 - val_loss: 0.7924 - val_accuracy: 0.7606\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7957 - accuracy: 0.7557 - val_loss: 0.8302 - val_accuracy: 0.7495\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8115 - accuracy: 0.7527 - val_loss: 0.8287 - val_accuracy: 0.7485\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7906 - accuracy: 0.7590 - val_loss: 0.9191 - val_accuracy: 0.7143\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.8110 - accuracy: 0.7488 - val_loss: 0.7465 - val_accuracy: 0.7752\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7816 - accuracy: 0.7605 - val_loss: 0.7910 - val_accuracy: 0.7614\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7746 - accuracy: 0.7618 - val_loss: 0.8227 - val_accuracy: 0.7509\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7891 - accuracy: 0.7588 - val_loss: 0.8162 - val_accuracy: 0.7487\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7848 - accuracy: 0.7581 - val_loss: 0.8404 - val_accuracy: 0.7486\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7891 - accuracy: 0.7592 - val_loss: 0.7478 - val_accuracy: 0.7758\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7709 - accuracy: 0.7649 - val_loss: 0.7651 - val_accuracy: 0.7708\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7892 - accuracy: 0.7616 - val_loss: 0.8965 - val_accuracy: 0.7264\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7956 - accuracy: 0.7554 - val_loss: 0.7780 - val_accuracy: 0.7628\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7815 - accuracy: 0.7592 - val_loss: 0.7772 - val_accuracy: 0.7651\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7714 - accuracy: 0.7622 - val_loss: 0.7504 - val_accuracy: 0.7754\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7534 - accuracy: 0.7714 - val_loss: 0.7474 - val_accuracy: 0.7761\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7735 - accuracy: 0.7616 - val_loss: 0.7461 - val_accuracy: 0.7747\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7884 - accuracy: 0.7594 - val_loss: 0.7816 - val_accuracy: 0.7645\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7763 - accuracy: 0.7622 - val_loss: 0.7937 - val_accuracy: 0.7620\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7768 - accuracy: 0.7599 - val_loss: 0.7654 - val_accuracy: 0.7673\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7709 - accuracy: 0.7644 - val_loss: 0.7817 - val_accuracy: 0.7654\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7681 - accuracy: 0.7637 - val_loss: 0.8131 - val_accuracy: 0.7577\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7804 - accuracy: 0.7572 - val_loss: 0.7446 - val_accuracy: 0.7761\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7661 - accuracy: 0.7667 - val_loss: 0.8336 - val_accuracy: 0.7430\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7670 - accuracy: 0.7671 - val_loss: 0.8654 - val_accuracy: 0.7355\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7800 - accuracy: 0.7637 - val_loss: 0.8527 - val_accuracy: 0.7394\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7895 - accuracy: 0.7536 - val_loss: 0.7664 - val_accuracy: 0.7709\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7770 - accuracy: 0.7625 - val_loss: 0.8028 - val_accuracy: 0.7536\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7769 - accuracy: 0.7625 - val_loss: 0.7814 - val_accuracy: 0.7650\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7636 - accuracy: 0.7677 - val_loss: 0.7875 - val_accuracy: 0.7598\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7633 - accuracy: 0.7684 - val_loss: 0.8447 - val_accuracy: 0.7394\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7661 - accuracy: 0.7647 - val_loss: 0.7661 - val_accuracy: 0.7693\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7603 - accuracy: 0.7663 - val_loss: 0.7200 - val_accuracy: 0.7843\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7610 - accuracy: 0.7646 - val_loss: 0.7683 - val_accuracy: 0.7673\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7560 - accuracy: 0.7662 - val_loss: 0.7464 - val_accuracy: 0.7748\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7655 - accuracy: 0.7647 - val_loss: 0.7792 - val_accuracy: 0.7630\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7577 - accuracy: 0.7655 - val_loss: 0.7513 - val_accuracy: 0.7738\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7482 - accuracy: 0.7686 - val_loss: 0.7771 - val_accuracy: 0.7695\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7471 - accuracy: 0.7714 - val_loss: 0.7681 - val_accuracy: 0.7698\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7423 - accuracy: 0.7723 - val_loss: 0.7693 - val_accuracy: 0.7675\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7501 - accuracy: 0.7702 - val_loss: 0.7801 - val_accuracy: 0.7604\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7450 - accuracy: 0.7708 - val_loss: 0.8032 - val_accuracy: 0.7569\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7450 - accuracy: 0.7736 - val_loss: 0.7624 - val_accuracy: 0.7700\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7367 - accuracy: 0.7743 - val_loss: 0.7813 - val_accuracy: 0.7666\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7505 - accuracy: 0.7715 - val_loss: 0.7372 - val_accuracy: 0.7794\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7550 - accuracy: 0.7703 - val_loss: 0.7575 - val_accuracy: 0.7691\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7475 - accuracy: 0.7711 - val_loss: 0.7647 - val_accuracy: 0.7735\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7662 - accuracy: 0.7637 - val_loss: 0.7650 - val_accuracy: 0.7702\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7565 - accuracy: 0.7668 - val_loss: 0.7517 - val_accuracy: 0.7746\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7405 - accuracy: 0.7723 - val_loss: 0.8107 - val_accuracy: 0.7551\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7462 - accuracy: 0.7739 - val_loss: 0.8016 - val_accuracy: 0.7564\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7696 - accuracy: 0.7635 - val_loss: 0.7573 - val_accuracy: 0.7755\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.7428 - accuracy: 0.7709 - val_loss: 0.7680 - val_accuracy: 0.7697\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.7442 - accuracy: 0.7700 - val_loss: 0.7597 - val_accuracy: 0.7688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPh51mLmwcP4",
        "outputId": "7f20ee3f-4971-45ca-f78e-3178f956956c"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7597 - accuracy: 0.7688\n",
            "Validation accuracy: 76.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pNkJhpVwhSH"
      },
      "source": [
        "**NN model, ReLU activations, Adam optimizer: Changing Learning Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moEPldXHwftk",
        "outputId": "38098e99-8f9e-4f2c-f10e-457e8893fe47"
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 Âµs, sys: 1 Âµs, total: 4 Âµs\n",
            "Wall time: 10.5 Âµs\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "210/210 [==============================] - 3s 11ms/step - loss: 0.6387 - accuracy: 0.8050 - val_loss: 0.6610 - val_accuracy: 0.8040\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6147 - accuracy: 0.8140 - val_loss: 0.6486 - val_accuracy: 0.8076\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6124 - accuracy: 0.8142 - val_loss: 0.6552 - val_accuracy: 0.8055\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6152 - accuracy: 0.8120 - val_loss: 0.6450 - val_accuracy: 0.8094\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6041 - accuracy: 0.8157 - val_loss: 0.6459 - val_accuracy: 0.8090\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6030 - accuracy: 0.8177 - val_loss: 0.6471 - val_accuracy: 0.8086\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6053 - accuracy: 0.8177 - val_loss: 0.6436 - val_accuracy: 0.8105\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5964 - accuracy: 0.8207 - val_loss: 0.6496 - val_accuracy: 0.8090\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5915 - accuracy: 0.8187 - val_loss: 0.6564 - val_accuracy: 0.8045\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6086 - accuracy: 0.8153 - val_loss: 0.6394 - val_accuracy: 0.8109\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6018 - accuracy: 0.8198 - val_loss: 0.6365 - val_accuracy: 0.8128\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6070 - accuracy: 0.8183 - val_loss: 0.6430 - val_accuracy: 0.8104\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6082 - accuracy: 0.8175 - val_loss: 0.6403 - val_accuracy: 0.8110\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6008 - accuracy: 0.8166 - val_loss: 0.6451 - val_accuracy: 0.8077\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5911 - accuracy: 0.8228 - val_loss: 0.6384 - val_accuracy: 0.8119\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5948 - accuracy: 0.8193 - val_loss: 0.6403 - val_accuracy: 0.8113\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5969 - accuracy: 0.8203 - val_loss: 0.6473 - val_accuracy: 0.8092\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5879 - accuracy: 0.8238 - val_loss: 0.6439 - val_accuracy: 0.8103\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5965 - accuracy: 0.8225 - val_loss: 0.6457 - val_accuracy: 0.8084\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5931 - accuracy: 0.8196 - val_loss: 0.6316 - val_accuracy: 0.8141\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5969 - accuracy: 0.8214 - val_loss: 0.6490 - val_accuracy: 0.8080\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6008 - accuracy: 0.8181 - val_loss: 0.6346 - val_accuracy: 0.8144\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6040 - accuracy: 0.8177 - val_loss: 0.6304 - val_accuracy: 0.8140\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5946 - accuracy: 0.8202 - val_loss: 0.6362 - val_accuracy: 0.8122\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5989 - accuracy: 0.8205 - val_loss: 0.6420 - val_accuracy: 0.8098\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5921 - accuracy: 0.8219 - val_loss: 0.6347 - val_accuracy: 0.8129\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5770 - accuracy: 0.8263 - val_loss: 0.6352 - val_accuracy: 0.8129\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5947 - accuracy: 0.8179 - val_loss: 0.6344 - val_accuracy: 0.8138\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5923 - accuracy: 0.8209 - val_loss: 0.6416 - val_accuracy: 0.8109\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6053 - accuracy: 0.8160 - val_loss: 0.6348 - val_accuracy: 0.8125\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5969 - accuracy: 0.8171 - val_loss: 0.6372 - val_accuracy: 0.8121\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6024 - accuracy: 0.8170 - val_loss: 0.6370 - val_accuracy: 0.8135\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5817 - accuracy: 0.8223 - val_loss: 0.6531 - val_accuracy: 0.8061\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5907 - accuracy: 0.8212 - val_loss: 0.6348 - val_accuracy: 0.8128\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6008 - accuracy: 0.8172 - val_loss: 0.6361 - val_accuracy: 0.8121\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5937 - accuracy: 0.8206 - val_loss: 0.6483 - val_accuracy: 0.8090\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5905 - accuracy: 0.8219 - val_loss: 0.6383 - val_accuracy: 0.8108\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5883 - accuracy: 0.8214 - val_loss: 0.6335 - val_accuracy: 0.8136\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5912 - accuracy: 0.8223 - val_loss: 0.6341 - val_accuracy: 0.8128\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5855 - accuracy: 0.8203 - val_loss: 0.6326 - val_accuracy: 0.8144\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5864 - accuracy: 0.8217 - val_loss: 0.6417 - val_accuracy: 0.8101\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5966 - accuracy: 0.8184 - val_loss: 0.6322 - val_accuracy: 0.8151\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5980 - accuracy: 0.8158 - val_loss: 0.6333 - val_accuracy: 0.8147\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5980 - accuracy: 0.8195 - val_loss: 0.6321 - val_accuracy: 0.8135\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5993 - accuracy: 0.8206 - val_loss: 0.6401 - val_accuracy: 0.8119\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5838 - accuracy: 0.8232 - val_loss: 0.6281 - val_accuracy: 0.8153\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5878 - accuracy: 0.8214 - val_loss: 0.6455 - val_accuracy: 0.8073\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5896 - accuracy: 0.8221 - val_loss: 0.6335 - val_accuracy: 0.8131\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5951 - accuracy: 0.8215 - val_loss: 0.6338 - val_accuracy: 0.8125\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5852 - accuracy: 0.8217 - val_loss: 0.6419 - val_accuracy: 0.8108\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5879 - accuracy: 0.8216 - val_loss: 0.6375 - val_accuracy: 0.8116\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5877 - accuracy: 0.8206 - val_loss: 0.6298 - val_accuracy: 0.8140\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5877 - accuracy: 0.8243 - val_loss: 0.6277 - val_accuracy: 0.8154\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5820 - accuracy: 0.8249 - val_loss: 0.6274 - val_accuracy: 0.8156\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5761 - accuracy: 0.8261 - val_loss: 0.6292 - val_accuracy: 0.8158\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5951 - accuracy: 0.8206 - val_loss: 0.6295 - val_accuracy: 0.8146\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5792 - accuracy: 0.8233 - val_loss: 0.6299 - val_accuracy: 0.8139\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.6018 - accuracy: 0.8181 - val_loss: 0.6326 - val_accuracy: 0.8131\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5843 - accuracy: 0.8227 - val_loss: 0.6304 - val_accuracy: 0.8141\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5912 - accuracy: 0.8215 - val_loss: 0.6268 - val_accuracy: 0.8157\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5855 - accuracy: 0.8226 - val_loss: 0.6367 - val_accuracy: 0.8128\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5832 - accuracy: 0.8221 - val_loss: 0.6365 - val_accuracy: 0.8127\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5790 - accuracy: 0.8272 - val_loss: 0.6318 - val_accuracy: 0.8138\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5811 - accuracy: 0.8211 - val_loss: 0.6294 - val_accuracy: 0.8154\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5811 - accuracy: 0.8243 - val_loss: 0.6271 - val_accuracy: 0.8162\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5801 - accuracy: 0.8256 - val_loss: 0.6310 - val_accuracy: 0.8138\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5847 - accuracy: 0.8222 - val_loss: 0.6318 - val_accuracy: 0.8145\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5737 - accuracy: 0.8266 - val_loss: 0.6263 - val_accuracy: 0.8152\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.5759 - accuracy: 0.8269 - val_loss: 0.6280 - val_accuracy: 0.8155\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.5687 - accuracy: 0.8247 - val_loss: 0.6308 - val_accuracy: 0.8140\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.5874 - accuracy: 0.8196 - val_loss: 0.6383 - val_accuracy: 0.8109\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.5762 - accuracy: 0.8263 - val_loss: 0.6275 - val_accuracy: 0.8147\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.5783 - accuracy: 0.8233 - val_loss: 0.6248 - val_accuracy: 0.8163\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5819 - accuracy: 0.8254 - val_loss: 0.6360 - val_accuracy: 0.8121\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5939 - accuracy: 0.8200 - val_loss: 0.6281 - val_accuracy: 0.8146\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5796 - accuracy: 0.8263 - val_loss: 0.6300 - val_accuracy: 0.8151\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5757 - accuracy: 0.8274 - val_loss: 0.6264 - val_accuracy: 0.8158\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5793 - accuracy: 0.8252 - val_loss: 0.6248 - val_accuracy: 0.8152\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5765 - accuracy: 0.8237 - val_loss: 0.6244 - val_accuracy: 0.8156\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5791 - accuracy: 0.8240 - val_loss: 0.6336 - val_accuracy: 0.8126\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5771 - accuracy: 0.8260 - val_loss: 0.6247 - val_accuracy: 0.8157\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5803 - accuracy: 0.8222 - val_loss: 0.6300 - val_accuracy: 0.8138\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5796 - accuracy: 0.8241 - val_loss: 0.6273 - val_accuracy: 0.8158\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5767 - accuracy: 0.8258 - val_loss: 0.6308 - val_accuracy: 0.8135\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5731 - accuracy: 0.8265 - val_loss: 0.6376 - val_accuracy: 0.8124\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5764 - accuracy: 0.8258 - val_loss: 0.6237 - val_accuracy: 0.8163\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5834 - accuracy: 0.8234 - val_loss: 0.6290 - val_accuracy: 0.8146\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5816 - accuracy: 0.8258 - val_loss: 0.6317 - val_accuracy: 0.8133\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5744 - accuracy: 0.8278 - val_loss: 0.6313 - val_accuracy: 0.8136\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5838 - accuracy: 0.8235 - val_loss: 0.6252 - val_accuracy: 0.8158\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5875 - accuracy: 0.8204 - val_loss: 0.6259 - val_accuracy: 0.8168\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5831 - accuracy: 0.8218 - val_loss: 0.6342 - val_accuracy: 0.8110\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5875 - accuracy: 0.8197 - val_loss: 0.6293 - val_accuracy: 0.8146\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5812 - accuracy: 0.8238 - val_loss: 0.6268 - val_accuracy: 0.8163\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5876 - accuracy: 0.8194 - val_loss: 0.6204 - val_accuracy: 0.8186\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5752 - accuracy: 0.8260 - val_loss: 0.6329 - val_accuracy: 0.8133\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5757 - accuracy: 0.8258 - val_loss: 0.6257 - val_accuracy: 0.8150\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5735 - accuracy: 0.8262 - val_loss: 0.6287 - val_accuracy: 0.8139\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5776 - accuracy: 0.8254 - val_loss: 0.6268 - val_accuracy: 0.8156\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 2s 10ms/step - loss: 0.5795 - accuracy: 0.8235 - val_loss: 0.6320 - val_accuracy: 0.8142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD45sue4wm-V",
        "outputId": "ff4e54bc-405b-4cf1-dd8d-060d136d20ae"
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6320 - accuracy: 0.8142\n",
            "Validation accuracy: 81.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frgWa2eMwrkB"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   Improves the accuracy score considerably\n",
        "*   Best accuracy achieved till now is using relu activations, SGD optimizer, changing learning rate to 0.001.\n",
        "\n",
        "Let's try and change the number of activators and see if the score improves.\n",
        "\n",
        "\n",
        "\n",
        "### **NN model, ReLU activations, Changing Number of Activators, SGD optimizers**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoqaEmSqwq6V",
        "outputId": "89a2e59a-dc1d-4c2c-d7f5-a20a8a258cd6"
      },
      "source": [
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model3 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model3.add(Dense(256, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model3.add(Dense(128))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - Adding second hidden layer\n",
        "model3.add(Dense(64))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model3.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV6VH2_Tx21p",
        "outputId": "0f97d37b-04a0-49f0-de71-034639091eca"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 304,202\n",
            "Trainable params: 304,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kGpQb5Xx5F0",
        "outputId": "0d8ba79e-60c5-4df6-d140-e46004915d5f"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 2.3066 - accuracy: 0.1046 - val_loss: 2.2892 - val_accuracy: 0.1432\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2862 - accuracy: 0.1448 - val_loss: 2.2770 - val_accuracy: 0.1590\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2739 - accuracy: 0.1652 - val_loss: 2.2613 - val_accuracy: 0.2002\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2565 - accuracy: 0.2062 - val_loss: 2.2418 - val_accuracy: 0.1943\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2340 - accuracy: 0.2412 - val_loss: 2.2121 - val_accuracy: 0.2813\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2021 - accuracy: 0.2896 - val_loss: 2.1710 - val_accuracy: 0.3191\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.1604 - accuracy: 0.3091 - val_loss: 2.1176 - val_accuracy: 0.3325\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.1026 - accuracy: 0.3408 - val_loss: 2.0488 - val_accuracy: 0.3575\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.0288 - accuracy: 0.3670 - val_loss: 1.9746 - val_accuracy: 0.3965\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.9525 - accuracy: 0.3899 - val_loss: 1.8958 - val_accuracy: 0.4141\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.8731 - accuracy: 0.4174 - val_loss: 1.8142 - val_accuracy: 0.4290\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.7954 - accuracy: 0.4334 - val_loss: 1.7364 - val_accuracy: 0.4509\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.7163 - accuracy: 0.4619 - val_loss: 1.6550 - val_accuracy: 0.4772\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.6472 - accuracy: 0.4847 - val_loss: 1.5819 - val_accuracy: 0.5126\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.5879 - accuracy: 0.5070 - val_loss: 1.5600 - val_accuracy: 0.5063\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.5252 - accuracy: 0.5262 - val_loss: 1.4980 - val_accuracy: 0.5253\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.4742 - accuracy: 0.5453 - val_loss: 1.4620 - val_accuracy: 0.5435\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.4223 - accuracy: 0.5606 - val_loss: 1.4098 - val_accuracy: 0.5599\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.4013 - accuracy: 0.5639 - val_loss: 1.3434 - val_accuracy: 0.5862\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.3327 - accuracy: 0.5953 - val_loss: 1.3211 - val_accuracy: 0.5881\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.3074 - accuracy: 0.5975 - val_loss: 1.3106 - val_accuracy: 0.5901\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.2826 - accuracy: 0.6061 - val_loss: 1.2377 - val_accuracy: 0.6233\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.2506 - accuracy: 0.6166 - val_loss: 1.2049 - val_accuracy: 0.6349\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.2173 - accuracy: 0.6284 - val_loss: 1.1840 - val_accuracy: 0.6413\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.2098 - accuracy: 0.6271 - val_loss: 1.2040 - val_accuracy: 0.6227\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1671 - accuracy: 0.6423 - val_loss: 1.1883 - val_accuracy: 0.6314\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1587 - accuracy: 0.6440 - val_loss: 1.1240 - val_accuracy: 0.6639\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1399 - accuracy: 0.6533 - val_loss: 1.1325 - val_accuracy: 0.6555\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1067 - accuracy: 0.6670 - val_loss: 1.1200 - val_accuracy: 0.6536\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1024 - accuracy: 0.6652 - val_loss: 1.0888 - val_accuracy: 0.6709\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.0932 - accuracy: 0.6679 - val_loss: 1.0693 - val_accuracy: 0.6779\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0650 - accuracy: 0.6791 - val_loss: 1.0659 - val_accuracy: 0.6780\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0594 - accuracy: 0.6786 - val_loss: 1.0594 - val_accuracy: 0.6787\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.0459 - accuracy: 0.6853 - val_loss: 1.0114 - val_accuracy: 0.6966\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0220 - accuracy: 0.6869 - val_loss: 1.0102 - val_accuracy: 0.6951\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.0114 - accuracy: 0.6947 - val_loss: 0.9976 - val_accuracy: 0.6975\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9961 - accuracy: 0.7016 - val_loss: 1.0104 - val_accuracy: 0.6908\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9873 - accuracy: 0.7025 - val_loss: 0.9679 - val_accuracy: 0.7075\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9798 - accuracy: 0.7041 - val_loss: 0.9648 - val_accuracy: 0.7103\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9646 - accuracy: 0.7096 - val_loss: 0.9773 - val_accuracy: 0.7031\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9592 - accuracy: 0.7097 - val_loss: 0.9402 - val_accuracy: 0.7154\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9415 - accuracy: 0.7184 - val_loss: 0.9258 - val_accuracy: 0.7223\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9387 - accuracy: 0.7141 - val_loss: 0.9189 - val_accuracy: 0.7230\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9348 - accuracy: 0.7183 - val_loss: 0.9864 - val_accuracy: 0.6968\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9217 - accuracy: 0.7228 - val_loss: 0.9270 - val_accuracy: 0.7205\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9120 - accuracy: 0.7232 - val_loss: 0.9013 - val_accuracy: 0.7286\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8966 - accuracy: 0.7300 - val_loss: 0.8781 - val_accuracy: 0.7355\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8972 - accuracy: 0.7265 - val_loss: 0.8911 - val_accuracy: 0.7308\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8897 - accuracy: 0.7322 - val_loss: 0.8727 - val_accuracy: 0.7360\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8794 - accuracy: 0.7333 - val_loss: 0.8963 - val_accuracy: 0.7274\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8723 - accuracy: 0.7375 - val_loss: 0.8489 - val_accuracy: 0.7446\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8592 - accuracy: 0.7390 - val_loss: 0.8696 - val_accuracy: 0.7358\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8628 - accuracy: 0.7416 - val_loss: 0.8773 - val_accuracy: 0.7316\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8509 - accuracy: 0.7435 - val_loss: 0.8475 - val_accuracy: 0.7425\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8419 - accuracy: 0.7448 - val_loss: 0.8276 - val_accuracy: 0.7501\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8353 - accuracy: 0.7468 - val_loss: 0.8559 - val_accuracy: 0.7397\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8280 - accuracy: 0.7507 - val_loss: 0.8418 - val_accuracy: 0.7428\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8121 - accuracy: 0.7547 - val_loss: 0.8084 - val_accuracy: 0.7557\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8068 - accuracy: 0.7567 - val_loss: 0.8138 - val_accuracy: 0.7570\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8030 - accuracy: 0.7563 - val_loss: 0.7996 - val_accuracy: 0.7585\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8053 - accuracy: 0.7565 - val_loss: 0.8021 - val_accuracy: 0.7559\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7891 - accuracy: 0.7591 - val_loss: 0.7829 - val_accuracy: 0.7653\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7801 - accuracy: 0.7632 - val_loss: 0.7917 - val_accuracy: 0.7610\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7840 - accuracy: 0.7624 - val_loss: 0.7840 - val_accuracy: 0.7624\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7734 - accuracy: 0.7660 - val_loss: 0.7862 - val_accuracy: 0.7624\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7755 - accuracy: 0.7653 - val_loss: 0.7767 - val_accuracy: 0.7654\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7540 - accuracy: 0.7730 - val_loss: 0.7709 - val_accuracy: 0.7675\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7471 - accuracy: 0.7734 - val_loss: 0.7741 - val_accuracy: 0.7659\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7637 - accuracy: 0.7680 - val_loss: 0.7526 - val_accuracy: 0.7734\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7453 - accuracy: 0.7736 - val_loss: 0.7563 - val_accuracy: 0.7707\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7381 - accuracy: 0.7785 - val_loss: 0.7550 - val_accuracy: 0.7730\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7347 - accuracy: 0.7773 - val_loss: 0.7480 - val_accuracy: 0.7753\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7265 - accuracy: 0.7797 - val_loss: 0.7470 - val_accuracy: 0.7739\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7256 - accuracy: 0.7794 - val_loss: 0.7410 - val_accuracy: 0.7766\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7133 - accuracy: 0.7837 - val_loss: 0.7329 - val_accuracy: 0.7816\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7033 - accuracy: 0.7869 - val_loss: 0.7537 - val_accuracy: 0.7698\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7151 - accuracy: 0.7852 - val_loss: 0.7035 - val_accuracy: 0.7882\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7159 - accuracy: 0.7858 - val_loss: 0.7178 - val_accuracy: 0.7843\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7008 - accuracy: 0.7880 - val_loss: 0.7345 - val_accuracy: 0.7771\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6982 - accuracy: 0.7877 - val_loss: 0.7105 - val_accuracy: 0.7864\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6850 - accuracy: 0.7931 - val_loss: 0.6984 - val_accuracy: 0.7919\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6873 - accuracy: 0.7910 - val_loss: 0.7299 - val_accuracy: 0.7821\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6858 - accuracy: 0.7923 - val_loss: 0.6973 - val_accuracy: 0.7902\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6858 - accuracy: 0.7937 - val_loss: 0.6843 - val_accuracy: 0.7961\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6733 - accuracy: 0.7952 - val_loss: 0.6931 - val_accuracy: 0.7933\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6762 - accuracy: 0.7950 - val_loss: 0.6755 - val_accuracy: 0.7969\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6695 - accuracy: 0.7999 - val_loss: 0.6804 - val_accuracy: 0.7958\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6635 - accuracy: 0.8005 - val_loss: 0.6990 - val_accuracy: 0.7901\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6609 - accuracy: 0.7999 - val_loss: 0.6589 - val_accuracy: 0.8043\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6612 - accuracy: 0.7987 - val_loss: 0.6670 - val_accuracy: 0.8012\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6582 - accuracy: 0.8009 - val_loss: 0.6513 - val_accuracy: 0.8065\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6392 - accuracy: 0.8067 - val_loss: 0.6721 - val_accuracy: 0.7980\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6456 - accuracy: 0.8077 - val_loss: 0.7441 - val_accuracy: 0.7707\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6430 - accuracy: 0.8057 - val_loss: 0.6844 - val_accuracy: 0.7948\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6422 - accuracy: 0.8074 - val_loss: 0.6704 - val_accuracy: 0.8008\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6267 - accuracy: 0.8107 - val_loss: 0.6536 - val_accuracy: 0.8043\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6348 - accuracy: 0.8074 - val_loss: 0.6622 - val_accuracy: 0.8016\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6294 - accuracy: 0.8114 - val_loss: 0.6527 - val_accuracy: 0.8045\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6159 - accuracy: 0.8134 - val_loss: 0.6533 - val_accuracy: 0.8030\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6174 - accuracy: 0.8145 - val_loss: 0.6237 - val_accuracy: 0.8153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2F1XKecx8K0",
        "outputId": "22fd6624-5109-4156-d612-5a9c457da6a3"
      },
      "source": [
        "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations and changing the number of activators\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.6237 - accuracy: 0.8153\n",
            "Validation accuracy: 81.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8U9yGJFyMF3"
      },
      "source": [
        "**NN model, ReLU activations, Changing Number of Activators, Adam optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC5iqt9rx_s4",
        "outputId": "b42e2d99-1182-4dde-a8a2-1841e8e1a7c9"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 1.3323 - accuracy: 0.6453 - val_loss: 0.7212 - val_accuracy: 0.7808\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7631 - accuracy: 0.7678 - val_loss: 0.7862 - val_accuracy: 0.7602\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7729 - accuracy: 0.7608 - val_loss: 0.7934 - val_accuracy: 0.7605\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7550 - accuracy: 0.7682 - val_loss: 0.7202 - val_accuracy: 0.7819\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7219 - accuracy: 0.7779 - val_loss: 0.7496 - val_accuracy: 0.7735\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7364 - accuracy: 0.7739 - val_loss: 0.6836 - val_accuracy: 0.7957\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6935 - accuracy: 0.7867 - val_loss: 0.6689 - val_accuracy: 0.7981\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6760 - accuracy: 0.7931 - val_loss: 0.7449 - val_accuracy: 0.7715\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6680 - accuracy: 0.7945 - val_loss: 0.7114 - val_accuracy: 0.7827\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6601 - accuracy: 0.7983 - val_loss: 0.7047 - val_accuracy: 0.7858\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6557 - accuracy: 0.7980 - val_loss: 0.6362 - val_accuracy: 0.8131\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6021 - accuracy: 0.8145 - val_loss: 0.6253 - val_accuracy: 0.8116\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6118 - accuracy: 0.8092 - val_loss: 0.6628 - val_accuracy: 0.7982\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6110 - accuracy: 0.8109 - val_loss: 0.7121 - val_accuracy: 0.7868\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6119 - accuracy: 0.8139 - val_loss: 0.6335 - val_accuracy: 0.8083\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5794 - accuracy: 0.8224 - val_loss: 0.6100 - val_accuracy: 0.8156\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5689 - accuracy: 0.8250 - val_loss: 0.6064 - val_accuracy: 0.8186\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5577 - accuracy: 0.8295 - val_loss: 0.5702 - val_accuracy: 0.8286\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5607 - accuracy: 0.8267 - val_loss: 0.5404 - val_accuracy: 0.8391\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5259 - accuracy: 0.8375 - val_loss: 0.5606 - val_accuracy: 0.8331\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5302 - accuracy: 0.8377 - val_loss: 0.5350 - val_accuracy: 0.8397\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5121 - accuracy: 0.8399 - val_loss: 0.5604 - val_accuracy: 0.8308\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5137 - accuracy: 0.8402 - val_loss: 0.5466 - val_accuracy: 0.8375\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5037 - accuracy: 0.8429 - val_loss: 0.5737 - val_accuracy: 0.8260\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5014 - accuracy: 0.8444 - val_loss: 0.5269 - val_accuracy: 0.8411\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5060 - accuracy: 0.8442 - val_loss: 0.5641 - val_accuracy: 0.8289\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4812 - accuracy: 0.8533 - val_loss: 0.5202 - val_accuracy: 0.8451\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4777 - accuracy: 0.8532 - val_loss: 0.5637 - val_accuracy: 0.8298\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4827 - accuracy: 0.8505 - val_loss: 0.5045 - val_accuracy: 0.8504\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4586 - accuracy: 0.8570 - val_loss: 0.5230 - val_accuracy: 0.8416\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.4569 - accuracy: 0.8598 - val_loss: 0.5203 - val_accuracy: 0.8413\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.4530 - accuracy: 0.8595 - val_loss: 0.5342 - val_accuracy: 0.8375\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.4507 - accuracy: 0.8599 - val_loss: 0.5024 - val_accuracy: 0.8502\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4423 - accuracy: 0.8632 - val_loss: 0.4992 - val_accuracy: 0.8495\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4450 - accuracy: 0.8628 - val_loss: 0.4643 - val_accuracy: 0.8633\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4267 - accuracy: 0.8705 - val_loss: 0.5087 - val_accuracy: 0.8472\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4400 - accuracy: 0.8621 - val_loss: 0.4566 - val_accuracy: 0.8655\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4143 - accuracy: 0.8707 - val_loss: 0.4695 - val_accuracy: 0.8614\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4384 - accuracy: 0.8619 - val_loss: 0.4459 - val_accuracy: 0.8702\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4065 - accuracy: 0.8707 - val_loss: 0.4562 - val_accuracy: 0.8652\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3974 - accuracy: 0.8760 - val_loss: 0.5021 - val_accuracy: 0.8483\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4153 - accuracy: 0.8717 - val_loss: 0.4500 - val_accuracy: 0.8664\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4045 - accuracy: 0.8752 - val_loss: 0.4343 - val_accuracy: 0.8730\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3919 - accuracy: 0.8789 - val_loss: 0.4803 - val_accuracy: 0.8569\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4017 - accuracy: 0.8747 - val_loss: 0.5143 - val_accuracy: 0.8454\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3812 - accuracy: 0.8828 - val_loss: 0.4244 - val_accuracy: 0.8746\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3899 - accuracy: 0.8774 - val_loss: 0.4534 - val_accuracy: 0.8650\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3809 - accuracy: 0.8803 - val_loss: 0.4703 - val_accuracy: 0.8605\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3760 - accuracy: 0.8821 - val_loss: 0.4757 - val_accuracy: 0.8564\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3825 - accuracy: 0.8779 - val_loss: 0.4178 - val_accuracy: 0.8775\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3764 - accuracy: 0.8812 - val_loss: 0.4741 - val_accuracy: 0.8573\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3920 - accuracy: 0.8770 - val_loss: 0.4479 - val_accuracy: 0.8653\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3673 - accuracy: 0.8830 - val_loss: 0.4819 - val_accuracy: 0.8551\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3562 - accuracy: 0.8866 - val_loss: 0.4197 - val_accuracy: 0.8778\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3614 - accuracy: 0.8867 - val_loss: 0.4303 - val_accuracy: 0.8721\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3440 - accuracy: 0.8929 - val_loss: 0.4632 - val_accuracy: 0.8649\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3622 - accuracy: 0.8848 - val_loss: 0.4254 - val_accuracy: 0.8772\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3394 - accuracy: 0.8929 - val_loss: 0.4462 - val_accuracy: 0.8677\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3481 - accuracy: 0.8897 - val_loss: 0.4472 - val_accuracy: 0.8684\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3466 - accuracy: 0.8892 - val_loss: 0.3916 - val_accuracy: 0.8866\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3268 - accuracy: 0.8968 - val_loss: 0.4321 - val_accuracy: 0.8735\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3359 - accuracy: 0.8921 - val_loss: 0.4266 - val_accuracy: 0.8777\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3374 - accuracy: 0.8922 - val_loss: 0.4308 - val_accuracy: 0.8746\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3144 - accuracy: 0.9005 - val_loss: 0.4298 - val_accuracy: 0.8761\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3079 - accuracy: 0.9032 - val_loss: 0.4064 - val_accuracy: 0.8824\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3294 - accuracy: 0.8966 - val_loss: 0.4135 - val_accuracy: 0.8804\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3221 - accuracy: 0.8965 - val_loss: 0.4718 - val_accuracy: 0.8641\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3380 - accuracy: 0.8940 - val_loss: 0.4519 - val_accuracy: 0.8679\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3226 - accuracy: 0.8979 - val_loss: 0.3941 - val_accuracy: 0.8868\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3092 - accuracy: 0.9010 - val_loss: 0.4168 - val_accuracy: 0.8806\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3060 - accuracy: 0.9008 - val_loss: 0.4016 - val_accuracy: 0.8867\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3011 - accuracy: 0.9034 - val_loss: 0.4503 - val_accuracy: 0.8719\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3190 - accuracy: 0.8946 - val_loss: 0.4931 - val_accuracy: 0.8556\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3201 - accuracy: 0.8946 - val_loss: 0.4009 - val_accuracy: 0.8867\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2834 - accuracy: 0.9090 - val_loss: 0.4242 - val_accuracy: 0.8777\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3066 - accuracy: 0.9000 - val_loss: 0.3922 - val_accuracy: 0.8875\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2904 - accuracy: 0.9073 - val_loss: 0.4101 - val_accuracy: 0.8840\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2787 - accuracy: 0.9079 - val_loss: 0.3995 - val_accuracy: 0.8842\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2779 - accuracy: 0.9106 - val_loss: 0.3945 - val_accuracy: 0.8892\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2738 - accuracy: 0.9125 - val_loss: 0.4444 - val_accuracy: 0.8735\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2825 - accuracy: 0.9086 - val_loss: 0.4131 - val_accuracy: 0.8824\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2911 - accuracy: 0.9031 - val_loss: 0.3914 - val_accuracy: 0.8908\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2626 - accuracy: 0.9146 - val_loss: 0.4267 - val_accuracy: 0.8809\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2743 - accuracy: 0.9121 - val_loss: 0.4245 - val_accuracy: 0.8796\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2669 - accuracy: 0.9142 - val_loss: 0.3999 - val_accuracy: 0.8885\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2929 - accuracy: 0.9048 - val_loss: 0.4091 - val_accuracy: 0.8830\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2491 - accuracy: 0.9185 - val_loss: 0.4059 - val_accuracy: 0.8860\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2762 - accuracy: 0.9100 - val_loss: 0.4151 - val_accuracy: 0.8821\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2585 - accuracy: 0.9147 - val_loss: 0.3850 - val_accuracy: 0.8921\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2502 - accuracy: 0.9190 - val_loss: 0.3924 - val_accuracy: 0.8898\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2703 - accuracy: 0.9122 - val_loss: 0.3827 - val_accuracy: 0.8925\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2522 - accuracy: 0.9195 - val_loss: 0.3857 - val_accuracy: 0.8928\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2743 - accuracy: 0.9105 - val_loss: 0.4115 - val_accuracy: 0.8828\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2381 - accuracy: 0.9230 - val_loss: 0.4005 - val_accuracy: 0.8906\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2587 - accuracy: 0.9169 - val_loss: 0.4222 - val_accuracy: 0.8819\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2513 - accuracy: 0.9167 - val_loss: 0.4017 - val_accuracy: 0.8878\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2663 - accuracy: 0.9129 - val_loss: 0.3610 - val_accuracy: 0.9034\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2415 - accuracy: 0.9218 - val_loss: 0.3553 - val_accuracy: 0.9050\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2398 - accuracy: 0.9218 - val_loss: 0.3748 - val_accuracy: 0.9014\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2406 - accuracy: 0.9221 - val_loss: 0.3653 - val_accuracy: 0.9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmaVbuTGyUAT",
        "outputId": "cd07b504-87ab-4b05-be6c-2f9a9b6f37f1"
      },
      "source": [
        "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations and changing the number of activators\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3653 - accuracy: 0.9032\n",
            "Validation accuracy: 90.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzBrkanZ0DpB"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "\n",
        "\n",
        "*   Adding ReLU activations and changing activators results in improvement of score.\n",
        "*   Best accuracy achieved till now is using relu activations, changing number of activators and Adam optimizers with a learning rate of 0.001\n",
        "\n",
        "Let's try adding weight initilization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAvZOM9S0XF3"
      },
      "source": [
        "## **With Weight Initializers**\n",
        "\n",
        "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree.\n",
        "\n",
        "**NN model, relu activations, SGD optimizers with weight initializers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVFpN7MayXgG",
        "outputId": "5147448f-2ac5-418e-8d09-8325122eacec"
      },
      "source": [
        "print('NN model with weight initializers'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model4 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model4.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model4.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model4.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model4.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model4.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('softmax'))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with weight initializers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUxcObyi0rAB",
        "outputId": "44113e3c-bab9-4590-e66c-68fbcfe5d5c6"
      },
      "source": [
        "model4.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 305,962\n",
            "Trainable params: 305,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoKoLfcA0xIK",
        "outputId": "9484b312-ec60-4621-8798-b89f54ed01a7"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 2.3271 - accuracy: 0.1108 - val_loss: 2.2864 - val_accuracy: 0.1426\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2814 - accuracy: 0.1452 - val_loss: 2.2565 - val_accuracy: 0.1655\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.2466 - accuracy: 0.1727 - val_loss: 2.2117 - val_accuracy: 0.1960\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.1966 - accuracy: 0.2075 - val_loss: 2.1527 - val_accuracy: 0.2381\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.1332 - accuracy: 0.2461 - val_loss: 2.0663 - val_accuracy: 0.2698\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 2.0562 - accuracy: 0.2791 - val_loss: 2.1436 - val_accuracy: 0.2275\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.9809 - accuracy: 0.3183 - val_loss: 1.9157 - val_accuracy: 0.3434\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.8863 - accuracy: 0.3627 - val_loss: 1.8350 - val_accuracy: 0.3724\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.7850 - accuracy: 0.4029 - val_loss: 1.6541 - val_accuracy: 0.4662\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 1.6829 - accuracy: 0.4439 - val_loss: 1.5671 - val_accuracy: 0.5017\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.5950 - accuracy: 0.4862 - val_loss: 1.5607 - val_accuracy: 0.4914\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.5356 - accuracy: 0.5068 - val_loss: 1.5545 - val_accuracy: 0.4778\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 1.4512 - accuracy: 0.5365 - val_loss: 1.3640 - val_accuracy: 0.5716\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.3887 - accuracy: 0.5639 - val_loss: 1.3360 - val_accuracy: 0.5704\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.3431 - accuracy: 0.5789 - val_loss: 1.4505 - val_accuracy: 0.5255\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.2864 - accuracy: 0.5971 - val_loss: 1.2414 - val_accuracy: 0.6152\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.2374 - accuracy: 0.6098 - val_loss: 1.1972 - val_accuracy: 0.6290\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1964 - accuracy: 0.6250 - val_loss: 1.1461 - val_accuracy: 0.6451\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1671 - accuracy: 0.6381 - val_loss: 1.1252 - val_accuracy: 0.6525\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1486 - accuracy: 0.6415 - val_loss: 1.1736 - val_accuracy: 0.6244\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1202 - accuracy: 0.6487 - val_loss: 1.1169 - val_accuracy: 0.6492\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 1.0958 - accuracy: 0.6608 - val_loss: 1.0901 - val_accuracy: 0.6594\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0726 - accuracy: 0.6659 - val_loss: 1.0288 - val_accuracy: 0.6873\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.0503 - accuracy: 0.6754 - val_loss: 1.0798 - val_accuracy: 0.6668\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0318 - accuracy: 0.6842 - val_loss: 1.0027 - val_accuracy: 0.6889\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9966 - accuracy: 0.6946 - val_loss: 1.0057 - val_accuracy: 0.6914\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9748 - accuracy: 0.7065 - val_loss: 0.9664 - val_accuracy: 0.7030\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9559 - accuracy: 0.7049 - val_loss: 1.0282 - val_accuracy: 0.6718\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9541 - accuracy: 0.7089 - val_loss: 1.0311 - val_accuracy: 0.6790\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9254 - accuracy: 0.7164 - val_loss: 0.9117 - val_accuracy: 0.7226\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.9199 - accuracy: 0.7203 - val_loss: 0.9010 - val_accuracy: 0.7243\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8937 - accuracy: 0.7288 - val_loss: 0.8900 - val_accuracy: 0.7292\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8655 - accuracy: 0.7358 - val_loss: 0.8661 - val_accuracy: 0.7395\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8701 - accuracy: 0.7341 - val_loss: 0.8694 - val_accuracy: 0.7358\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.8489 - accuracy: 0.7427 - val_loss: 0.8558 - val_accuracy: 0.7388\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8517 - accuracy: 0.7391 - val_loss: 0.9033 - val_accuracy: 0.7235\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8339 - accuracy: 0.7458 - val_loss: 0.8672 - val_accuracy: 0.7365\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.8308 - accuracy: 0.7463 - val_loss: 0.8207 - val_accuracy: 0.7494\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8058 - accuracy: 0.7550 - val_loss: 0.8027 - val_accuracy: 0.7582\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.8035 - accuracy: 0.7538 - val_loss: 0.8848 - val_accuracy: 0.7289\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7879 - accuracy: 0.7627 - val_loss: 0.7991 - val_accuracy: 0.7582\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7732 - accuracy: 0.7674 - val_loss: 0.7646 - val_accuracy: 0.7677\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7699 - accuracy: 0.7673 - val_loss: 0.7513 - val_accuracy: 0.7748\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7537 - accuracy: 0.7686 - val_loss: 0.7434 - val_accuracy: 0.7761\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7422 - accuracy: 0.7751 - val_loss: 0.7614 - val_accuracy: 0.7654\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7552 - accuracy: 0.7702 - val_loss: 0.7437 - val_accuracy: 0.7755\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7402 - accuracy: 0.7755 - val_loss: 0.7412 - val_accuracy: 0.7742\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7268 - accuracy: 0.7771 - val_loss: 0.7279 - val_accuracy: 0.7811\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7205 - accuracy: 0.7806 - val_loss: 0.7623 - val_accuracy: 0.7663\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7239 - accuracy: 0.7763 - val_loss: 0.7027 - val_accuracy: 0.7893\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.7103 - accuracy: 0.7842 - val_loss: 0.7069 - val_accuracy: 0.7841\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6938 - accuracy: 0.7880 - val_loss: 0.7055 - val_accuracy: 0.7880\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6913 - accuracy: 0.7863 - val_loss: 0.6945 - val_accuracy: 0.7923\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6867 - accuracy: 0.7872 - val_loss: 0.6770 - val_accuracy: 0.7983\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6918 - accuracy: 0.7895 - val_loss: 0.6697 - val_accuracy: 0.8005\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6729 - accuracy: 0.7912 - val_loss: 0.7062 - val_accuracy: 0.7834\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6692 - accuracy: 0.7940 - val_loss: 0.6681 - val_accuracy: 0.7988\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6648 - accuracy: 0.7969 - val_loss: 0.6519 - val_accuracy: 0.8050\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6602 - accuracy: 0.7991 - val_loss: 0.6809 - val_accuracy: 0.7936\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6536 - accuracy: 0.7974 - val_loss: 0.7243 - val_accuracy: 0.7764\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6444 - accuracy: 0.8027 - val_loss: 0.7016 - val_accuracy: 0.7843\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6205 - accuracy: 0.8094 - val_loss: 0.6281 - val_accuracy: 0.8122\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6434 - accuracy: 0.8016 - val_loss: 0.6382 - val_accuracy: 0.8076\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6261 - accuracy: 0.8065 - val_loss: 0.7587 - val_accuracy: 0.7682\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6265 - accuracy: 0.8061 - val_loss: 0.6715 - val_accuracy: 0.7958\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6184 - accuracy: 0.8090 - val_loss: 0.6069 - val_accuracy: 0.8184\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6065 - accuracy: 0.8121 - val_loss: 0.6185 - val_accuracy: 0.8138\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5993 - accuracy: 0.8154 - val_loss: 0.6367 - val_accuracy: 0.8087\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6078 - accuracy: 0.8123 - val_loss: 0.6006 - val_accuracy: 0.8201\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6006 - accuracy: 0.8143 - val_loss: 0.6435 - val_accuracy: 0.8030\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5966 - accuracy: 0.8148 - val_loss: 0.5997 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5917 - accuracy: 0.8176 - val_loss: 0.7120 - val_accuracy: 0.7792\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5857 - accuracy: 0.8204 - val_loss: 0.5975 - val_accuracy: 0.8217\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5645 - accuracy: 0.8271 - val_loss: 0.5983 - val_accuracy: 0.8186\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5811 - accuracy: 0.8200 - val_loss: 0.6602 - val_accuracy: 0.7978\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5789 - accuracy: 0.8230 - val_loss: 0.5904 - val_accuracy: 0.8214\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5591 - accuracy: 0.8256 - val_loss: 0.6026 - val_accuracy: 0.8177\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5683 - accuracy: 0.8264 - val_loss: 0.6676 - val_accuracy: 0.7946\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5633 - accuracy: 0.8265 - val_loss: 0.6083 - val_accuracy: 0.8155\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5524 - accuracy: 0.8272 - val_loss: 0.5805 - val_accuracy: 0.8260\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5600 - accuracy: 0.8256 - val_loss: 0.6037 - val_accuracy: 0.8145\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5400 - accuracy: 0.8348 - val_loss: 0.6210 - val_accuracy: 0.8086\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5516 - accuracy: 0.8305 - val_loss: 0.5822 - val_accuracy: 0.8228\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5444 - accuracy: 0.8322 - val_loss: 0.5700 - val_accuracy: 0.8268\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5405 - accuracy: 0.8343 - val_loss: 0.5489 - val_accuracy: 0.8363\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5271 - accuracy: 0.8393 - val_loss: 0.5588 - val_accuracy: 0.8316\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5389 - accuracy: 0.8328 - val_loss: 0.5843 - val_accuracy: 0.8224\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5182 - accuracy: 0.8435 - val_loss: 0.5513 - val_accuracy: 0.8347\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5309 - accuracy: 0.8354 - val_loss: 0.5543 - val_accuracy: 0.8322\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5154 - accuracy: 0.8410 - val_loss: 0.5776 - val_accuracy: 0.8258\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5221 - accuracy: 0.8391 - val_loss: 0.5543 - val_accuracy: 0.8318\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5056 - accuracy: 0.8447 - val_loss: 0.5483 - val_accuracy: 0.8364\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5154 - accuracy: 0.8383 - val_loss: 0.5613 - val_accuracy: 0.8303\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5195 - accuracy: 0.8395 - val_loss: 0.5333 - val_accuracy: 0.8397\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5200 - accuracy: 0.8390 - val_loss: 0.5117 - val_accuracy: 0.8485\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.4961 - accuracy: 0.8446 - val_loss: 0.5211 - val_accuracy: 0.8459\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5057 - accuracy: 0.8424 - val_loss: 0.5243 - val_accuracy: 0.8434\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.5050 - accuracy: 0.8442 - val_loss: 0.5191 - val_accuracy: 0.8451\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.4986 - accuracy: 0.8474 - val_loss: 0.5184 - val_accuracy: 0.8450\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4922 - accuracy: 0.8485 - val_loss: 0.5270 - val_accuracy: 0.8430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od27J28G0z4w",
        "outputId": "120fcc6c-36d3-4e30-96e3-0d93358e5691"
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\n",
        "results4 = model4.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5270 - accuracy: 0.8430\n",
            "Validation accuracy: 84.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B_Q6Y5y07oj"
      },
      "source": [
        "**NN model, ReLU activations, Adam optimizers with weight initializers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgcly-cJ03Vi",
        "outputId": "2e4cbcfb-e243-4ac9-a265-de507face2a9"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model4.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 1.4406 - accuracy: 0.6152 - val_loss: 0.7655 - val_accuracy: 0.7584\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7367 - accuracy: 0.7714 - val_loss: 0.7668 - val_accuracy: 0.7565\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7105 - accuracy: 0.7780 - val_loss: 0.6869 - val_accuracy: 0.7933\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6847 - accuracy: 0.7878 - val_loss: 0.6585 - val_accuracy: 0.7973\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6831 - accuracy: 0.7859 - val_loss: 0.6863 - val_accuracy: 0.7905\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6605 - accuracy: 0.7938 - val_loss: 0.6469 - val_accuracy: 0.7993\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6631 - accuracy: 0.7918 - val_loss: 0.6732 - val_accuracy: 0.7925\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6455 - accuracy: 0.7992 - val_loss: 0.6968 - val_accuracy: 0.7886\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6288 - accuracy: 0.8020 - val_loss: 0.7186 - val_accuracy: 0.7722\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6303 - accuracy: 0.8028 - val_loss: 0.6285 - val_accuracy: 0.8076\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5924 - accuracy: 0.8158 - val_loss: 0.5854 - val_accuracy: 0.8217\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5845 - accuracy: 0.8186 - val_loss: 0.6417 - val_accuracy: 0.7995\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5734 - accuracy: 0.8228 - val_loss: 0.6255 - val_accuracy: 0.8088\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5785 - accuracy: 0.8172 - val_loss: 0.5823 - val_accuracy: 0.8236\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5640 - accuracy: 0.8236 - val_loss: 0.6177 - val_accuracy: 0.8112\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5477 - accuracy: 0.8316 - val_loss: 0.5945 - val_accuracy: 0.8173\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5546 - accuracy: 0.8262 - val_loss: 0.5371 - val_accuracy: 0.8363\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5105 - accuracy: 0.8403 - val_loss: 0.5573 - val_accuracy: 0.8281\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5272 - accuracy: 0.8356 - val_loss: 0.5360 - val_accuracy: 0.8354\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5259 - accuracy: 0.8340 - val_loss: 0.5738 - val_accuracy: 0.8215\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4974 - accuracy: 0.8437 - val_loss: 0.5583 - val_accuracy: 0.8273\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4993 - accuracy: 0.8392 - val_loss: 0.5822 - val_accuracy: 0.8201\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4924 - accuracy: 0.8443 - val_loss: 0.5219 - val_accuracy: 0.8368\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4901 - accuracy: 0.8441 - val_loss: 0.5140 - val_accuracy: 0.8409\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4697 - accuracy: 0.8504 - val_loss: 0.4841 - val_accuracy: 0.8525\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4678 - accuracy: 0.8517 - val_loss: 0.5091 - val_accuracy: 0.8422\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4773 - accuracy: 0.8490 - val_loss: 0.4903 - val_accuracy: 0.8506\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4689 - accuracy: 0.8521 - val_loss: 0.4951 - val_accuracy: 0.8489\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4783 - accuracy: 0.8487 - val_loss: 0.4858 - val_accuracy: 0.8502\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4360 - accuracy: 0.8610 - val_loss: 0.4969 - val_accuracy: 0.8478\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4541 - accuracy: 0.8550 - val_loss: 0.5776 - val_accuracy: 0.8218\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4453 - accuracy: 0.8584 - val_loss: 0.5008 - val_accuracy: 0.8464\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4490 - accuracy: 0.8579 - val_loss: 0.5283 - val_accuracy: 0.8346\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.4199 - accuracy: 0.8668 - val_loss: 0.5421 - val_accuracy: 0.8336\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4315 - accuracy: 0.8623 - val_loss: 0.4724 - val_accuracy: 0.8549\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4116 - accuracy: 0.8689 - val_loss: 0.4790 - val_accuracy: 0.8536\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4204 - accuracy: 0.8662 - val_loss: 0.4550 - val_accuracy: 0.8617\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4150 - accuracy: 0.8665 - val_loss: 0.4425 - val_accuracy: 0.8643\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4269 - accuracy: 0.8624 - val_loss: 0.4403 - val_accuracy: 0.8660\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3904 - accuracy: 0.8762 - val_loss: 0.5341 - val_accuracy: 0.8344\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4167 - accuracy: 0.8651 - val_loss: 0.4907 - val_accuracy: 0.8481\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4088 - accuracy: 0.8676 - val_loss: 0.4440 - val_accuracy: 0.8638\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4078 - accuracy: 0.8680 - val_loss: 0.4295 - val_accuracy: 0.8713\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3840 - accuracy: 0.8767 - val_loss: 0.4371 - val_accuracy: 0.8670\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3767 - accuracy: 0.8798 - val_loss: 0.4595 - val_accuracy: 0.8607\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3750 - accuracy: 0.8785 - val_loss: 0.3908 - val_accuracy: 0.8828\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3512 - accuracy: 0.8871 - val_loss: 0.4421 - val_accuracy: 0.8655\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3950 - accuracy: 0.8736 - val_loss: 0.4123 - val_accuracy: 0.8762\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3542 - accuracy: 0.8858 - val_loss: 0.4429 - val_accuracy: 0.8663\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3660 - accuracy: 0.8812 - val_loss: 0.4157 - val_accuracy: 0.8757\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3643 - accuracy: 0.8827 - val_loss: 0.4111 - val_accuracy: 0.8773\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3477 - accuracy: 0.8884 - val_loss: 0.4532 - val_accuracy: 0.8628\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3600 - accuracy: 0.8831 - val_loss: 0.4307 - val_accuracy: 0.8709\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3405 - accuracy: 0.8878 - val_loss: 0.4327 - val_accuracy: 0.8704\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3483 - accuracy: 0.8879 - val_loss: 0.4240 - val_accuracy: 0.8727\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3451 - accuracy: 0.8884 - val_loss: 0.4236 - val_accuracy: 0.8707\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3493 - accuracy: 0.8873 - val_loss: 0.4257 - val_accuracy: 0.8726\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3425 - accuracy: 0.8896 - val_loss: 0.4423 - val_accuracy: 0.8648\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3147 - accuracy: 0.8980 - val_loss: 0.4636 - val_accuracy: 0.8570\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3245 - accuracy: 0.8941 - val_loss: 0.3817 - val_accuracy: 0.8888\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3332 - accuracy: 0.8913 - val_loss: 0.4170 - val_accuracy: 0.8762\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3071 - accuracy: 0.9018 - val_loss: 0.3685 - val_accuracy: 0.8921\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3253 - accuracy: 0.8965 - val_loss: 0.3969 - val_accuracy: 0.8835\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3121 - accuracy: 0.8975 - val_loss: 0.4108 - val_accuracy: 0.8786\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3149 - accuracy: 0.8995 - val_loss: 0.4098 - val_accuracy: 0.8799\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3037 - accuracy: 0.9012 - val_loss: 0.4022 - val_accuracy: 0.8801\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3163 - accuracy: 0.8963 - val_loss: 0.4015 - val_accuracy: 0.8811\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3036 - accuracy: 0.9020 - val_loss: 0.4834 - val_accuracy: 0.8538\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3205 - accuracy: 0.8960 - val_loss: 0.4020 - val_accuracy: 0.8824\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2971 - accuracy: 0.9039 - val_loss: 0.3909 - val_accuracy: 0.8846\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2999 - accuracy: 0.9022 - val_loss: 0.4220 - val_accuracy: 0.8774\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2926 - accuracy: 0.9061 - val_loss: 0.3857 - val_accuracy: 0.8855\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3082 - accuracy: 0.8992 - val_loss: 0.3930 - val_accuracy: 0.8850\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2943 - accuracy: 0.9017 - val_loss: 0.4138 - val_accuracy: 0.8777\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2795 - accuracy: 0.9080 - val_loss: 0.4533 - val_accuracy: 0.8675\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2880 - accuracy: 0.9074 - val_loss: 0.4387 - val_accuracy: 0.8711\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2932 - accuracy: 0.9008 - val_loss: 0.3823 - val_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2770 - accuracy: 0.9097 - val_loss: 0.3953 - val_accuracy: 0.8849\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2772 - accuracy: 0.9115 - val_loss: 0.3842 - val_accuracy: 0.8928\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2752 - accuracy: 0.9092 - val_loss: 0.3599 - val_accuracy: 0.8981\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2650 - accuracy: 0.9137 - val_loss: 0.3540 - val_accuracy: 0.8988\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2726 - accuracy: 0.9110 - val_loss: 0.4104 - val_accuracy: 0.8830\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2733 - accuracy: 0.9112 - val_loss: 0.3923 - val_accuracy: 0.8866\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2662 - accuracy: 0.9106 - val_loss: 0.4166 - val_accuracy: 0.8795\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2794 - accuracy: 0.9083 - val_loss: 0.3972 - val_accuracy: 0.8864\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2501 - accuracy: 0.9195 - val_loss: 0.3766 - val_accuracy: 0.8942\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2466 - accuracy: 0.9175 - val_loss: 0.4011 - val_accuracy: 0.8878\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2476 - accuracy: 0.9182 - val_loss: 0.4504 - val_accuracy: 0.8728\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2763 - accuracy: 0.9080 - val_loss: 0.4308 - val_accuracy: 0.8760\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2541 - accuracy: 0.9167 - val_loss: 0.3529 - val_accuracy: 0.9046\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2490 - accuracy: 0.9174 - val_loss: 0.4163 - val_accuracy: 0.8810\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2673 - accuracy: 0.9103 - val_loss: 0.3860 - val_accuracy: 0.8900\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2471 - accuracy: 0.9193 - val_loss: 0.3547 - val_accuracy: 0.9029\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2362 - accuracy: 0.9236 - val_loss: 0.3771 - val_accuracy: 0.8957\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2442 - accuracy: 0.9202 - val_loss: 0.3750 - val_accuracy: 0.8974\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2369 - accuracy: 0.9210 - val_loss: 0.4158 - val_accuracy: 0.8837\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2455 - accuracy: 0.9175 - val_loss: 0.3847 - val_accuracy: 0.8951\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2341 - accuracy: 0.9233 - val_loss: 0.3710 - val_accuracy: 0.9006\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2275 - accuracy: 0.9237 - val_loss: 0.3924 - val_accuracy: 0.8922\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.2457 - accuracy: 0.9177 - val_loss: 0.4104 - val_accuracy: 0.8823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhIiQLZx1V3L",
        "outputId": "d0cb0678-ffb6-4fba-f649-6aa252567625"
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\n",
        "results4 = model4.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4104 - accuracy: 0.8823\n",
            "Validation accuracy: 88.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8465ONn1das"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   Adding weight initialiers didn't result in improvement of score.\n",
        "*   ReLU activations, changing number of activators, Adam optimizers gives the best score out of the ones tried as of now.\n",
        "\n",
        "Let's try Batch Normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E81O8Su10hz"
      },
      "source": [
        "## **Batch Normalization**\n",
        "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective. Normalize each mini-batch before nonlinearity.\n",
        "\n",
        "NN model, relu activations, SGD optimizers with weight initializers and batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UzDjrSw1aSp",
        "outputId": "13f91656-6ddf-4a3a-b154-e6ea8c9ab3cc"
      },
      "source": [
        "print('NN model with batch normalization'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model5 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model5.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model5.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model5.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model5.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model5.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model5.add(Activation('softmax'))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with batch normalization\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdBJeOPa2H8d",
        "outputId": "a71e1096-a09f-4b6d-f7c9-f6005c18ca7b"
      },
      "source": [
        "model5.summary()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 307,882\n",
            "Trainable params: 306,922\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZb1Syq2K9Q",
        "outputId": "ce876f66-abdc-41dc-d1ad-afdc0e71b095"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 5s 20ms/step - loss: 2.4738 - accuracy: 0.1502 - val_loss: 2.2160 - val_accuracy: 0.2208\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 1.9336 - accuracy: 0.3419 - val_loss: 1.8018 - val_accuracy: 0.4123\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 1.6274 - accuracy: 0.4837 - val_loss: 1.5493 - val_accuracy: 0.5087\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 1.4170 - accuracy: 0.5659 - val_loss: 1.3813 - val_accuracy: 0.5732\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 1.2611 - accuracy: 0.6217 - val_loss: 1.2302 - val_accuracy: 0.6233\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 1.1453 - accuracy: 0.6573 - val_loss: 1.1797 - val_accuracy: 0.6337\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 1.0531 - accuracy: 0.6848 - val_loss: 1.1204 - val_accuracy: 0.6531\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.9889 - accuracy: 0.7032 - val_loss: 1.0916 - val_accuracy: 0.6571\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.9332 - accuracy: 0.7173 - val_loss: 0.9644 - val_accuracy: 0.7008\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.8907 - accuracy: 0.7306 - val_loss: 0.9244 - val_accuracy: 0.7134\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.8387 - accuracy: 0.7451 - val_loss: 0.9139 - val_accuracy: 0.7165\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.8076 - accuracy: 0.7547 - val_loss: 0.9292 - val_accuracy: 0.7106\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.7708 - accuracy: 0.7622 - val_loss: 0.9007 - val_accuracy: 0.7157\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.7450 - accuracy: 0.7756 - val_loss: 0.9610 - val_accuracy: 0.6926\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.7151 - accuracy: 0.7777 - val_loss: 0.9401 - val_accuracy: 0.7001\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.6927 - accuracy: 0.7866 - val_loss: 0.8212 - val_accuracy: 0.7439\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.6709 - accuracy: 0.7950 - val_loss: 0.9943 - val_accuracy: 0.6885\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.6592 - accuracy: 0.7976 - val_loss: 0.8747 - val_accuracy: 0.7284\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.6283 - accuracy: 0.8065 - val_loss: 0.8666 - val_accuracy: 0.7313\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.6195 - accuracy: 0.8096 - val_loss: 0.7985 - val_accuracy: 0.7495\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.5937 - accuracy: 0.8151 - val_loss: 0.7552 - val_accuracy: 0.7602\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.5808 - accuracy: 0.8218 - val_loss: 0.8026 - val_accuracy: 0.7488\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.5677 - accuracy: 0.8222 - val_loss: 0.8387 - val_accuracy: 0.7341\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.5479 - accuracy: 0.8311 - val_loss: 0.9163 - val_accuracy: 0.7165\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.5362 - accuracy: 0.8330 - val_loss: 1.1330 - val_accuracy: 0.6772\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.5281 - accuracy: 0.8359 - val_loss: 0.7263 - val_accuracy: 0.7724\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.5125 - accuracy: 0.8414 - val_loss: 0.7824 - val_accuracy: 0.7533\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4993 - accuracy: 0.8459 - val_loss: 0.9306 - val_accuracy: 0.7129\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4955 - accuracy: 0.8468 - val_loss: 0.9731 - val_accuracy: 0.7047\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4815 - accuracy: 0.8525 - val_loss: 0.6472 - val_accuracy: 0.7962\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4758 - accuracy: 0.8531 - val_loss: 0.8410 - val_accuracy: 0.7441\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4595 - accuracy: 0.8594 - val_loss: 0.7177 - val_accuracy: 0.7721\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4594 - accuracy: 0.8591 - val_loss: 0.7094 - val_accuracy: 0.7734\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4396 - accuracy: 0.8638 - val_loss: 0.8067 - val_accuracy: 0.7538\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4379 - accuracy: 0.8642 - val_loss: 0.8316 - val_accuracy: 0.7430\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4239 - accuracy: 0.8696 - val_loss: 0.6568 - val_accuracy: 0.7928\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4138 - accuracy: 0.8721 - val_loss: 0.6513 - val_accuracy: 0.7966\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4177 - accuracy: 0.8706 - val_loss: 0.7677 - val_accuracy: 0.7567\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4051 - accuracy: 0.8750 - val_loss: 0.7790 - val_accuracy: 0.7575\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4012 - accuracy: 0.8776 - val_loss: 0.6443 - val_accuracy: 0.7983\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3882 - accuracy: 0.8799 - val_loss: 0.6283 - val_accuracy: 0.8001\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3872 - accuracy: 0.8798 - val_loss: 1.1654 - val_accuracy: 0.6830\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3706 - accuracy: 0.8881 - val_loss: 0.6613 - val_accuracy: 0.7943\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3737 - accuracy: 0.8852 - val_loss: 0.6025 - val_accuracy: 0.8149\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3592 - accuracy: 0.8882 - val_loss: 1.1373 - val_accuracy: 0.6967\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3526 - accuracy: 0.8939 - val_loss: 0.7095 - val_accuracy: 0.7768\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3569 - accuracy: 0.8895 - val_loss: 0.6617 - val_accuracy: 0.7906\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3524 - accuracy: 0.8891 - val_loss: 0.7179 - val_accuracy: 0.7822\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3355 - accuracy: 0.8959 - val_loss: 0.5571 - val_accuracy: 0.8310\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3371 - accuracy: 0.8976 - val_loss: 0.7391 - val_accuracy: 0.7846\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3453 - accuracy: 0.8944 - val_loss: 0.7923 - val_accuracy: 0.7679\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3340 - accuracy: 0.8962 - val_loss: 0.5712 - val_accuracy: 0.8221\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3205 - accuracy: 0.9016 - val_loss: 0.8397 - val_accuracy: 0.7495\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3172 - accuracy: 0.9035 - val_loss: 0.6153 - val_accuracy: 0.8083\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3241 - accuracy: 0.8988 - val_loss: 1.2984 - val_accuracy: 0.6625\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3057 - accuracy: 0.9058 - val_loss: 2.5086 - val_accuracy: 0.5250\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3042 - accuracy: 0.9081 - val_loss: 0.7751 - val_accuracy: 0.7760\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2948 - accuracy: 0.9093 - val_loss: 0.6391 - val_accuracy: 0.8100\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2909 - accuracy: 0.9111 - val_loss: 0.6850 - val_accuracy: 0.8002\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2965 - accuracy: 0.9075 - val_loss: 1.0782 - val_accuracy: 0.7264\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2877 - accuracy: 0.9122 - val_loss: 0.5412 - val_accuracy: 0.8343\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2769 - accuracy: 0.9179 - val_loss: 0.5675 - val_accuracy: 0.8267\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2828 - accuracy: 0.9143 - val_loss: 0.5951 - val_accuracy: 0.8156\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2784 - accuracy: 0.9142 - val_loss: 0.6195 - val_accuracy: 0.8139\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2723 - accuracy: 0.9175 - val_loss: 0.5720 - val_accuracy: 0.8255\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2621 - accuracy: 0.9214 - val_loss: 0.7653 - val_accuracy: 0.7821\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2562 - accuracy: 0.9231 - val_loss: 0.7636 - val_accuracy: 0.7850\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2637 - accuracy: 0.9212 - val_loss: 0.6995 - val_accuracy: 0.7971\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2590 - accuracy: 0.9209 - val_loss: 0.5984 - val_accuracy: 0.8194\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2524 - accuracy: 0.9227 - val_loss: 1.0150 - val_accuracy: 0.7333\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2465 - accuracy: 0.9262 - val_loss: 0.5391 - val_accuracy: 0.8372\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2485 - accuracy: 0.9246 - val_loss: 0.5369 - val_accuracy: 0.8385\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2402 - accuracy: 0.9271 - val_loss: 0.5525 - val_accuracy: 0.8328\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2365 - accuracy: 0.9300 - val_loss: 0.5519 - val_accuracy: 0.8340\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2365 - accuracy: 0.9265 - val_loss: 0.6648 - val_accuracy: 0.8041\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2361 - accuracy: 0.9283 - val_loss: 0.4782 - val_accuracy: 0.8571\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2306 - accuracy: 0.9303 - val_loss: 0.5365 - val_accuracy: 0.8403\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2330 - accuracy: 0.9314 - val_loss: 0.6596 - val_accuracy: 0.8098\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2284 - accuracy: 0.9310 - val_loss: 0.7573 - val_accuracy: 0.7840\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2122 - accuracy: 0.9365 - val_loss: 0.5213 - val_accuracy: 0.8443\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2243 - accuracy: 0.9325 - val_loss: 0.4998 - val_accuracy: 0.8534\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2124 - accuracy: 0.9374 - val_loss: 0.5673 - val_accuracy: 0.8331\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2170 - accuracy: 0.9335 - val_loss: 0.5825 - val_accuracy: 0.8282\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2116 - accuracy: 0.9324 - val_loss: 0.6037 - val_accuracy: 0.8223\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.2120 - accuracy: 0.9339 - val_loss: 0.7147 - val_accuracy: 0.8078\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2072 - accuracy: 0.9359 - val_loss: 0.5619 - val_accuracy: 0.8320\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2070 - accuracy: 0.9374 - val_loss: 0.6372 - val_accuracy: 0.8151\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2079 - accuracy: 0.9380 - val_loss: 0.4769 - val_accuracy: 0.8605\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1979 - accuracy: 0.9413 - val_loss: 0.5679 - val_accuracy: 0.8379\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1906 - accuracy: 0.9420 - val_loss: 0.7124 - val_accuracy: 0.7992\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1925 - accuracy: 0.9421 - val_loss: 0.5101 - val_accuracy: 0.8463\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1896 - accuracy: 0.9437 - val_loss: 0.5542 - val_accuracy: 0.8402\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1787 - accuracy: 0.9467 - val_loss: 0.6299 - val_accuracy: 0.8303\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1818 - accuracy: 0.9451 - val_loss: 0.6389 - val_accuracy: 0.8202\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1802 - accuracy: 0.9454 - val_loss: 0.5778 - val_accuracy: 0.8332\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1776 - accuracy: 0.9443 - val_loss: 0.5887 - val_accuracy: 0.8347\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1775 - accuracy: 0.9460 - val_loss: 0.6191 - val_accuracy: 0.8226\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1758 - accuracy: 0.9460 - val_loss: 0.7422 - val_accuracy: 0.8011\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1712 - accuracy: 0.9494 - val_loss: 0.5084 - val_accuracy: 0.8577\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1691 - accuracy: 0.9484 - val_loss: 0.5233 - val_accuracy: 0.8529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5IjvNEE2OMl",
        "outputId": "756942ff-41b9-48e3-ad72-d3dacf21211d"
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results5 = model5.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5233 - accuracy: 0.8529\n",
            "Validation accuracy: 85.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnZ8lqR52VP4"
      },
      "source": [
        "**NN model, ReLU activations, Adam optimizers with weight initializers and batch normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro3t-gFJ2SIo",
        "outputId": "b1725369-3a45-4d62-ded6-12936c784a2a"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model5.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 6s 20ms/step - loss: 0.8799 - accuracy: 0.7454 - val_loss: 4.0018 - val_accuracy: 0.3689\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.5694 - accuracy: 0.8159 - val_loss: 1.9536 - val_accuracy: 0.4557\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.5365 - accuracy: 0.8280 - val_loss: 1.4517 - val_accuracy: 0.5662\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4818 - accuracy: 0.8431 - val_loss: 1.5266 - val_accuracy: 0.5638\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4604 - accuracy: 0.8517 - val_loss: 1.1952 - val_accuracy: 0.5969\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4360 - accuracy: 0.8597 - val_loss: 1.6678 - val_accuracy: 0.5344\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4063 - accuracy: 0.8684 - val_loss: 1.4184 - val_accuracy: 0.6083\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3859 - accuracy: 0.8750 - val_loss: 1.5037 - val_accuracy: 0.5767\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.3858 - accuracy: 0.8758 - val_loss: 1.5134 - val_accuracy: 0.5879\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3734 - accuracy: 0.8796 - val_loss: 1.1029 - val_accuracy: 0.6593\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3563 - accuracy: 0.8859 - val_loss: 1.0725 - val_accuracy: 0.6717\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3435 - accuracy: 0.8875 - val_loss: 1.5664 - val_accuracy: 0.5732\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3393 - accuracy: 0.8897 - val_loss: 1.2126 - val_accuracy: 0.6393\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3206 - accuracy: 0.8966 - val_loss: 1.7822 - val_accuracy: 0.5769\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.3214 - accuracy: 0.8955 - val_loss: 1.3541 - val_accuracy: 0.6197\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.3150 - accuracy: 0.8978 - val_loss: 0.9177 - val_accuracy: 0.7175\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.2998 - accuracy: 0.9039 - val_loss: 1.4925 - val_accuracy: 0.6016\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2991 - accuracy: 0.9020 - val_loss: 0.7876 - val_accuracy: 0.7496\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2881 - accuracy: 0.9070 - val_loss: 1.2688 - val_accuracy: 0.6638\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2770 - accuracy: 0.9104 - val_loss: 1.2842 - val_accuracy: 0.6253\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2791 - accuracy: 0.9081 - val_loss: 1.3821 - val_accuracy: 0.6152\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2633 - accuracy: 0.9161 - val_loss: 1.0403 - val_accuracy: 0.6942\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.2506 - accuracy: 0.9191 - val_loss: 1.1537 - val_accuracy: 0.7018\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2464 - accuracy: 0.9196 - val_loss: 0.8330 - val_accuracy: 0.7660\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2428 - accuracy: 0.9205 - val_loss: 1.1624 - val_accuracy: 0.6759\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2266 - accuracy: 0.9280 - val_loss: 1.0431 - val_accuracy: 0.7129\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2236 - accuracy: 0.9282 - val_loss: 1.6539 - val_accuracy: 0.6207\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2216 - accuracy: 0.9285 - val_loss: 1.0635 - val_accuracy: 0.6990\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2046 - accuracy: 0.9355 - val_loss: 1.5488 - val_accuracy: 0.6486\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2048 - accuracy: 0.9318 - val_loss: 1.1470 - val_accuracy: 0.7020\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2132 - accuracy: 0.9294 - val_loss: 1.1728 - val_accuracy: 0.7021\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.2156 - accuracy: 0.9306 - val_loss: 1.1069 - val_accuracy: 0.7106\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1872 - accuracy: 0.9400 - val_loss: 0.9083 - val_accuracy: 0.7524\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1794 - accuracy: 0.9408 - val_loss: 1.4811 - val_accuracy: 0.6622\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1919 - accuracy: 0.9371 - val_loss: 1.0956 - val_accuracy: 0.7197\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1907 - accuracy: 0.9378 - val_loss: 1.0875 - val_accuracy: 0.7132\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1794 - accuracy: 0.9410 - val_loss: 0.9777 - val_accuracy: 0.7388\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1824 - accuracy: 0.9394 - val_loss: 1.0976 - val_accuracy: 0.7239\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1737 - accuracy: 0.9430 - val_loss: 0.7905 - val_accuracy: 0.7840\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1632 - accuracy: 0.9481 - val_loss: 1.5093 - val_accuracy: 0.6772\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1667 - accuracy: 0.9447 - val_loss: 1.2550 - val_accuracy: 0.7085\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1546 - accuracy: 0.9484 - val_loss: 1.1234 - val_accuracy: 0.7296\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1521 - accuracy: 0.9508 - val_loss: 1.1578 - val_accuracy: 0.7149\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1595 - accuracy: 0.9475 - val_loss: 1.2534 - val_accuracy: 0.7269\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1588 - accuracy: 0.9480 - val_loss: 1.0752 - val_accuracy: 0.7448\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1410 - accuracy: 0.9535 - val_loss: 1.6254 - val_accuracy: 0.6601\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1549 - accuracy: 0.9486 - val_loss: 0.9235 - val_accuracy: 0.7695\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1436 - accuracy: 0.9518 - val_loss: 0.6784 - val_accuracy: 0.8106\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.1508 - accuracy: 0.9507 - val_loss: 1.1589 - val_accuracy: 0.7451\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1432 - accuracy: 0.9525 - val_loss: 1.2747 - val_accuracy: 0.7223\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1278 - accuracy: 0.9578 - val_loss: 0.8118 - val_accuracy: 0.7944\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1403 - accuracy: 0.9539 - val_loss: 0.9293 - val_accuracy: 0.7586\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1385 - accuracy: 0.9554 - val_loss: 1.0947 - val_accuracy: 0.7577\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1415 - accuracy: 0.9528 - val_loss: 1.3872 - val_accuracy: 0.7318\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1294 - accuracy: 0.9566 - val_loss: 0.9057 - val_accuracy: 0.7838\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1282 - accuracy: 0.9563 - val_loss: 0.8890 - val_accuracy: 0.7943\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1234 - accuracy: 0.9581 - val_loss: 1.1527 - val_accuracy: 0.7504\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1313 - accuracy: 0.9560 - val_loss: 0.9918 - val_accuracy: 0.7648\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1173 - accuracy: 0.9611 - val_loss: 0.9671 - val_accuracy: 0.7596\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1364 - accuracy: 0.9554 - val_loss: 0.8000 - val_accuracy: 0.8108\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.1107 - accuracy: 0.9631 - val_loss: 1.0319 - val_accuracy: 0.7606\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1129 - accuracy: 0.9622 - val_loss: 1.1580 - val_accuracy: 0.7474\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1085 - accuracy: 0.9635 - val_loss: 1.0283 - val_accuracy: 0.7627\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1198 - accuracy: 0.9594 - val_loss: 1.1549 - val_accuracy: 0.7336\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1187 - accuracy: 0.9603 - val_loss: 1.2560 - val_accuracy: 0.7395\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1116 - accuracy: 0.9636 - val_loss: 0.9066 - val_accuracy: 0.7892\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1082 - accuracy: 0.9642 - val_loss: 1.1253 - val_accuracy: 0.7606\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1071 - accuracy: 0.9659 - val_loss: 1.1118 - val_accuracy: 0.7486\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0988 - accuracy: 0.9665 - val_loss: 1.2527 - val_accuracy: 0.7403\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1126 - accuracy: 0.9630 - val_loss: 0.7439 - val_accuracy: 0.8241\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1048 - accuracy: 0.9639 - val_loss: 0.9888 - val_accuracy: 0.7922\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0961 - accuracy: 0.9681 - val_loss: 0.8171 - val_accuracy: 0.7972\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1042 - accuracy: 0.9662 - val_loss: 0.8105 - val_accuracy: 0.7951\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0997 - accuracy: 0.9661 - val_loss: 1.1635 - val_accuracy: 0.7741\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0994 - accuracy: 0.9672 - val_loss: 1.0336 - val_accuracy: 0.7685\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0950 - accuracy: 0.9679 - val_loss: 0.7576 - val_accuracy: 0.8179\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1147 - accuracy: 0.9618 - val_loss: 1.0805 - val_accuracy: 0.7751\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0974 - accuracy: 0.9680 - val_loss: 0.9541 - val_accuracy: 0.7929\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1084 - accuracy: 0.9645 - val_loss: 0.6266 - val_accuracy: 0.8521\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0871 - accuracy: 0.9710 - val_loss: 0.9951 - val_accuracy: 0.7848\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0925 - accuracy: 0.9687 - val_loss: 1.1380 - val_accuracy: 0.7649\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0859 - accuracy: 0.9717 - val_loss: 1.4141 - val_accuracy: 0.7361\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0874 - accuracy: 0.9703 - val_loss: 1.0907 - val_accuracy: 0.7681\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0844 - accuracy: 0.9709 - val_loss: 1.4420 - val_accuracy: 0.7336\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0819 - accuracy: 0.9723 - val_loss: 1.3284 - val_accuracy: 0.7580\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.1003 - accuracy: 0.9659 - val_loss: 0.7377 - val_accuracy: 0.8297\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0839 - accuracy: 0.9726 - val_loss: 0.9362 - val_accuracy: 0.8020\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0872 - accuracy: 0.9717 - val_loss: 0.6999 - val_accuracy: 0.8359\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0860 - accuracy: 0.9716 - val_loss: 0.8591 - val_accuracy: 0.8083\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.0747 - accuracy: 0.9757 - val_loss: 0.8779 - val_accuracy: 0.8103\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.0743 - accuracy: 0.9751 - val_loss: 1.4909 - val_accuracy: 0.7368\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.0890 - accuracy: 0.9692 - val_loss: 1.0718 - val_accuracy: 0.7867\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0757 - accuracy: 0.9756 - val_loss: 0.6951 - val_accuracy: 0.8417\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0713 - accuracy: 0.9763 - val_loss: 0.9775 - val_accuracy: 0.8036\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0753 - accuracy: 0.9741 - val_loss: 0.7881 - val_accuracy: 0.8328\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0744 - accuracy: 0.9743 - val_loss: 1.0602 - val_accuracy: 0.7847\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 4s 21ms/step - loss: 0.0695 - accuracy: 0.9767 - val_loss: 0.6664 - val_accuracy: 0.8515\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0741 - accuracy: 0.9751 - val_loss: 1.0473 - val_accuracy: 0.8089\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0773 - accuracy: 0.9734 - val_loss: 1.2078 - val_accuracy: 0.7683\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.0846 - accuracy: 0.9726 - val_loss: 0.8550 - val_accuracy: 0.8205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRr4S0af2bqk",
        "outputId": "5ba9f506-da64-4e07-b74e-850407a2fda3"
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results5 = model5.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8550 - accuracy: 0.8205\n",
            "Validation accuracy: 82.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPo2CHbd2hnh"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "*   Batch normalization didn't result in improvement of score.\n",
        "*   ReLU activations, changing number of activators, Adam optimizers achieved the best score.\n",
        "\n",
        "Let's try Batch Normalization with  Dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8poapebQ2xRy"
      },
      "source": [
        "## **Dropout**\n",
        "\n",
        "**NN model, relu activations, SGD optimizers with weight initializers, batch normalization and dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp8Cw_rJ2fZE",
        "outputId": "e916627f-e2de-472c-9403-1c0b901f6ec8"
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model6 = Sequential()\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model6.add(Dense(512, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization()) \n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model6.add(Dense(256, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model6.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model6.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 4 - adding fourth hidden layer\n",
        "model6.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model6.add(Dense(10, kernel_initializer = 'he_normal',bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model6.add(Activation('softmax'))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfaV8zQ43FbD",
        "outputId": "91ba0d37-8673-40c8-d9a7-0f73babf989c"
      },
      "source": [
        "model6.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 703,658\n",
            "Trainable params: 701,674\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhlEvT1u3QvC",
        "outputId": "e86a665a-6ce2-4eed-ae97-21884027b351"
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model6.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 10s 42ms/step - loss: 2.7871 - accuracy: 0.1051 - val_loss: 2.3137 - val_accuracy: 0.1273\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.4490 - accuracy: 0.1181 - val_loss: 2.2354 - val_accuracy: 0.1702\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 2.3463 - accuracy: 0.1393 - val_loss: 2.1464 - val_accuracy: 0.2350\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.2731 - accuracy: 0.1639 - val_loss: 2.0955 - val_accuracy: 0.2694\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.2140 - accuracy: 0.1883 - val_loss: 2.0248 - val_accuracy: 0.3089\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.1494 - accuracy: 0.2106 - val_loss: 1.9216 - val_accuracy: 0.3704\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.0896 - accuracy: 0.2396 - val_loss: 1.8662 - val_accuracy: 0.3909\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 2.0234 - accuracy: 0.2712 - val_loss: 1.7975 - val_accuracy: 0.4213\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.9597 - accuracy: 0.2945 - val_loss: 1.7312 - val_accuracy: 0.4405\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.8960 - accuracy: 0.3191 - val_loss: 1.6517 - val_accuracy: 0.4654\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.8418 - accuracy: 0.3441 - val_loss: 1.5869 - val_accuracy: 0.4900\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.7897 - accuracy: 0.3588 - val_loss: 1.5102 - val_accuracy: 0.5252\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.7352 - accuracy: 0.3800 - val_loss: 1.4515 - val_accuracy: 0.5354\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.6885 - accuracy: 0.3987 - val_loss: 1.4073 - val_accuracy: 0.5515\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.6424 - accuracy: 0.4220 - val_loss: 1.3886 - val_accuracy: 0.5560\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.6000 - accuracy: 0.4332 - val_loss: 1.2869 - val_accuracy: 0.6004\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.5475 - accuracy: 0.4571 - val_loss: 1.2516 - val_accuracy: 0.6139\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.5152 - accuracy: 0.4704 - val_loss: 1.2061 - val_accuracy: 0.6243\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.4732 - accuracy: 0.4884 - val_loss: 1.2193 - val_accuracy: 0.6176\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.4412 - accuracy: 0.5047 - val_loss: 1.1453 - val_accuracy: 0.6447\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.3988 - accuracy: 0.5168 - val_loss: 1.1085 - val_accuracy: 0.6621\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.3712 - accuracy: 0.5289 - val_loss: 1.0497 - val_accuracy: 0.6760\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.3433 - accuracy: 0.5468 - val_loss: 1.0424 - val_accuracy: 0.6699\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.3174 - accuracy: 0.5527 - val_loss: 1.0126 - val_accuracy: 0.6860\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.2905 - accuracy: 0.5608 - val_loss: 1.0261 - val_accuracy: 0.6813\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.2761 - accuracy: 0.5666 - val_loss: 0.9699 - val_accuracy: 0.7018\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.2486 - accuracy: 0.5777 - val_loss: 0.9718 - val_accuracy: 0.6985\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.2242 - accuracy: 0.5896 - val_loss: 0.9325 - val_accuracy: 0.7103\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.2048 - accuracy: 0.5951 - val_loss: 0.9755 - val_accuracy: 0.6894\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.1927 - accuracy: 0.6044 - val_loss: 0.9471 - val_accuracy: 0.7020\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.1672 - accuracy: 0.6136 - val_loss: 0.8873 - val_accuracy: 0.7236\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.1534 - accuracy: 0.6147 - val_loss: 0.8608 - val_accuracy: 0.7320\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.1307 - accuracy: 0.6255 - val_loss: 0.8598 - val_accuracy: 0.7261\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.1220 - accuracy: 0.6247 - val_loss: 0.8461 - val_accuracy: 0.7319\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 1.1036 - accuracy: 0.6345 - val_loss: 0.8340 - val_accuracy: 0.7397\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.0925 - accuracy: 0.6397 - val_loss: 0.8193 - val_accuracy: 0.7484\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.0636 - accuracy: 0.6498 - val_loss: 0.8116 - val_accuracy: 0.7485\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.0646 - accuracy: 0.6506 - val_loss: 0.7813 - val_accuracy: 0.7577\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.0446 - accuracy: 0.6581 - val_loss: 0.7941 - val_accuracy: 0.7501\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.0419 - accuracy: 0.6581 - val_loss: 0.8073 - val_accuracy: 0.7472\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 1.0282 - accuracy: 0.6650 - val_loss: 0.8003 - val_accuracy: 0.7479\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.0181 - accuracy: 0.6684 - val_loss: 0.7762 - val_accuracy: 0.7572\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 1.0072 - accuracy: 0.6735 - val_loss: 0.7542 - val_accuracy: 0.7666\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9857 - accuracy: 0.6827 - val_loss: 0.7734 - val_accuracy: 0.7534\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.9955 - accuracy: 0.6774 - val_loss: 0.7482 - val_accuracy: 0.7691\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.9730 - accuracy: 0.6867 - val_loss: 0.7634 - val_accuracy: 0.7607\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9778 - accuracy: 0.6839 - val_loss: 0.7196 - val_accuracy: 0.7834\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9365 - accuracy: 0.6988 - val_loss: 0.7126 - val_accuracy: 0.7827\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9372 - accuracy: 0.6990 - val_loss: 0.7709 - val_accuracy: 0.7532\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9327 - accuracy: 0.7013 - val_loss: 0.7421 - val_accuracy: 0.7675\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9307 - accuracy: 0.6998 - val_loss: 0.7146 - val_accuracy: 0.7722\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.9252 - accuracy: 0.7039 - val_loss: 0.7236 - val_accuracy: 0.7780\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9195 - accuracy: 0.7089 - val_loss: 0.6438 - val_accuracy: 0.8082\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8872 - accuracy: 0.7163 - val_loss: 0.6462 - val_accuracy: 0.8030\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9008 - accuracy: 0.7149 - val_loss: 0.6662 - val_accuracy: 0.7948\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8802 - accuracy: 0.7225 - val_loss: 0.6708 - val_accuracy: 0.7932\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8792 - accuracy: 0.7217 - val_loss: 0.6653 - val_accuracy: 0.7926\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8813 - accuracy: 0.7237 - val_loss: 0.6980 - val_accuracy: 0.7839\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8584 - accuracy: 0.7273 - val_loss: 0.7465 - val_accuracy: 0.7579\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8423 - accuracy: 0.7374 - val_loss: 0.6112 - val_accuracy: 0.8157\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8309 - accuracy: 0.7418 - val_loss: 0.6215 - val_accuracy: 0.8051\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8369 - accuracy: 0.7390 - val_loss: 0.6164 - val_accuracy: 0.8127\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8297 - accuracy: 0.7409 - val_loss: 0.6746 - val_accuracy: 0.7896\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.8258 - accuracy: 0.7424 - val_loss: 0.7249 - val_accuracy: 0.7757\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8116 - accuracy: 0.7479 - val_loss: 0.6091 - val_accuracy: 0.8106\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8010 - accuracy: 0.7486 - val_loss: 0.6629 - val_accuracy: 0.7888\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8059 - accuracy: 0.7498 - val_loss: 0.6908 - val_accuracy: 0.7856\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7974 - accuracy: 0.7482 - val_loss: 0.5985 - val_accuracy: 0.8140\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7915 - accuracy: 0.7550 - val_loss: 0.6278 - val_accuracy: 0.8041\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.7889 - accuracy: 0.7536 - val_loss: 0.5832 - val_accuracy: 0.8202\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7890 - accuracy: 0.7539 - val_loss: 0.5829 - val_accuracy: 0.8192\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7703 - accuracy: 0.7597 - val_loss: 0.5503 - val_accuracy: 0.8306\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7596 - accuracy: 0.7657 - val_loss: 0.5572 - val_accuracy: 0.8299\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7727 - accuracy: 0.7620 - val_loss: 0.6663 - val_accuracy: 0.7944\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7501 - accuracy: 0.7692 - val_loss: 0.6207 - val_accuracy: 0.8086\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7462 - accuracy: 0.7693 - val_loss: 0.5618 - val_accuracy: 0.8276\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.7465 - accuracy: 0.7672 - val_loss: 0.5741 - val_accuracy: 0.8222\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.7480 - accuracy: 0.7684 - val_loss: 0.5577 - val_accuracy: 0.8301\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.7352 - accuracy: 0.7712 - val_loss: 0.5730 - val_accuracy: 0.8223\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.7363 - accuracy: 0.7707 - val_loss: 0.5414 - val_accuracy: 0.8352\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7346 - accuracy: 0.7746 - val_loss: 0.5410 - val_accuracy: 0.8328\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7240 - accuracy: 0.7740 - val_loss: 0.5498 - val_accuracy: 0.8306\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7203 - accuracy: 0.7777 - val_loss: 0.5266 - val_accuracy: 0.8363\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7154 - accuracy: 0.7798 - val_loss: 0.5359 - val_accuracy: 0.8358\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7186 - accuracy: 0.7808 - val_loss: 0.5553 - val_accuracy: 0.8281\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7161 - accuracy: 0.7841 - val_loss: 0.4985 - val_accuracy: 0.8512\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7062 - accuracy: 0.7855 - val_loss: 0.5228 - val_accuracy: 0.8403\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7031 - accuracy: 0.7859 - val_loss: 0.5430 - val_accuracy: 0.8343\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6909 - accuracy: 0.7909 - val_loss: 0.5195 - val_accuracy: 0.8418\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.6874 - accuracy: 0.7907 - val_loss: 0.5068 - val_accuracy: 0.8446\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6757 - accuracy: 0.7945 - val_loss: 0.5266 - val_accuracy: 0.8386\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6751 - accuracy: 0.7955 - val_loss: 0.5272 - val_accuracy: 0.8353\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.6814 - accuracy: 0.7930 - val_loss: 0.5286 - val_accuracy: 0.8319\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6792 - accuracy: 0.7912 - val_loss: 0.5504 - val_accuracy: 0.8311\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6769 - accuracy: 0.7931 - val_loss: 0.5780 - val_accuracy: 0.8212\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6723 - accuracy: 0.7953 - val_loss: 0.5099 - val_accuracy: 0.8442\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.6623 - accuracy: 0.8007 - val_loss: 0.5434 - val_accuracy: 0.8342\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.6628 - accuracy: 0.8002 - val_loss: 0.5428 - val_accuracy: 0.8323\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.6638 - accuracy: 0.7978 - val_loss: 0.6511 - val_accuracy: 0.8003\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 8s 37ms/step - loss: 0.6577 - accuracy: 0.7970 - val_loss: 0.5061 - val_accuracy: 0.8439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61LmMpvR3UZi",
        "outputId": "106d6554-1e05-45b5-80bc-ce4da742dce8"
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
        "results6 = model6.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5061 - accuracy: 0.8439\n",
            "Validation accuracy: 84.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjEzdqCf3Xh3",
        "outputId": "8e64283e-c5b3-46b9-efef-800de1cf88bd"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model6.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 10s 40ms/step - loss: 1.1438 - accuracy: 0.6454 - val_loss: 1.5554 - val_accuracy: 0.4544\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.9114 - accuracy: 0.7135 - val_loss: 1.4543 - val_accuracy: 0.5175\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8632 - accuracy: 0.7343 - val_loss: 1.3675 - val_accuracy: 0.5439\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.8052 - accuracy: 0.7544 - val_loss: 1.1486 - val_accuracy: 0.6391\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7721 - accuracy: 0.7658 - val_loss: 0.9103 - val_accuracy: 0.7094\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7367 - accuracy: 0.7758 - val_loss: 0.8888 - val_accuracy: 0.7351\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.7128 - accuracy: 0.7856 - val_loss: 1.2597 - val_accuracy: 0.5787\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6973 - accuracy: 0.7891 - val_loss: 0.9527 - val_accuracy: 0.6924\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6547 - accuracy: 0.7995 - val_loss: 1.1708 - val_accuracy: 0.6111\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6577 - accuracy: 0.8043 - val_loss: 1.1962 - val_accuracy: 0.6173\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.6336 - accuracy: 0.8080 - val_loss: 1.2904 - val_accuracy: 0.5688\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.6240 - accuracy: 0.8109 - val_loss: 0.8426 - val_accuracy: 0.7351\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.6207 - accuracy: 0.8100 - val_loss: 0.8795 - val_accuracy: 0.7258\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.5847 - accuracy: 0.8217 - val_loss: 0.9345 - val_accuracy: 0.6934\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5827 - accuracy: 0.8254 - val_loss: 1.1219 - val_accuracy: 0.6257\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.5841 - accuracy: 0.8271 - val_loss: 0.9758 - val_accuracy: 0.6870\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.5671 - accuracy: 0.8276 - val_loss: 1.1455 - val_accuracy: 0.6187\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5577 - accuracy: 0.8323 - val_loss: 0.9304 - val_accuracy: 0.7037\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5412 - accuracy: 0.8357 - val_loss: 0.7396 - val_accuracy: 0.7735\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5392 - accuracy: 0.8381 - val_loss: 0.8559 - val_accuracy: 0.7200\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.5322 - accuracy: 0.8387 - val_loss: 0.8484 - val_accuracy: 0.7327\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5238 - accuracy: 0.8404 - val_loss: 1.1190 - val_accuracy: 0.6446\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.5237 - accuracy: 0.8415 - val_loss: 0.8212 - val_accuracy: 0.7735\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.5147 - accuracy: 0.8478 - val_loss: 0.8139 - val_accuracy: 0.7416\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.5036 - accuracy: 0.8459 - val_loss: 0.8383 - val_accuracy: 0.7325\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.4914 - accuracy: 0.8521 - val_loss: 0.7662 - val_accuracy: 0.7529\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.4835 - accuracy: 0.8537 - val_loss: 1.3027 - val_accuracy: 0.6032\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.4745 - accuracy: 0.8575 - val_loss: 0.8173 - val_accuracy: 0.7372\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4740 - accuracy: 0.8573 - val_loss: 0.9811 - val_accuracy: 0.6674\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4797 - accuracy: 0.8554 - val_loss: 0.7453 - val_accuracy: 0.7571\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4605 - accuracy: 0.8620 - val_loss: 0.7725 - val_accuracy: 0.7588\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.4594 - accuracy: 0.8624 - val_loss: 0.7621 - val_accuracy: 0.7498\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4551 - accuracy: 0.8635 - val_loss: 0.6683 - val_accuracy: 0.7955\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.4252 - accuracy: 0.8736 - val_loss: 0.6847 - val_accuracy: 0.7883\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4430 - accuracy: 0.8675 - val_loss: 0.7764 - val_accuracy: 0.7587\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4532 - accuracy: 0.8633 - val_loss: 0.7848 - val_accuracy: 0.7395\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4372 - accuracy: 0.8670 - val_loss: 0.7630 - val_accuracy: 0.7601\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4413 - accuracy: 0.8661 - val_loss: 0.7065 - val_accuracy: 0.7794\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4357 - accuracy: 0.8694 - val_loss: 0.8697 - val_accuracy: 0.7254\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4212 - accuracy: 0.8746 - val_loss: 0.7205 - val_accuracy: 0.7643\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4221 - accuracy: 0.8724 - val_loss: 1.0052 - val_accuracy: 0.6909\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4120 - accuracy: 0.8766 - val_loss: 0.8243 - val_accuracy: 0.7192\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4108 - accuracy: 0.8744 - val_loss: 0.6595 - val_accuracy: 0.8059\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4125 - accuracy: 0.8765 - val_loss: 0.6751 - val_accuracy: 0.7826\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.4063 - accuracy: 0.8780 - val_loss: 0.8970 - val_accuracy: 0.7104\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3913 - accuracy: 0.8817 - val_loss: 0.9365 - val_accuracy: 0.7071\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3833 - accuracy: 0.8854 - val_loss: 0.7281 - val_accuracy: 0.7604\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3833 - accuracy: 0.8829 - val_loss: 0.5788 - val_accuracy: 0.8127\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3762 - accuracy: 0.8872 - val_loss: 0.6088 - val_accuracy: 0.7980\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3907 - accuracy: 0.8835 - val_loss: 1.0817 - val_accuracy: 0.6574\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3816 - accuracy: 0.8858 - val_loss: 0.6420 - val_accuracy: 0.7986\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3771 - accuracy: 0.8883 - val_loss: 0.7706 - val_accuracy: 0.7622\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3619 - accuracy: 0.8902 - val_loss: 0.7343 - val_accuracy: 0.7664\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3761 - accuracy: 0.8846 - val_loss: 0.8015 - val_accuracy: 0.7413\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3818 - accuracy: 0.8869 - val_loss: 0.5695 - val_accuracy: 0.8200\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3649 - accuracy: 0.8894 - val_loss: 0.8087 - val_accuracy: 0.7279\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3606 - accuracy: 0.8901 - val_loss: 0.6534 - val_accuracy: 0.7885\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 9s 41ms/step - loss: 0.3518 - accuracy: 0.8941 - val_loss: 0.5799 - val_accuracy: 0.8132\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3670 - accuracy: 0.8872 - val_loss: 0.6509 - val_accuracy: 0.7871\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3361 - accuracy: 0.8984 - val_loss: 0.6410 - val_accuracy: 0.7911\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3532 - accuracy: 0.8932 - val_loss: 0.5657 - val_accuracy: 0.8174\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3433 - accuracy: 0.8961 - val_loss: 0.6616 - val_accuracy: 0.7808\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3472 - accuracy: 0.8943 - val_loss: 0.8352 - val_accuracy: 0.7226\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3320 - accuracy: 0.9001 - val_loss: 0.6560 - val_accuracy: 0.7874\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3366 - accuracy: 0.8983 - val_loss: 0.6528 - val_accuracy: 0.7865\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3265 - accuracy: 0.9000 - val_loss: 0.7327 - val_accuracy: 0.7592\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3280 - accuracy: 0.9015 - val_loss: 0.5294 - val_accuracy: 0.8349\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3177 - accuracy: 0.9059 - val_loss: 0.7723 - val_accuracy: 0.7550\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3297 - accuracy: 0.8997 - val_loss: 0.4948 - val_accuracy: 0.8511\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3186 - accuracy: 0.9039 - val_loss: 0.6034 - val_accuracy: 0.8059\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3339 - accuracy: 0.8980 - val_loss: 0.6617 - val_accuracy: 0.7863\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3223 - accuracy: 0.9002 - val_loss: 0.5784 - val_accuracy: 0.8114\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3162 - accuracy: 0.9026 - val_loss: 0.6688 - val_accuracy: 0.7867\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3317 - accuracy: 0.8967 - val_loss: 0.5323 - val_accuracy: 0.8301\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3161 - accuracy: 0.9061 - val_loss: 0.6663 - val_accuracy: 0.7781\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3160 - accuracy: 0.9033 - val_loss: 0.5872 - val_accuracy: 0.8033\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 8s 38ms/step - loss: 0.3152 - accuracy: 0.9033 - val_loss: 0.5166 - val_accuracy: 0.8371\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3039 - accuracy: 0.9090 - val_loss: 0.4601 - val_accuracy: 0.8529\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3076 - accuracy: 0.9069 - val_loss: 0.5382 - val_accuracy: 0.8260\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3064 - accuracy: 0.9066 - val_loss: 0.7144 - val_accuracy: 0.7781\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 8s 40ms/step - loss: 0.3050 - accuracy: 0.9076 - val_loss: 0.7173 - val_accuracy: 0.7686\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3043 - accuracy: 0.9081 - val_loss: 0.4431 - val_accuracy: 0.8589\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2931 - accuracy: 0.9110 - val_loss: 0.5725 - val_accuracy: 0.8149\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3062 - accuracy: 0.9077 - val_loss: 0.5973 - val_accuracy: 0.8082\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2824 - accuracy: 0.9145 - val_loss: 0.6698 - val_accuracy: 0.7766\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.3035 - accuracy: 0.9069 - val_loss: 0.5736 - val_accuracy: 0.8158\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2847 - accuracy: 0.9142 - val_loss: 0.6411 - val_accuracy: 0.7935\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2926 - accuracy: 0.9110 - val_loss: 0.6313 - val_accuracy: 0.7996\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2945 - accuracy: 0.9108 - val_loss: 0.6138 - val_accuracy: 0.8005\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2865 - accuracy: 0.9133 - val_loss: 0.6250 - val_accuracy: 0.7931\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2858 - accuracy: 0.9152 - val_loss: 0.4944 - val_accuracy: 0.8394\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2857 - accuracy: 0.9136 - val_loss: 0.5458 - val_accuracy: 0.8262\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2817 - accuracy: 0.9160 - val_loss: 0.9887 - val_accuracy: 0.7336\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2852 - accuracy: 0.9136 - val_loss: 0.5662 - val_accuracy: 0.8179\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2838 - accuracy: 0.9129 - val_loss: 0.4580 - val_accuracy: 0.8555\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2769 - accuracy: 0.9155 - val_loss: 0.5751 - val_accuracy: 0.8174\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2820 - accuracy: 0.9147 - val_loss: 0.5048 - val_accuracy: 0.8333\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2731 - accuracy: 0.9178 - val_loss: 0.4744 - val_accuracy: 0.8487\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2649 - accuracy: 0.9187 - val_loss: 0.5757 - val_accuracy: 0.8172\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 0.2709 - accuracy: 0.9167 - val_loss: 0.5062 - val_accuracy: 0.8392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi5LU1UX3Zxw",
        "outputId": "0b8bed9c-7b6a-480e-ddda-22cb7c833cbb"
      },
      "source": [
        "print('NN model with dropout - adam optimizer'); print('--'*40)\n",
        "results6 = model6.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5062 - accuracy: 0.8392\n",
            "Validation accuracy: 83.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1V-hslG3g_B"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "\n",
        "*   Didn't result in any improvement of score.\n",
        "*   NN model, relu activations, SGD optimizers with weight initializers and batch normalization is still the best model.\n",
        "\n",
        "Let's try Batch Normalization and Dropout with Adam optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV9RhlJe3xna"
      },
      "source": [
        "## **Prediction on test dataset using Model 3 - ReLU activations, Adam optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amVrstrR3eX8",
        "outputId": "72ecb7ec-49f8-4daa-9ca9-c0c29175efa5"
      },
      "source": [
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model3 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model3.add(Dense(256, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model3.add(Dense(128))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - Adding second hidden layer\n",
        "model3.add(Dense(64))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model3.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmI_ukdJ38Ff",
        "outputId": "2a5049e6-c71a-4726-ae86-3cf392eb1122"
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 2.3123 - accuracy: 0.1051 - val_loss: 2.2075 - val_accuracy: 0.1655\n",
            "Epoch 2/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 2.0376 - accuracy: 0.2626 - val_loss: 1.5049 - val_accuracy: 0.5069\n",
            "Epoch 3/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 1.4453 - accuracy: 0.5240 - val_loss: 1.2718 - val_accuracy: 0.5929\n",
            "Epoch 4/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 1.2240 - accuracy: 0.6136 - val_loss: 1.1598 - val_accuracy: 0.6342\n",
            "Epoch 5/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.1127 - accuracy: 0.6525 - val_loss: 1.0187 - val_accuracy: 0.6878\n",
            "Epoch 6/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 1.0129 - accuracy: 0.6854 - val_loss: 0.9758 - val_accuracy: 0.7000\n",
            "Epoch 7/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.9686 - accuracy: 0.7007 - val_loss: 0.9654 - val_accuracy: 0.6982\n",
            "Epoch 8/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.9385 - accuracy: 0.7107 - val_loss: 0.9348 - val_accuracy: 0.7113\n",
            "Epoch 9/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.9025 - accuracy: 0.7217 - val_loss: 0.8983 - val_accuracy: 0.7217\n",
            "Epoch 10/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.8574 - accuracy: 0.7339 - val_loss: 0.8528 - val_accuracy: 0.7359\n",
            "Epoch 11/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.8338 - accuracy: 0.7427 - val_loss: 0.8272 - val_accuracy: 0.7438\n",
            "Epoch 12/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.8057 - accuracy: 0.7521 - val_loss: 0.8158 - val_accuracy: 0.7490\n",
            "Epoch 13/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7883 - accuracy: 0.7574 - val_loss: 0.7612 - val_accuracy: 0.7667\n",
            "Epoch 14/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.7682 - accuracy: 0.7644 - val_loss: 0.7490 - val_accuracy: 0.7703\n",
            "Epoch 15/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7554 - accuracy: 0.7664 - val_loss: 0.7312 - val_accuracy: 0.7772\n",
            "Epoch 16/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7323 - accuracy: 0.7737 - val_loss: 0.7603 - val_accuracy: 0.7653\n",
            "Epoch 17/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.7231 - accuracy: 0.7770 - val_loss: 0.7028 - val_accuracy: 0.7869\n",
            "Epoch 18/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6864 - accuracy: 0.7877 - val_loss: 0.7212 - val_accuracy: 0.7797\n",
            "Epoch 19/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.6913 - accuracy: 0.7887 - val_loss: 0.7404 - val_accuracy: 0.7691\n",
            "Epoch 20/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6821 - accuracy: 0.7907 - val_loss: 0.6550 - val_accuracy: 0.8014\n",
            "Epoch 21/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6554 - accuracy: 0.7968 - val_loss: 0.6739 - val_accuracy: 0.7961\n",
            "Epoch 22/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6529 - accuracy: 0.7988 - val_loss: 0.6682 - val_accuracy: 0.7980\n",
            "Epoch 23/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6326 - accuracy: 0.8052 - val_loss: 0.6342 - val_accuracy: 0.8082\n",
            "Epoch 24/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.6199 - accuracy: 0.8078 - val_loss: 0.6628 - val_accuracy: 0.7991\n",
            "Epoch 25/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.6239 - accuracy: 0.8090 - val_loss: 0.6184 - val_accuracy: 0.8143\n",
            "Epoch 26/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.6110 - accuracy: 0.8131 - val_loss: 0.6296 - val_accuracy: 0.8063\n",
            "Epoch 27/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5959 - accuracy: 0.8136 - val_loss: 0.6121 - val_accuracy: 0.8135\n",
            "Epoch 28/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5851 - accuracy: 0.8179 - val_loss: 0.6141 - val_accuracy: 0.8130\n",
            "Epoch 29/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5736 - accuracy: 0.8240 - val_loss: 0.5789 - val_accuracy: 0.8266\n",
            "Epoch 30/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5684 - accuracy: 0.8242 - val_loss: 0.5622 - val_accuracy: 0.8299\n",
            "Epoch 31/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5394 - accuracy: 0.8362 - val_loss: 0.5864 - val_accuracy: 0.8215\n",
            "Epoch 32/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5473 - accuracy: 0.8308 - val_loss: 0.5753 - val_accuracy: 0.8243\n",
            "Epoch 33/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5377 - accuracy: 0.8319 - val_loss: 0.5992 - val_accuracy: 0.8183\n",
            "Epoch 34/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5385 - accuracy: 0.8335 - val_loss: 0.6203 - val_accuracy: 0.8089\n",
            "Epoch 35/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5304 - accuracy: 0.8371 - val_loss: 0.5524 - val_accuracy: 0.8328\n",
            "Epoch 36/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5168 - accuracy: 0.8385 - val_loss: 0.5287 - val_accuracy: 0.8413\n",
            "Epoch 37/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5124 - accuracy: 0.8413 - val_loss: 0.5536 - val_accuracy: 0.8290\n",
            "Epoch 38/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.5004 - accuracy: 0.8445 - val_loss: 0.5572 - val_accuracy: 0.8295\n",
            "Epoch 39/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5054 - accuracy: 0.8439 - val_loss: 0.5274 - val_accuracy: 0.8404\n",
            "Epoch 40/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.5001 - accuracy: 0.8414 - val_loss: 0.5328 - val_accuracy: 0.8383\n",
            "Epoch 41/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4950 - accuracy: 0.8449 - val_loss: 0.5205 - val_accuracy: 0.8420\n",
            "Epoch 42/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4893 - accuracy: 0.8488 - val_loss: 0.5488 - val_accuracy: 0.8330\n",
            "Epoch 43/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4811 - accuracy: 0.8505 - val_loss: 0.5355 - val_accuracy: 0.8389\n",
            "Epoch 44/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4830 - accuracy: 0.8485 - val_loss: 0.5166 - val_accuracy: 0.8429\n",
            "Epoch 45/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4719 - accuracy: 0.8535 - val_loss: 0.5125 - val_accuracy: 0.8446\n",
            "Epoch 46/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4784 - accuracy: 0.8524 - val_loss: 0.5102 - val_accuracy: 0.8464\n",
            "Epoch 47/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4538 - accuracy: 0.8581 - val_loss: 0.5232 - val_accuracy: 0.8415\n",
            "Epoch 48/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4493 - accuracy: 0.8591 - val_loss: 0.5023 - val_accuracy: 0.8488\n",
            "Epoch 49/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4423 - accuracy: 0.8603 - val_loss: 0.5151 - val_accuracy: 0.8445\n",
            "Epoch 50/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4564 - accuracy: 0.8546 - val_loss: 0.5141 - val_accuracy: 0.8451\n",
            "Epoch 51/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4541 - accuracy: 0.8543 - val_loss: 0.4889 - val_accuracy: 0.8536\n",
            "Epoch 52/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4431 - accuracy: 0.8621 - val_loss: 0.4859 - val_accuracy: 0.8545\n",
            "Epoch 53/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4282 - accuracy: 0.8679 - val_loss: 0.5422 - val_accuracy: 0.8359\n",
            "Epoch 54/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4423 - accuracy: 0.8606 - val_loss: 0.5015 - val_accuracy: 0.8478\n",
            "Epoch 55/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4197 - accuracy: 0.8688 - val_loss: 0.4798 - val_accuracy: 0.8566\n",
            "Epoch 56/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4310 - accuracy: 0.8636 - val_loss: 0.4693 - val_accuracy: 0.8594\n",
            "Epoch 57/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4283 - accuracy: 0.8666 - val_loss: 0.5344 - val_accuracy: 0.8352\n",
            "Epoch 58/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4253 - accuracy: 0.8668 - val_loss: 0.4799 - val_accuracy: 0.8547\n",
            "Epoch 59/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4164 - accuracy: 0.8678 - val_loss: 0.4966 - val_accuracy: 0.8500\n",
            "Epoch 60/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4094 - accuracy: 0.8700 - val_loss: 0.4965 - val_accuracy: 0.8487\n",
            "Epoch 61/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.4148 - accuracy: 0.8693 - val_loss: 0.4765 - val_accuracy: 0.8583\n",
            "Epoch 62/100\n",
            "210/210 [==============================] - 3s 16ms/step - loss: 0.3975 - accuracy: 0.8766 - val_loss: 0.4702 - val_accuracy: 0.8597\n",
            "Epoch 63/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3890 - accuracy: 0.8771 - val_loss: 0.4997 - val_accuracy: 0.8474\n",
            "Epoch 64/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.4066 - accuracy: 0.8692 - val_loss: 0.4540 - val_accuracy: 0.8638\n",
            "Epoch 65/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3881 - accuracy: 0.8772 - val_loss: 0.4446 - val_accuracy: 0.8686\n",
            "Epoch 66/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3998 - accuracy: 0.8744 - val_loss: 0.4521 - val_accuracy: 0.8658\n",
            "Epoch 67/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3903 - accuracy: 0.8779 - val_loss: 0.4363 - val_accuracy: 0.8711\n",
            "Epoch 68/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3871 - accuracy: 0.8774 - val_loss: 0.4598 - val_accuracy: 0.8620\n",
            "Epoch 69/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3974 - accuracy: 0.8741 - val_loss: 0.4514 - val_accuracy: 0.8659\n",
            "Epoch 70/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3777 - accuracy: 0.8806 - val_loss: 0.4678 - val_accuracy: 0.8595\n",
            "Epoch 71/100\n",
            "210/210 [==============================] - 3s 17ms/step - loss: 0.3761 - accuracy: 0.8826 - val_loss: 0.4659 - val_accuracy: 0.8615\n",
            "Epoch 72/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3838 - accuracy: 0.8782 - val_loss: 0.4918 - val_accuracy: 0.8548\n",
            "Epoch 73/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3863 - accuracy: 0.8775 - val_loss: 0.4400 - val_accuracy: 0.8692\n",
            "Epoch 74/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3851 - accuracy: 0.8787 - val_loss: 0.4346 - val_accuracy: 0.8719\n",
            "Epoch 75/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3507 - accuracy: 0.8895 - val_loss: 0.5015 - val_accuracy: 0.8490\n",
            "Epoch 76/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3721 - accuracy: 0.8816 - val_loss: 0.4226 - val_accuracy: 0.8756\n",
            "Epoch 77/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3617 - accuracy: 0.8877 - val_loss: 0.4640 - val_accuracy: 0.8614\n",
            "Epoch 78/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3606 - accuracy: 0.8852 - val_loss: 0.4358 - val_accuracy: 0.8689\n",
            "Epoch 79/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3614 - accuracy: 0.8840 - val_loss: 0.4348 - val_accuracy: 0.8716\n",
            "Epoch 80/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3481 - accuracy: 0.8907 - val_loss: 0.4464 - val_accuracy: 0.8687\n",
            "Epoch 81/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3445 - accuracy: 0.8893 - val_loss: 0.4351 - val_accuracy: 0.8712\n",
            "Epoch 82/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3536 - accuracy: 0.8887 - val_loss: 0.4405 - val_accuracy: 0.8686\n",
            "Epoch 83/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3618 - accuracy: 0.8835 - val_loss: 0.4127 - val_accuracy: 0.8805\n",
            "Epoch 84/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3496 - accuracy: 0.8861 - val_loss: 0.4160 - val_accuracy: 0.8776\n",
            "Epoch 85/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3363 - accuracy: 0.8929 - val_loss: 0.4476 - val_accuracy: 0.8656\n",
            "Epoch 86/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3435 - accuracy: 0.8901 - val_loss: 0.4532 - val_accuracy: 0.8684\n",
            "Epoch 87/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3455 - accuracy: 0.8907 - val_loss: 0.4003 - val_accuracy: 0.8830\n",
            "Epoch 88/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3299 - accuracy: 0.8936 - val_loss: 0.4258 - val_accuracy: 0.8764\n",
            "Epoch 89/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3393 - accuracy: 0.8920 - val_loss: 0.4447 - val_accuracy: 0.8696\n",
            "Epoch 90/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3259 - accuracy: 0.8966 - val_loss: 0.4122 - val_accuracy: 0.8785\n",
            "Epoch 91/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3231 - accuracy: 0.8981 - val_loss: 0.4504 - val_accuracy: 0.8658\n",
            "Epoch 92/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3310 - accuracy: 0.8940 - val_loss: 0.4323 - val_accuracy: 0.8727\n",
            "Epoch 93/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3340 - accuracy: 0.8915 - val_loss: 0.4304 - val_accuracy: 0.8754\n",
            "Epoch 94/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3225 - accuracy: 0.8986 - val_loss: 0.4680 - val_accuracy: 0.8613\n",
            "Epoch 95/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3378 - accuracy: 0.8928 - val_loss: 0.4523 - val_accuracy: 0.8671\n",
            "Epoch 96/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3302 - accuracy: 0.8947 - val_loss: 0.3973 - val_accuracy: 0.8864\n",
            "Epoch 97/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3144 - accuracy: 0.8986 - val_loss: 0.4003 - val_accuracy: 0.8839\n",
            "Epoch 98/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3160 - accuracy: 0.8983 - val_loss: 0.3912 - val_accuracy: 0.8885\n",
            "Epoch 99/100\n",
            "210/210 [==============================] - 4s 17ms/step - loss: 0.3126 - accuracy: 0.8991 - val_loss: 0.4077 - val_accuracy: 0.8839\n",
            "Epoch 100/100\n",
            "210/210 [==============================] - 4s 18ms/step - loss: 0.3065 - accuracy: 0.9016 - val_loss: 0.3955 - val_accuracy: 0.8863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "535J2nsZ3-p_",
        "outputId": "c4b95f61-5c29-4a02-cf28-0dc53bc462d7"
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3955 - accuracy: 0.8863\n",
            "Validation accuracy: 88.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxckcIdy4R-g",
        "outputId": "00c71060-d020-4dd6-97b9-1c022e52b366"
      },
      "source": [
        "print('Testing the model on test dataset')\n",
        "predictions = model3.predict_classes(X_test)\n",
        "score = model3.evaluate(X_test, y_test)\n",
        "print('Test loss :', score[0])\n",
        "print('Test accuracy :', score[1])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing the model on test dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "563/563 [==============================] - 2s 3ms/step - loss: 0.6680 - accuracy: 0.8281\n",
            "Test loss : 0.6679835319519043\n",
            "Test accuracy : 0.8281111121177673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy-JymMm6frU",
        "outputId": "63aac236-cb1a-428a-fc0d-07f09d5140ea"
      },
      "source": [
        "print('Classification Report'); print('--'*40)\n",
        "print(classification_report(y_test_o, predictions))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86      1814\n",
            "           1       0.82      0.86      0.84      1828\n",
            "           2       0.85      0.82      0.83      1803\n",
            "           3       0.79      0.79      0.79      1719\n",
            "           4       0.88      0.85      0.87      1812\n",
            "           5       0.77      0.83      0.80      1768\n",
            "           6       0.80      0.84      0.82      1832\n",
            "           7       0.88      0.84      0.86      1808\n",
            "           8       0.80      0.80      0.80      1812\n",
            "           9       0.83      0.80      0.82      1804\n",
            "\n",
            "    accuracy                           0.83     18000\n",
            "   macro avg       0.83      0.83      0.83     18000\n",
            "weighted avg       0.83      0.83      0.83     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "7fOCaOcY6lAl",
        "outputId": "3d4a8fd8-ea01-4a87-a039-5e4146651366"
      },
      "source": [
        "print('Confusion matrix')\n",
        "plt.figure(figsize = (15, 10))\n",
        "sns.heatmap(confusion_matrix(y_test_o, predictions), annot = True)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcef37a0e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "TkX_7FzI6ply",
        "outputId": "b7a1e778-2592-428f-a800-f106bb533bab"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model3 Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model3 Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBFtJJDe6tcI",
        "outputId": "37bba383-87f7-4575-bf3d-7b35b7f507cb"
      },
      "source": [
        "model3.predict_classes(X_test)[5]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "ZP_1GQn77BtK",
        "outputId": "ac789e37-3524-440f-9bc9-45a7edf86818"
      },
      "source": [
        "#Showing the image\n",
        "plt.imshow(X_test[20].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcef2df01d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdM0lEQVR4nO2da4xdV3XH/8uOYyeeyfXbHo8n+DVpiZzajYpLIAokKciJBA4CVUkBEeGqqCISqPSDRaQ2LY0U2gJfQLRFjmKVlJQ2ARuUOk2NHwpJnRjqZ1wyxnYS2+OZcTwPO3hmYnv3wz37cOfMXuvee+beM+7m/5NGc+7eZ5+z7753zT6z/nutLc45EELiZMpkd4AQ0jxo4IREDA2ckIihgRMSMTRwQiLmmmZdeHBwkO55QgqkVCpJtmxCM7iIrBORX4jIURHZOJFrEUIaT24DF5GpAL4F4B4ANwN4QERublTHCCETZyKP6GsBHHXOHQMAEXkKwHoAr2ZP7OvrAwAMDAxg1qxZE7ilztSpU9W6KVPG/h3r6+vD/Pnzq17z0qVLwfJ33nlHbTMyMqLWZRcVXbp0CddcU/0jsO6n9VErD9W1trbi/PnzwT5WYo3xzJkzg+XTp09X22TvNTo6imuvvTY91rh48aJaNzg4GCzv7e1V25w9e3bM67vuugs/+clPAADd3d1qu66uLrXu+PHjat2vfvWrYLl/754nn3wSn/zkJwEAIuOevgEAe/bsUe8DAJJ3JZuIfALAOufcHyevPw3g951zDwFj/we3BoIQkp/Ozs70OPQ/eNOcbJX4WZszOGfwSjiDj6UZM/hEnGynAHRUvF6SlBFCrhImMoO/AqBTRJahbNj3A/ij0IkXLlwYd1zPX3aPNStlZ+lKsn8ZgV/PtPXM/J5p06bl6sfw8LB6fqjOY81mQ0NDwfLTp0+rbbKzwerVq3Hs2DEA4bHyXH/99Wqd9pTR0tKitgndy3/G1udy3XXXqXXaDG5dLy9Xrlxp6PUuX76sluX9Vzq3gTvnLonIQwCeAzAVwOPOucN5r0cIaTwT+h/cOfcsgGcb1BdCSIPhUlVCIoYGTkjE0MAJiRgaOCERU8hCl5MnTwIAFi1alB6fOXNGPb+/vz9Y3tbWprbp6OhQ67KyypQpUzAwMADAln40OcaSfqyFK9nFFqOjo6nkduLECbVdT0+PWqctmrAWg4RkvlrGwy+GCaH10VpQ9K53vWvM6ylTpqQyWalUUtu9/fbbap02HlbfQ3KdL5szZ47azvoehCQvj7YYKrRYyEualjRrwRmckIihgRMSMTRwQiKGBk5IxNDACYmYQrzo+/btAwCsW7cuPba8xj5BRJbly5erbawgjxtvvHHM6xkzZqQBHFYQgua5tIIM6gkMmTp1alqmKQfV6jSvfXt7u9omxIIFCwDYQQ1aIEe1Oo0bbrhhzOu5c+em79XyGluBOZoX3WoT+u74z9jqh/XdsUJ8te9ISLXx/bC+3xacwQmJGBo4IRFDAyckYmjghEQMDZyQiKGBExIxhchklcEB/thajJ9nYb0lXVnBBJbUoUlGVuCCliMNAN54440xr5ctW5aWWVKYhRbMsWLFCrVNSDLyspomMwG29KONiZVl1ge4eObOnZuWWZ+LlZuvMv9fJdb7CklQ/nzre2X1I0+70Pj6MspkhJBx0MAJiRgaOCERQwMnJGJo4IREDA2ckIgpRCZbvXr1uGMrh5qWc8tHPIVYunSpWtfa2jru+r7MkmO0bXw0KQb4df65ENkIumXLlqVlVg41a8NGLXdZNlKrktD78udbWxdZEVmaPGjJRSHpypdZueGsiD3ts7HyuIW20bLu4alng8dKNLnRksm0zQerMSEDF5ETAM4DuAzgknPu9yZyPUJIY2nEDH6nc+5s9dMIIUXD/8EJiRjJuy0pAIjIcQD9AByAf3TO/ZOvGxwcTC9sbZROCMlPZ2dnelwqlcb9oz7RR/TbnXOnRGQBgOdF5H+dc7uzJx09ehQAsHLlyvT43Llz6kXzONmstddz584dd32fZL7RTjb//kJk/9Ddeeed2LFjB4D8TjbNuWilt7KcZVadtVnFm2++GSy3nGzZTQVuueUWHDx4EAAwb948tZ3lANMmE2utf9bJ9tGPfhRbt25Vz/f4vobYu3evWqd997OO0R/96Ef4yEc+AkD/Lh44cMDs44Qe0Z1zp5LfvQB+AGDtRK5HCGksuWdwEZkJYIpz7nxy/GEAfx0699ZbbwVQllIqjzW0GdySTqwtcrLJ7N5++21zSxqPNrtbTx/Z7YmqtfNl1nuz+qrV1fNkMjo6mpZZMpk1c2pJF61kjKHP2ZdZ0VNWVJsWNWb1PSST1YL1dGJFS2p1V5tMthDAD5IbXwPgX5xz2yZwPUJIg8lt4M65YwBWVz2REDJpUCYjJGJo4IREDA2ckIihgRMSMYVEk/mIp6GhofQ4tA+TR1v0YUkPlowQklx8mSV1aHXWQhdLJrNkkNmzZ6vtFi9erNYtXLgwWJ6NoKsku5jl7Nmz6SILKwrKkvK0RJlW0sVQosa33nor2MdK8kpGjcZaBWp9r/JEk1mypwVncEIihgZOSMTQwAmJGBo4IRFDAyckYgrxold6v/3xjBkz1POvuSbcLSuvlrU1TcgzbAW7eLQABctT7r3AIUKeZu/Nt0JhFy1apNZpudcslSI0Hn7MLc+wtaWU5jW2vOGh8fXnWwEgVp3mbba80KE6X2Z5wy3yqDOh8rz393AGJyRiaOCERAwNnJCIoYETEjE0cEIihgZOSMQUIpNVBmf44zyBC5aEYwU1hGQhS1bzaEEvfX19aptQAIUnJHf5HGjZzK+VtLS0qHWalFfvWPkyKxeaJTVp0lXuIAmjH5YEqH0PrO9H6PvmyyyZyupjo7Hy0FlwBickYmjghEQMDZyQiKGBExIxNHBCIoYGTkjEFCKThbBkMm37HCsnW71RS77MknG0/GSW5GLJb6FcYr7Mkn4stPtZWxBZub8sSU6LXLPaaZGB1bC2GrLGWLufNR6hyEZfZslTlkxm5Y3T6qzcgU3LySYij4tIr4gcqiibIyLPi0hX8lvPGEgImTRqeUR/AsC6TNlGANudc50AtievCSFXGVUNPNnvO7st5noAm5PjzQDua3C/CCENQKwljelJIksB/Ng5typ5PeCcm5UcC4B+/9ozODiYXljblJ0QMjE6OzvT41KpNO6f+wk72ZxzTkTMvxLeOTM0NJQez5s3Tz1fc4hYaZasVElZh41zLnV0WM4LzZnz8ssvq22suhUrVox5vX79emzZsgUAcM8996jtlixZotZp6+Utp1LWSXjhwoXUSZZn3TsAHD58OFh+8OBBtU123f7HP/5xPP300wCAmTNnqu0sB22ezSqyzsM77rgDu3fvBmA72fbs2aPW/fSnP1XrtFiG7GYV27dvx9133w1A/zyPHj2q3gfIL5P1iEgbACS/9SRlhJBJI+8MvhXAZwA8lvzeYp1cGWnkjy3JS5tFrNnWml2yf72Hh4fNxH0eTQ7r6elR21h9nD9/vlpmyWTWWGkzjBUFFZoNfPSUdS8LTUKzZuKzZ8+OK7MkSI+1LZM2/tb4WvKlJb/WO8bV6kLlvsxKeGlRi0z2PQAvAfgtETkpIhtQNuwPiUgXgD9IXhNCrjKqzuDOuQeUqrsb3BdCSIPhUlVCIoYGTkjE0MAJiRgaOCERU0g0WX9//7hjKznhrFmzguWW9GAtfsgyPDycSnGWLKPVWYsmLPkttLjHl1kyiCVd5Un8lzfJoLXqUZPDLHnKivJr9B5plnwZupcvs/bDs6RZK5pM678l19Wy4jQEZ3BCIoYGTkjE0MAJiRgaOCERQwMnJGJo4IRETCEymY8Jfve7350eW3HHWuI8a/8uLUEiYCezs5IC5tkPypLrrOR+loxjSWGa5GLJXRbWeOTZt8ySd0Lyny+z7mV91tr96k2e6M+37mXJl5ZMVk+bWvIWWHAGJyRiaOCERAwNnJCIoYETEjE0cEIiphAv+pEjRwCUvej+OORR9miL+Ds6OtQ2loc9FAjhPaeWt1PLx2Xl6Zo9W9/kJeRh92V5vK5521mqQt7cX3naWVso1et9r4bVJvR985+xFVBieebzBAiFPOW+rGk52Qgh/3+hgRMSMTRwQiKGBk5IxNDACYkYGjghEVOITFbp/vfH2sZ+AHDy5MlguSWd1BuQ4eUPSwbR+mjl6ZozZ45aZ2FJLpYUZgWHNBpLqtHy5Vl9DwVy+LK8wTJ5ctRZ/bA+F+u7k+e7mjcoyqKWrYseF5FeETlUUfaIiJwSkX3Jz7257k4IaSq1/Ll7AsC6QPk3nHNrkp9nG9stQkgjqGrgzrndAM4V0BdCSIORWvIti8hSAD92zq1KXj8C4EEAQwD2AviSc66/ss3g4GB64a6urkb1lxBSQWdnZ3pcKpXGOTzyemi+DeArAFzy+2sAPqudvGVLefvw9evXp8eW8yK0jzYAtLe3q22WLl2q1i1YsGDM63PnzqXOMMtRsmfPnmD5c889p7ax1svfeeedY17Pmzcv3SPbem/Whg+a88XKRJJdm3/mzBksWrQIgO1Is+pCe30DwM6dO9U2L7744pjXDz30EL75zW8CsJ1lt9xyi1qntbM22shucFH5Pe3t7VXbHT58WK07fvy4Wqc5b7Ofyw9/+EPcd999APQNJPbu3aveB8gpkznnepxzl51zVwB8B8DaPNchhDSXXDO4iLQ557qTlx8DcMg6f+XKleOOBwcH1fO1aC1ryyDreqGZx2+hZEX9aH9pracPq86SY6wtlCy0GdyaAUMSlC+zxsOKANTuZz1JWFsXWU8tVn6yPFFXVm44i7wRgNpnZm0plVcmq9pKRL4H4IMA5onISQB/CeCDIrIG5Uf0EwA+l+vuhJCmUtXAnXMPBIo3NaEvhJAGw6WqhEQMDZyQiKGBExIxNHBCIqaQUKTFixePO7ZkCC2a7Nw5fcWsJQtNnz59zOuWlpZ04YMVtaRd05JirPcVkv98mSUnZftfibYS0ZJwrGSHeWUybRzrlRR9Wd4FN9pYWX0PvWd/HUuSs6Qrq077rK2ki3nhDE5IxNDACYkYGjghEUMDJyRiaOCERAwNnJCIKUQmu+GGG4LHGloMrpWosVQqqXVLliwZV1ZL0kVN6gjtMeaxZKFQskZfZsl1luSVRyazoqesBCBWHzV5rd42vsySh7TYaECXw6z95EKfmY9ms/phjbEl22rtQuW+LE8ySYAzOCFRQwMnJGJo4IREDA2ckIihgRMSMYV40b3Xc3R0ND22PNGaZzvvVjFWkIfl9da86C0tLWqbixcvqnVWcEW9Wy958uQFs3KhWUES1lhp1JLbLIQ1Hla+Ns2LbgXshOp8WZ7tmoB8gShWsAm96ISQcdDACYkYGjghEUMDJyRiaOCERAwNnJCIKUQm85uqjY6OpseWxDBv3rxguSXTWAEIlkxmyQ+zZ88Olre1taltTpw4odaFtlfyZXnkOqvOyvGWHfvh4eG0LE/+NwDo6+sLllt59ELX82VWUJKVX03royWxWlsGNVqirHbNRlN1BheRDhHZISKvishhEflCUj5HRJ4Xka7kd9gaCCGTRi2P6JdQ3v/7ZgDvBfB5EbkZwEYA251znQC2J68JIVcRVQ3cOdftnPt5cnwewBEA7QDWA9icnLYZwH3N6iQhJB9i/V817mSRpQB2A1gF4A3n3KykXAD0+9cAMDg4mF64q6urQd0lhFTS2dmZHpdKpXFOgZqdbCLSAuBpAF90zg1VOhicc05E1L8U3lnV39+fHu/fv1+9186dO4PlliPqpptuUuva29vHvF6+fDmOHTsGIN+a5yNHjqhtLCdb5T7pAPCBD3wAu3btAgCsXbtWbVe5cUSWPE627PrwoaGh1KllOdks59DRo0eD5du2bVPbZMfx4YcfxqOPPgoAWLZsmdru9ttvV+s0x9eZM2fUNtmxuu222/DSSy8B0DfhAIADBw6oddp4hO7nyY79d7/7XXzqU58CoI/9Cy+8oN4HqFEmE5FpKBv3k865Z5LiHhFpS+rbAITzLBFCJo2qM3jy+L0JwBHn3NcrqrYC+AyAx5LfW9SbVMwy/tjKkaVJJFZ+L0tKsnJdWZFhc+bMCZYPDQ2pbV5//XW17uzZs2rZhQsX1HbWrKo9gVgzeFZSHBoaSstaW1vVdlZOvNOnTwfLrZnT+lxmzZo1rs5jyWRvvfVWsDwkUVrX80851nfO+sysp8088prVD4taHtHfD+DTAA6KyL6k7MsoG/b3RWQDgNcB/GGuHhBCmkZVA3fOvQBA+5Nzd2O7QwhpJFyqSkjE0MAJiRgaOCERQwMnJGIKiSar3LLHHw8MDKjna1KHlcBPk7SAsPTjy6yoJUue0rAWzljRU3mTE1ryoEZIQvNllvRjyWSaLGdJWqE6X2aNoyUZafJUvZLW+fPnAdjympVgc2RkRK3TZLLQ+/L3yPM5A5zBCYkaGjghEUMDJyRiaOCERAwNnJCIoYETEjGFyGTd3d0AgFKplB739urRpaGoK8CWkpYuXarWhRIy+jJLCgvtFWWVA3Zyv1AUmi+zZENL4tFkPktWCfXRy1xeHgrR39+v1mkSmhVDHpI2fZkVTZYnZt2S1kJ992V59mMD7O+IlmTFiq7j3mSEkHHQwAmJGBo4IRFDAyckYmjghERMIV70np4eAGUvuj+2FvFrWIEL1tZFVlCD1U7zRFse0nq9177M8qJrqoLVF2trKMtrnLcflQFFlVx//fVqm5A33G9bpW1fBdh546x8eRohT7kvs/KnWd9HSz3QvOh51R4LzuCERAwNnJCIoYETEjE0cEIihgZOSMTQwAmJmEJkMi+v3HTTTabU4imVSsHyuXPnqm0WLVqk1mWlmpGRkbRs5syZVfuTxZJHrBxvoaARfy0r35kVmKMFIVj9OHfu3JjXra2taVlfX5/azvrstIAYSyYLSWELFy5U6zzW+GsBSVbQSEji82XWFltW8JMVHKL1xZLrmiaTiUiHiOwQkVdF5LCIfCEpf0RETonIvuTn3lw9IIQ0jVpm8EsAvuSc+7mItAL4mYg8n9R9wzn3983rHiFkItSyN1k3gO7k+LyIHAHQbrcihFwNiLZsLniyyFIAuwGsAvBnAB4EMARgL8qzfJoRYHBwML1wV1dXQzpLCBlLZ2dnelwqlcatq63ZwEWkBcAuAI86554RkYUAzgJwAL4CoM0591l/fqWBb968GQDwvve9Dy+++CIA4Pjx4+q9tKwilpNtzZo1at2qVavGvB4ZGUmdFpYzSmP//v1qnX9/IbKOqAcffBBPPPEEAODGG29U261cuVKt05yL1vvKOtJaW1vTMc/rZNM2q7DIOtLe85734JVXXgEw9otbrV0lr732WrDcXzdE9nPZsGEDNm3aBMCOmbC+w5ZjVHOyZeMHnnrqKdx///0A9H3sd+3alR6HDLwmmUxEpgF4GsCTzrlnAMA51+Ocu+ycuwLgOwDW1nItQkhxVP0fXMrhNJsAHHHOfb2ivC35/xwAPgbgkHaNyr9m/tiSLbRoobwRRlmJoXIGt6J+tO14rHtZcl1olps9ezYAW3LxEXghtD7Onz+/5n60tramZVbetay8Vks/rCeJUN41X5ZXFtLG0dpKyJKnLKyoQisHXJ6cbNb31KIWL/r7AXwawEER2ZeUfRnAAyKyBuVH9BMAPperB4SQplGLF/0FAKGg2Gcb3x1CSCPhUlVCIoYGTkjE0MAJiRgaOCERU0g0WeXCFX9sJQUMbWkD2AtdrAijkGThy/LIGVZixfZ2fRVvqI/+fC1pIWAnQtTkHGsBU+hePlmhFdVmjZUma1kyWUhu9GWWBGVFeGlynVZeDasfVkJGC20cQxFovsz6zllwBickYmjghEQMDZyQiKGBExIxNHBCIoYGTkjEFCKTVUoo/tiKumprawuWWxJUnrhuwE6Op0lNWmwuYPcxtPfUkiVLAAAnT55U21nSlbYXV73RWF5us8bDkiK1+2kJNIFwQkZfZvXj4sWLap0mh9Ur/4X2kasHKzpQkzZDfc8r73k4gxMSMTRwQiKGBk5IxNDACYkYGjghEUMDJyRiCpHJKqUjf+zloWrnV2JJa1ZkTyjhnpdhLBlCkzqspIvWXlxZmenKlStYvHixeS/Almy0KDQraWBI7vJRU1b/rcR/2h5vlnwZalPLXnFWAkVtjzSL0Pg2UybTrh16X74sr1zGGZyQiKGBExIxNHBCIoYGTkjE0MAJiZhCvOiVG+v5Y79lT4hQUAZgezat4ATLS6ptdAjowSb1eMoryXqUBwYG0jItwKYa2oaAlqoQUgF8UIiVg8zy5GpjUk8evZGRkfQ61udiecq1TRCtAKHQ9fx7tYJUrPGwxl/LKRfKu+bPbZoXXURmiMjLIrJfRA6LyF8l5ctEZI+IHBWRfxURPYsiIWRSqOURfQTAXc651QDWAFgnIu8F8FUA33DOrQTQD2BD87pJCMlDVQN3ZfwzzLTkxwG4C8C/J+WbAdzXlB4SQnIjVv7s9CSRqQB+BmAlgG8B+DsA/53M3hCRDgD/4Zxb5dsMDg6mF+7q6mpwtwkhANDZ2Zkel0qlcf/41+Rkc85dBrBGRGYB+AGA366nE2+++SYAoKOjIz22nGxaneZ8A2wnW9apMTw8nDp4rGWPjXayZTOHDAwMpPthW3tvd3d3q3WNcLItW7YMx48fB9B4J9vChQvVNiEnm88MYznZTp8+rdYdOhTepl4bJwA4c+bMmNcbN27EY489BsB2sll7qfvveT19yToCt23bhnXr1gEo206InTt3qvcB6pTJnHMDAHYAuA3ALBHxfyCWADhVz7UIIc2n6gwuIvMBvOOcGxCR6wB8CGUH2w4AnwDwFIDPANiiXaNyxvDH9cy4HmsGsa4XyrnlZ24rKEO7pjXrWzNg6F7+vVrBCdY1taca60kiVOeDPKwtpayti7QZ3MoNF3rKqGU7IKsf9QRyeKxcaNbnMtF8aVms8ch7r1oe0dsAbE7+D58C4PvOuR+LyKsAnhKRvwHwPwA25eoBIaRpVDVw59wBAL8bKD8GYG0zOkUIaQxcqkpIxNDACYkYGjghEVPTQpc8VC50IYQ0n9BCF87ghEQMDZyQiGnaIzohZPLhDE5IxNDACYmYQgxcRNaJyC+S7C8bi7in0o8TInJQRPaJyN6C7/24iPSKyKGKsjki8ryIdCW/9RC75vbjERE5lYzLPhG5t8l96BCRHSLyapIl6AtJeaHjYfSj6PFoXtYk51xTfwBMBfBLAMsBXAtgP4Cbm31fpS8nAMybpHvfAeBWAIcqyv4WwMbkeCOAr05SPx4B8OcFjkUbgFuT41YArwG4uejxMPpR9HgIgJbkeBqAPQDeC+D7AO5Pyv8BwJ/We+0iZvC1AI46544550ZRjj5bX8B9ryqcc7sBZIO+16OcDQcoKCuO0o9Ccc51O+d+nhyfB3AEQDsKHg+jH4XiyjQla1IRBt4OoDL6/SQmYRATHID/FJGficifTFIfKlnonPPZHM4A0LMjNJ+HRORA8gjf9H8VPCKyFOVgpj2YxPHI9AMoeDxEZKqI7APQC+B5lJ96B5xzPk40l938pjnZbnfO3QrgHgCfF5E7JrtDHld+DpsszfLbAFagnFSzG8DXiripiLQAeBrAF51zQ5V1RY5HoB+Fj4dz7rJzbg3KyVPWos6sSRpFGPgpAJX5ZiYt+4tz7lTyuxfl1FOTHe7aIyJtAJD87p2MTjjnepIv2BUA30EB4yIi01A2qiedc88kxYWPR6gfkzEeHtfgrElFGPgrADoTj+C1AO4HsLWA+45BRGaKSKs/BvBhAOEEXsWxFeVsOECVrDjNxBtVwsfQ5HGRcpqSTQCOOOe+XlFV6Hho/ZiE8Zif5DtERdakI/h11iQg73gU5CW8F2UP5S8BPFyUdzLTh+Uoe/D3AzhcdD8AfA/lx713UP5/agOAuQC2A+gC8F8A5kxSP/4ZwEEAB1A2srYm9+F2lB+/DwDYl/zcW/R4GP0oejx+B+WsSAdQ/mPyFxXf2ZcBHAXwbwCm13ttLlUlJGJ+05xshPxGQQMnJGJo4IREDA2ckIihgRMSMTRwQiKGBk5IxPwfjLTNzTfAxN8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MTn6xYI7BpP",
        "outputId": "7f43c20b-e857-4658-f50d-7c3c2d90c855"
      },
      "source": [
        "model3.predict_classes(X_test)[20]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "vdXR3mfz7BjV",
        "outputId": "2b2a771f-0e2b-46b9-8a8b-d704c549d138"
      },
      "source": [
        "plt.imshow(X_test[10].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcef2d4a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nA7-vNb7BYf",
        "outputId": "13a76db9-ddc8-4903-a413-e264c930c7f3"
      },
      "source": [
        "model3.predict_classes(X_test)[10]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrG6nDUI7NIQ"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "- Evaluated the accuracy using two methods i.e. baby sitting the NN and NN through API. \n",
        "- Followed all the required steps starting with loading the datasets to performing hyperparameter optimization and running a finer search by using a finer range. \n",
        "- Explored different options in optimizers, number of activators, learning rate and activation methods in NN through API. \n",
        "- Found that baby sitting process achieved the best accuracy of 21% using hyper parameter optimization. \n",
        "- It might have been further improved but that's the trade off vs time taken to run the script. \n",
        "- NN through API method achieved best accuracy of 90% on validation set. \n",
        "- Also printed the classification report, visualized the confusion matrix and summarized history for accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4Hof0cM7jZ1",
        "outputId": "96c4f265-0006-4d06-ee89-4543579b906c"
      },
      "source": [
        "!pip install nbconvert"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (5.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert) (3.3.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert) (4.7.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.11.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.1.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (1.4.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.0.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (20.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DekpRUgSFN0F",
        "outputId": "4037c2b9-58ec-4eab-e0ba-9bc4cc55279d"
      },
      "source": [
        "!jupyter nbconvert --to html Saumya Kothari - Introduction to Neural Networks & Deep Learning Project [Part 4].ipynb"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: Deep: command not found\n",
            "This application is used to convert notebook files (*.ipynb) to various other\n",
            "formats.\n",
            "\n",
            "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "-------\n",
            "\n",
            "Arguments that take values are actually convenience aliases to full\n",
            "Configurables, whose aliases are listed on the help line. For more information\n",
            "on full configurables, see '--help-all'.\n",
            "\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "    This mode is ideal for generating code-free reports.\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "    relevant when converting to notebook format)\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "    overwriting the existing notebook.\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "--generate-config\n",
            "    generate default config file\n",
            "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
            "    Default: 4\n",
            "    Choices: [1, 2, 3, 4]\n",
            "    The nbformat version to write. Use this to downgrade notebooks.\n",
            "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
            "    Default: ''\n",
            "    Directory to write output(s) to. Defaults to output to the directory of each\n",
            "    notebook. To recover previous default behaviour (outputting to the current\n",
            "    working directory) use . as the flag value.\n",
            "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
            "    Default: 'FilesWriter'\n",
            "    Writer class used to write the  results of the conversion\n",
            "--log-level=<Enum> (Application.log_level)\n",
            "    Default: 30\n",
            "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
            "    Set the log level by value or name.\n",
            "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
            "    Default: u''\n",
            "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
            "    but can be any url pointing to a copy  of reveal.js.\n",
            "    For speaker notes to work, this must be a relative path to a local  copy of\n",
            "    reveal.js: e.g., \"reveal.js\".\n",
            "    If a relative path is given, it must be a subdirectory of the current\n",
            "    directory (from which the server is run).\n",
            "    See the usage documentation\n",
            "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
            "    slideshow) for more details.\n",
            "--to=<Unicode> (NbConvertApp.export_format)\n",
            "    Default: 'html'\n",
            "    The export format to be used, either one of the built-in formats\n",
            "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
            "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
            "    the import path for an `Exporter` class\n",
            "--template=<Unicode> (TemplateExporter.template_file)\n",
            "    Default: u''\n",
            "    Name of the template file to use\n",
            "--output=<Unicode> (NbConvertApp.output_base)\n",
            "    Default: ''\n",
            "    overwrite base name use for output files. can only be used when converting\n",
            "    one notebook at a time.\n",
            "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
            "    Default: u''\n",
            "    PostProcessor class used to write the results of the conversion\n",
            "--config=<Unicode> (JupyterApp.config_file)\n",
            "    Default: u''\n",
            "    Full path of a config file.\n",
            "\n",
            "To see all available configurables, use `--help-all`\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb\n",
            "    \n",
            "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "    \n",
            "    You can specify the export format with `--to`.\n",
            "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "    \n",
            "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "    \n",
            "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "    can specify the flavor of the format used.\n",
            "    \n",
            "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "    \n",
            "    You can also pipe the output to stdout, rather than a file\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "    \n",
            "    PDF is generated via latex\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "    \n",
            "    You can get (and serve) a Reveal.js-powered slideshow\n",
            "    \n",
            "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "    \n",
            "    Multiple notebooks can be given at the command line in a couple of \n",
            "    different ways:\n",
            "    \n",
            "    > jupyter nbconvert notebook*.ipynb\n",
            "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "    \n",
            "    or you can specify the notebooks list in a config file, containing::\n",
            "    \n",
            "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "    \n",
            "    > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "[NbConvertApp] CRITICAL | Bad config encountered during initialization:\n",
            "[NbConvertApp] CRITICAL | Invalid argument: '-'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErOY5AGFFYxh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}